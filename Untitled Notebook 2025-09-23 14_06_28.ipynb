{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab9f2939-13a5-4766-af7e-fdac3d40b934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e95bf610-7560-4b17-a0d2-b5053e08e772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# test_df.write.mode('overwrite').parquet(\"/Volumes/main_prod/datascience_scratchpad/jatin/trajcl_exp/nyc/hyp_2/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cf0d70a-6f95-4ffe-a9e0-51b63f35bc83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nyc_traj_data_pd = pd.read_parquet(\"/Volumes/main_prod/datascience_scratchpad/jatin/trajcl_exp/nyc/hyp_2/test\")\n",
    "df_list = [nyc_traj_data_pd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "227c8d34-36dc-4748-9fd7-28d7216fe034",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nyc_traj_data_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "310c556a-b4cc-481b-ae47-27cfcfaf5bb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "i=1\n",
    "j=10\n",
    "int(df_list[0]['timestamps'].values[i][j+1] - df_list[0]['timestamps'].values[i][j])/1e9/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1dfbbb5-1954-4e07-81dc-b40919eae4b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b7585c7-3806-4bca-aabc-3f3eefe3473a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from config_infer import InferenceConfig\n",
    "from model.dual_attention import DualSTBTimeWeighted\n",
    "from utils.traj import *\n",
    "import pickle\n",
    "from utils.cellspace import *\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "cfg = InferenceConfig()\n",
    "# print(cfg.to_str())\n",
    "\n",
    "print(cfg.checkpoint_file)\n",
    "encoder_q = DualSTBTimeWeighted(cfg.seq_embedding_dim, \n",
    "                                            cfg.trans_hidden_dim, \n",
    "                                            cfg.trans_attention_head, \n",
    "                                            cfg.trans_attention_layer, \n",
    "                                            cfg.trans_attention_dropout, \n",
    "                                            cfg.trans_pos_encoder_dropout)\n",
    "\n",
    "encoder_q = encoder_q.to(cfg.device)\n",
    "device = cfg.device\n",
    "\n",
    "print(encoder_q)\n",
    "\n",
    "# load model from checkpoint\n",
    "checkpoint = torch.load(cfg.checkpoint_file, map_location=cfg.device)['model_state_dict']\n",
    "encoder_q_keys = [k for k in list(checkpoint.keys()) if 'encoder_q' in k]\n",
    "\n",
    "new_checkpoint = {}\n",
    "for k in encoder_q_keys:\n",
    "    new_k = k.replace('clmodel.encoder_q.', '')\n",
    "    new_checkpoint[new_k] = checkpoint[k]\n",
    "\n",
    "encoder_q.load_state_dict(new_checkpoint)\n",
    "encoder_q.eval()\n",
    "print(\"Model loaded from checkpoint.\")\n",
    "\n",
    "embs_parent = pickle.load(open(cfg.dataset_embs_file_parent, 'rb')).to('cpu').detach() # tensor\n",
    "embs_child = pickle.load(open(cfg.dataset_embs_file_child, 'rb')).to('cpu').detach() # tensor\n",
    "cellspace_parent = pickle.load(open(cfg.dataset_cell_file_parent, 'rb'))\n",
    "cellspace_child = pickle.load(open(cfg.dataset_cell_file_child, 'rb'))\n",
    "hier_cellspace = HirearchicalCellSpace(cellspace_parent, cellspace_child)\n",
    "\n",
    "def model_forward(trajs1_emb, trajs1_emb_p, trajs1_len, time_deltas1):\n",
    "    max_trajs1_len = trajs1_len.max().item() # trajs1_len[0]\n",
    "    src_padding_mask1 = torch.arange(max_trajs1_len, device = cfg.device)[None, :] >= trajs1_len[:, None]\n",
    "    # traj_embs = self.clmodel.encoder_q(**{'src': trajs1_emb, 'time_indices': time_indices1, 'attn_mask': None, 'src_padding_mask': src_padding_mask1, 'src_len': trajs1_len, 'srcspatial': trajs1_emb_p})\n",
    "    traj_embs = encoder_q(**{'src': trajs1_emb, 'time_deltas': time_deltas1, 'attn_mask': None, 'src_padding_mask': src_padding_mask1, 'src_len': trajs1_len, 'srcspatial': trajs1_emb_p})\n",
    "    return traj_embs\n",
    "\n",
    "def infer_batch(traj, time_indices):\n",
    "    traj_cell_parent, traj_cell_child, traj_p, traj_timedelta = zip(*[merc2cell2(l,t, hier_cellspace) for l,t in zip(traj, time_indices)])\n",
    "    # print(traj_cell)\n",
    "    traj_emb_p = [torch.tensor(generate_spatial_features(t, hier_cellspace)) for t in traj_p]\n",
    "    traj_emb_p = pad_sequence(traj_emb_p, batch_first = False).to(device)\n",
    "    traj_emb_cell_parent = [embs_parent[list(t)] for t in traj_cell_parent]\n",
    "    traj_emb_cell_child = [embs_child[list(t)] for t in traj_cell_child]\n",
    "    traj_emb_cell = [a + b for a, b in zip(traj_emb_cell_parent, traj_emb_cell_child)]\n",
    "    traj_emb_cell = pad_sequence(traj_emb_cell, batch_first = False).to(device)\n",
    "    traj_len = torch.tensor(list(map(len, traj_p)), dtype = torch.long, device = device)\n",
    "    traj_timedelta = pad_sequence([torch.log(torch.tensor(t)) for t in traj_timedelta], batch_first=False, padding_value=0).to(cfg.device)\n",
    "    # print(traj_emb_cell, traj_emb_p, traj_len)\n",
    "    traj_embs = model_forward(traj_emb_cell.float(), traj_emb_p.float(), traj_len, traj_timedelta)\n",
    "    return traj_embs, traj_cell_parent, traj_cell_child , traj_p, traj_timedelta\n",
    "\n",
    "batch_size = cfg.batch_size\n",
    "def infer(traj, time_indices):\n",
    "    if len(traj)> batch_size:\n",
    "        traj_embs = []\n",
    "        for i in range(0, len(traj), batch_size):\n",
    "            traj_batch = traj[i:i+batch_size]\n",
    "            time_indices_batch = time_indices[i:i+batch_size] \n",
    "            traj_embs.append(infer_batch(traj_batch, time_indices_batch))\n",
    "        return torch.cat(traj_embs, dim=0)\n",
    "    else:\n",
    "        return infer_batch(traj, time_indices)\n",
    "\n",
    "\n",
    "def get_data_for_userid(userid):\n",
    "    df_user = pd.concat([df[df['userid']==userid] for df in df_list]).reset_index(drop=True)\n",
    "    return df_user\n",
    "\n",
    "def get_data_for_employername(df_user, employername):\n",
    "    df_emp = df_user[df_user['employername']==employername].reset_index(drop=True)\n",
    "    return df_emp\n",
    "\n",
    "\n",
    "def get_data_for_partition(df_emp, partition):\n",
    "    df_part = df_emp[df_emp['partition_id']==partition].reset_index(drop=True)\n",
    "    df_part = df_part[~df_part['weekday'].isin([5, 6])].reset_index(drop=True) # filter out weekends\n",
    "    df_part = df_part[df_part['pck_amt']>0].reset_index(drop=True) # filter out zero paycheck amount\n",
    "    return df_part\n",
    "\n",
    "\n",
    "def get_traj_and_time_data(df_part):\n",
    "    traj = df_part['merc_seq'].values\n",
    "    time_indices = df_part['timestamps'].values\n",
    "    # time_indices = [np.array(time_indices[i], dtype='datetime64[ns]') for i in range(len(traj))]\n",
    "    # print(time_indices)\n",
    "    return traj, time_indices\n",
    "\n",
    "\n",
    "def apply_dbscan(embs, target_min_similarity=0.85):\n",
    "    from sklearn.cluster import DBSCAN     # require >= 0.9 cosine similarity\n",
    "    eps = 1.0 - target_min_similarity    # cosine distance threshold\n",
    "    n_embs = embs.shape[0]\n",
    "    db = DBSCAN(eps=eps, min_samples=max(int(n_embs*0.3), 5), metric=\"cosine\", n_jobs=-1).fit(embs)\n",
    "    return db\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from glob import glob\n",
    "    import pandas as pd\n",
    "    # parquet_files = glob(\"/home/sagemaker-user/TrajCL/data/nyc/test/*.parquet\")\n",
    "    total_count = 0\n",
    "    # df_list = []\n",
    "    # for file in parquet_files:\n",
    "    #     df = pd.read_parquet(file)\n",
    "    #     df_list.append(df)\n",
    "    #     total_count += len(df)\n",
    "\n",
    "\n",
    "    print(f\"Total records in repartitioned files: {total_count}\")\n",
    "\n",
    "    unique_userids = pd.concat([df['userid'] for df in df_list]).unique()\n",
    "    print(f\"Total unique userids: {len(unique_userids)}\")\n",
    "\n",
    "    unique_employernames = pd.concat([df['employername'] for df in df_list]).unique()\n",
    "    print(f\"Total unique employernames: {len(unique_employernames)}\")\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    import torch.nn as nn\n",
    "    output_dict = {}\n",
    "\n",
    "    # 0: moving, 1: static, 2: not enough data\n",
    "    def get_gt_and_pred_label(userid):\n",
    "        count = 0\n",
    "        user_data = get_data_for_userid(userid)\n",
    "        employers = user_data['employername'].unique()\n",
    "        for employer in employers:\n",
    "            emp_data = get_data_for_employername(user_data, employer)\n",
    "            partitions = emp_data['partition_id'].unique()\n",
    "            for partition in partitions:\n",
    "                # print(userid, employer, partition)\n",
    "                part_data = get_data_for_partition(emp_data, partition)\n",
    "                if len(part_data)<15:\n",
    "                    continue\n",
    "                else:\n",
    "                    # print(userid, employer, partition)\n",
    "                    # print(len(part_data))\n",
    "                    traj, time_indices = get_traj_and_time_data(part_data)\n",
    "                    # print(traj)\n",
    "                    embs, traj_cell_parent, traj_cell_child , traj_p, traj_timedelta = infer(traj, time_indices)\n",
    "                    embs = nn.functional.normalize(embs, dim=1)\n",
    "                    db_scan = apply_dbscan(embs.detach().cpu().numpy(), target_min_similarity=0.95)\n",
    "                    labels = db_scan.labels_                      # shape: (n_samples,)\n",
    "                    cluster_ids = [c for c in np.unique(labels) if c != -1]\n",
    "                    if len(cluster_ids)>0:\n",
    "                        pred_label = 1\n",
    "                    else:\n",
    "                        pred_label = 0\n",
    "                    output_dict[(userid, employer, partition)] = pred_label\n",
    "\n",
    "        return count\n",
    "\n",
    "    count=0\n",
    "    for i, userid in tqdm(enumerate(unique_userids)):\n",
    "        count+=get_gt_and_pred_label(userid)\n",
    "        if i>1000:\n",
    "            break\n",
    "\n",
    "\n",
    "    count_dict = {}\n",
    "    for k,v in output_dict.items():\n",
    "        if v in count_dict:\n",
    "            count_dict[v]+=1\n",
    "        else:\n",
    "            count_dict[v]=1\n",
    "    print(count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e30b972b-f667-4d55-96af-4343c245d50d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08a07c57-d918-4ffb-88c7-9673fc3454d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!python infer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61068c17-1a63-4947-8541-62ca92d6f2cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-09-23 14_06_28",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
