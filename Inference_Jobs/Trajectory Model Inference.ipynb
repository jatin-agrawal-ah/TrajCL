{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e51b6b8-44eb-4888-824f-81f754180f34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"pip install -r ../requirements.txt\")\n",
    "os.system(\"pip install tqdm\")\n",
    "\n",
    "from config_infer import InferenceConfig\n",
    "from model.dual_attention import DualSTBTimeWeighted\n",
    "from utils.traj import *\n",
    "import pickle\n",
    "from utils.cellspace import *\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "cfg = InferenceConfig()\n",
    "# print(cfg.to_str())\n",
    "\n",
    "print(cfg.checkpoint_file)\n",
    "encoder_q = DualSTBTimeWeighted(cfg.seq_embedding_dim, \n",
    "                                            cfg.trans_hidden_dim, \n",
    "                                            cfg.trans_attention_head, \n",
    "                                            cfg.trans_attention_layer, \n",
    "                                            cfg.trans_attention_dropout, \n",
    "                                            cfg.trans_pos_encoder_dropout)\n",
    "\n",
    "encoder_q = encoder_q.to(cfg.device)\n",
    "device = cfg.device\n",
    "\n",
    "print(encoder_q)\n",
    "\n",
    "# load model from checkpoint\n",
    "checkpoint = torch.load(cfg.checkpoint_file, map_location=cfg.device)['model_state_dict']\n",
    "encoder_q_keys = [k for k in list(checkpoint.keys()) if 'encoder_q' in k]\n",
    "\n",
    "new_checkpoint = {}\n",
    "for k in encoder_q_keys:\n",
    "    new_k = k.replace('clmodel.encoder_q.', '')\n",
    "    new_checkpoint[new_k] = checkpoint[k]\n",
    "\n",
    "encoder_q.load_state_dict(new_checkpoint)\n",
    "encoder_q.eval()\n",
    "print(\"Model loaded from checkpoint.\")\n",
    "\n",
    "embs_parent = pickle.load(open(cfg.dataset_embs_file_parent, 'rb')).to('cpu').detach() # tensor\n",
    "embs_child = pickle.load(open(cfg.dataset_embs_file_child, 'rb')).to('cpu').detach() # tensor\n",
    "cellspace_parent = pickle.load(open(cfg.dataset_cell_file_parent, 'rb'))\n",
    "cellspace_child = pickle.load(open(cfg.dataset_cell_file_child, 'rb'))\n",
    "hier_cellspace = HirearchicalCellSpace(cellspace_parent, cellspace_child)\n",
    "\n",
    "def model_forward(trajs1_emb, trajs1_emb_p, trajs1_len, time_deltas1):\n",
    "    max_trajs1_len = trajs1_len.max().item() # trajs1_len[0]\n",
    "    src_padding_mask1 = torch.arange(max_trajs1_len, device = cfg.device)[None, :] >= trajs1_len[:, None]\n",
    "    # traj_embs = self.clmodel.encoder_q(**{'src': trajs1_emb, 'time_indices': time_indices1, 'attn_mask': None, 'src_padding_mask': src_padding_mask1, 'src_len': trajs1_len, 'srcspatial': trajs1_emb_p})\n",
    "    traj_embs = encoder_q(**{'src': trajs1_emb, 'time_deltas': time_deltas1, 'attn_mask': None, 'src_padding_mask': src_padding_mask1, 'src_len': trajs1_len, 'srcspatial': trajs1_emb_p})\n",
    "    return traj_embs\n",
    "\n",
    "def infer_batch(traj, time_indices):\n",
    "    traj_cell_parent, traj_cell_child, traj_p, traj_timedelta = zip(*[merc2cell2(l[:800],t[:800], hier_cellspace) for l,t in zip(traj, time_indices)])\n",
    "    # print(traj_cell)\n",
    "    traj_emb_p = [torch.tensor(generate_spatial_features(t, hier_cellspace)) for t in traj_p]\n",
    "    traj_emb_p = pad_sequence(traj_emb_p, batch_first = False).to(device)\n",
    "    traj_emb_cell_parent = [embs_parent[list(t)] for t in traj_cell_parent]\n",
    "    traj_emb_cell_child = [embs_child[list(t)] for t in traj_cell_child]\n",
    "    traj_emb_cell = [a + b for a, b in zip(traj_emb_cell_parent, traj_emb_cell_child)]\n",
    "    traj_emb_cell = pad_sequence(traj_emb_cell, batch_first = False).to(device)\n",
    "    traj_len = torch.tensor(list(map(len, traj_p)), dtype = torch.long, device = device)\n",
    "    traj_timedelta = pad_sequence([torch.log(torch.tensor(t)) for t in traj_timedelta], batch_first=False, padding_value=0).to(cfg.device)\n",
    "    # print(traj_emb_cell, traj_emb_p, traj_len)\n",
    "    traj_embs = model_forward(traj_emb_cell.float(), traj_emb_p.float(), traj_len, traj_timedelta)\n",
    "    return traj_embs, traj_cell_parent, traj_cell_child , traj_p, traj_timedelta\n",
    "\n",
    "batch_size = cfg.batch_size\n",
    "# def infer(traj, time_indices):\n",
    "#     if len(traj)> batch_size:\n",
    "#         traj_embs = []\n",
    "#         for i in range(0, len(traj), batch_size):\n",
    "#             traj_batch = traj[i:i+batch_size]\n",
    "#             time_indices_batch = time_indices[i:i+batch_size] \n",
    "#             traj_embs.append(infer_batch(traj_batch, time_indices_batch))\n",
    "#         return torch.cat(traj_embs, dim=0)\n",
    "#     else:\n",
    "#         return infer_batch(traj, time_indices)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56421dd4-3da3-41a7-934f-7c6e8d1923ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "query = \"\"\"\n",
    "SELECT * FROM {} WHERE traj_date = current_date - 1\n",
    "\"\"\".format(cfg.traj_df_table_name)\n",
    "\n",
    "# query = \"\"\"\n",
    "# SELECT * FROM {} LIMIT 1000\n",
    "# \"\"\".format(cfg.traj_df_table_name)\n",
    "df = spark.sql(query)\n",
    "df_pd = df.toPandas()\n",
    "\n",
    "print(\"Length of the Dataframe {}\", str(len(df_pd)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfaaf911-8cf0-479f-8eaa-bb41d6379e79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emb_list = []\n",
    "\n",
    "for i in tqdm(range(0,len(df_pd), batch_size)):\n",
    "    traj = df_pd['merc_seq'].iloc[i:i+batch_size].tolist()\n",
    "    time_indices = df_pd['sorted_ts'].iloc[i:i+batch_size].tolist()\n",
    "    time_indices = [np.array(time_indices[i], dtype='datetime64[ns]') for i in range(len(time_indices))]\n",
    "    \n",
    "    traj_embs, traj_cell_parent, traj_cell_child, traj_p, traj_timedelta = infer_batch(traj, time_indices)\n",
    "\n",
    "    emb_list.append(traj_embs.detach().cpu())\n",
    "\n",
    "all_emb = torch.cat(emb_list, dim=0)\n",
    "\n",
    "print(\"final embedding shape\", all_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9f89c6-54e1-4f3a-82ce-22caa808246a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_new = pd.DataFrame()\n",
    "\n",
    "df_new['userid'] = df_pd['userid']\n",
    "df_new['traj_date'] = df_pd['traj_date']\n",
    "df_new['embedding'] = all_emb.tolist()\n",
    "df_new['model_version'] = cfg.model_version\n",
    "\n",
    "\n",
    "df_new.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2754adb-b4d4-4a6c-afd1-5391c8796c7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import *\n",
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField(\"userid\", IntegerType(), True),\n",
    "#     StructField(\"traj_date\", DateType(), True),\n",
    "#     StructField(\"embedding\", ArrayType(FloatType()), True),\n",
    "#     StructField(\"model_version\", StringType(), True)\n",
    "# ])\n",
    "\n",
    "# empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "# empty_df.write.mode(\"overwrite\").saveAsTable(\"main_prod.datascience_scratchpad.traj_emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07f6843f-16f3-480f-b3c2-36e8d797e8d2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"userid\":107},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758655339933}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_spark = spark.createDataFrame(df_new)\n",
    "display(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f82b3c08-81f3-4334-af19-529ea6d160e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_spark.createOrReplaceTempView('traj_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12d83671-7458-4788-99ec-41efc31c3309",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# MERGE INTO main_prod.ml_data.traj_emb AS target\n",
    "# USING traj_data AS source\n",
    "# ON target.userid = source.userid\n",
    "#    AND target.traj_date = source.traj_date\n",
    "# WHEN MATCHED THEN \n",
    "#   UPDATE SET *\n",
    "# WHEN NOT MATCHED THEN\n",
    "#   INSERT *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dd6cc18-780b-47ee-8ef6-dd2d13abfa34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"merge into {} t\n",
    "using traj_data s\n",
    "on t.userid = s.userid and t.traj_date = s.traj_date \n",
    "when matched then update set * \n",
    "when not matched then insert *\"\"\".format(cfg.traj_emb_table_name))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8862198372599306,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Trajectory Model Inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
