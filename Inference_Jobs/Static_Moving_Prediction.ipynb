{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d221668-911a-4c8a-a142-437219620d3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from config_infer import InferenceConfig\n",
    "cfg = InferenceConfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b66c78b-9d67-488c-9d0e-9c966299d73f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select * from main_prod.earnings_analysis.fact_user_earnings_daily where paydate >= current_date - 3 and paydate<current_date and total_pck_amt > 0\n",
    "\"\"\"\n",
    "\n",
    "df = spark.sql(query)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4768a4fa-e7c5-45e7-938e-9ae9e9b3cb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get userid employername and employerid\n",
    "\n",
    "\n",
    "userid_emp = df.select('userid','employerid').distinct()\n",
    "# display(userid_emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6b4980f-9bb7-42bc-81f8-1b297adf5db2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# userid_emp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ac6f92-2656-4a30-a455-3950354e0724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select userid, employerid, employername, paydate, prev_paydate, total_pck_amt from main_prod.earnings_analysis.fact_user_earnings_daily where paydate is not NULL and paydate<= current_date -1 \n",
    "\"\"\"\n",
    "\n",
    "facts_df = spark.sql(query)\n",
    "# display(facts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "878ef026-1829-434a-9544-cdf5b42d7f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "facts_df = facts_df.distinct()\n",
    "# display(facts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "005710f0-fc21-42cb-a1d4-f4ecebeae23f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "facts_df_fil= facts_df.join(userid_emp, ['userid','employerid'], \"inner\")\n",
    "# display(facts_df_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "792262f3-a4fc-4f40-b98e-fa623b9d40a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# facts_df_fil.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11d2f041-b315-4b9a-8e33-6e6a4a46c5c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for each userid, empolyerid pair keep the last 4 paydates and corresponding total_pck_amt\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "window = Window.partitionBy('userid','employerid').orderBy(desc('paydate'))\n",
    "facts_df_fil_2 = facts_df_fil.withColumn('rn', row_number().over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0318c113-dac4-4e5a-8159-cbc1f2cb304c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758794477698}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "traj_emb_df = spark.read.table(cfg.traj_emb_table_name)\n",
    "\n",
    "display(traj_emb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8854e2c1-4374-447d-95a3-4ac71b2a0810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joined = (\n",
    "    facts_df_fil_2.join(\n",
    "        broadcast(traj_emb_df),  # remove broadcast() if df1 is large\n",
    "        (facts_df_fil_2.userid == traj_emb_df.userid)\n",
    "        & (traj_emb_df.traj_date >= facts_df_fil_2.prev_paydate)   # for closed interval use <= below\n",
    "        & (traj_emb_df.traj_date < facts_df_fil_2.paydate),\n",
    "        \"left\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a9da4e9-48e7-4669-bb94-2f1603a28c29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "facts_df_count = (\n",
    "    joined.groupBy(facts_df_fil_2.userid, facts_df_fil_2.prev_paydate, facts_df_fil_2.paydate, facts_df_fil_2.rn)\n",
    "          .agg(F.count(traj_emb_df.traj_date).alias(\"count\"))\n",
    "          .select(\"userid\", \"prev_paydate\", \"paydate\", \"rn\", \"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "286b9119-6e11-4267-be4a-4663c3ad88a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def filter_by_count(prev_date, current_date, count):\n",
    "    n_days = (current_date - prev_date).days\n",
    "    if count >= n_days*0.5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "df_filtered_by_count = (\n",
    "    facts_df_count\n",
    "    .filter(F.udf(filter_by_count, returnType=BooleanType())(col(\"prev_paydate\"), col(\"paydate\"), col(\"count\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4732f163-fdc5-4fa5-99b1-fe0cff57bc0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_facts_df = facts_df_fil_2.join(df_filtered_by_count, [\"userid\", \"prev_paydate\", \"paydate\", \"rn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b00f443f-2121-4475-98a8-0d9edeec8783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.functions import col\n",
    "df = (\n",
    "    traj_emb_df.join(\n",
    "        broadcast(final_facts_df),\n",
    "        (traj_emb_df.userid == final_facts_df.userid)\n",
    "        & (col(\"traj_date\") >= col(\"prev_paydate\"))\n",
    "        & (col(\"traj_date\") < col(\"paydate\")),\n",
    "        \"inner\"       # use \"left\" if you want to keep df2 rows without a matching interval\n",
    "    )\n",
    "    .select(traj_emb_df.userid, final_facts_df.employerid, final_facts_df.employername, traj_emb_df.traj_date, traj_emb_df.embedding, final_facts_df.paydate, final_facts_df.prev_paydate, final_facts_df.rn)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5636558d-72dd-45a4-bdf9-7d345dff5317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Disable all autologging\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "# Or disable just sklearn autologging\n",
    "mlflow.sklearn.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f31a9db-b0ab-406c-85c5-bc378b3250bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import json \n",
    "def apply_dbscan(embs, target_min_similarity=0.9):\n",
    "    eps = 1.0 - target_min_similarity    # cosine distance threshold\n",
    "    n_embs = embs.shape[0]\n",
    "    min_samples = 3\n",
    "    db = DBSCAN(eps=eps, min_samples=max(int(n_embs*0.2), min_samples), metric=\"cosine\", n_jobs=-1).fit(embs)\n",
    "    return db\n",
    "\n",
    "def get_cluster(df):\n",
    "    embeddings = np.stack(df['embedding'].values)\n",
    "    db = apply_dbscan(embeddings)\n",
    "    labels = db.labels_\n",
    "    cluster_dict = {}\n",
    "    date_label_dict = {}\n",
    "    \n",
    "    for i,label in enumerate(labels):\n",
    "        if label != -1:\n",
    "            if label in cluster_dict.keys():\n",
    "                cluster_dict[label]+=1\n",
    "            else:\n",
    "                cluster_dict[label]=1\n",
    "            date_label_dict[df.iloc[i]['traj_date'].strftime(\"%Y-%m-%d\")] = label\n",
    "    \n",
    "    return db, cluster_dict, date_label_dict\n",
    "\n",
    "\n",
    "def is_static(cluster_dict, traj_count):\n",
    "    cluster_count = 0\n",
    "    for key in cluster_dict.keys():\n",
    "        if cluster_dict[key] >= int(0.25*traj_count):\n",
    "            cluster_count += cluster_dict[key]\n",
    "    if cluster_count >= int(0.6*traj_count):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# def cluster_exists(db):\n",
    "#     labels = db.labels_\n",
    "#     for label in labels:\n",
    "#         if label != -1:\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "\n",
    "def dbscan_predict_all(db, X_train, X_new):\n",
    "    nn = NearestNeighbors(radius=db.eps, metric=db.metric).fit(X_train)\n",
    "    dists, idxs = nn.radius_neighbors(X_new, return_distance=True)\n",
    "    y = db.labels_\n",
    "    pred = np.full(len(X_new), -1, dtype=int)\n",
    "    for i, (di, ii) in enumerate(zip(dists, idxs)):\n",
    "        if len(ii) == 0: \n",
    "            continue\n",
    "        lbls, di = y[ii], di\n",
    "        mask = lbls != -1\n",
    "        if mask.any():\n",
    "            pred[i] = lbls[mask][np.argmin(di[mask])]\n",
    "    return pred\n",
    "    \n",
    "def works_on_weekends_fn(db, weekday_df, weekend_df):\n",
    "    if len(weekday_df) == 0 or len(weekend_df) == 0:\n",
    "        return False\n",
    "    weekday_embs = np.stack(weekday_df['embedding'].values)\n",
    "    weekend_embs = np.stack(weekend_df['embedding'].values)\n",
    "    pred = dbscan_predict_all(db, weekday_embs, weekend_embs)\n",
    "    # if 40% of pred is not -1, then it works on weekends\n",
    "    pred_not_neg = pred[pred != -1]\n",
    "    return len(pred_not_neg) > 0.4*len(pred)\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F, types as T\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------\n",
    "# Helper functions assumed to exist and be importable on workers:\n",
    "#   get_cluster(pdf) -> any\n",
    "#   cluster_exists(cluster) -> bool\n",
    "#   works_on_weekends_fn(cluster, weekday_pdf, weekend_pdf) -> bool\n",
    "# Ensure they are defined in the same file or available on PYTHONPATH for executors.\n",
    "# -----------------------\n",
    "\n",
    "# Output schema (adjust types if your real types differ)\n",
    "out_schema = T.StructType([\n",
    "    T.StructField(\"userid\", T.IntegerType(), False),\n",
    "    T.StructField(\"employerid\", T.IntegerType(), False),\n",
    "    T.StructField(\"employername\", T.StringType(), True),\n",
    "    T.StructField(\"predicted_work_type\", T.StringType(), True),\n",
    "    T.StructField(\"predicted_on\", T.DateType(), True),\n",
    "    T.StructField(\"works_on_weekends\", T.BooleanType(), True),\n",
    "    T.StructField(\"paydate\", T.DateType(), True),\n",
    "    T.StructField(\"old_latest_date_label_dict\", T.MapType(T.StringType(), T.IntegerType()), True),\n",
    "    T.StructField(\"latest_date_label_dict\", T.MapType(T.StringType(),  T.IntegerType()), True),\n",
    "    T.StructField(\"old_latest_weekday_df_length\", T.LongType(), True),\n",
    "    T.StructField(\"latest_weekday_df_length\", T.LongType(), True),\n",
    "])\n",
    "\n",
    "def convert_dict_keys_to_str(d):\n",
    "    if isinstance(d, dict):\n",
    "        return {str(k): v for k, v in d.items()}\n",
    "    return d\n",
    "\n",
    "def compute_static_moving(group_pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Runs your pandas logic for one (userid, employerid, employername) group.\n",
    "    This function executes on a Spark worker.\n",
    "    \"\"\"\n",
    "    import datetime\n",
    "    today = datetime.date.today()\n",
    "\n",
    "    # We only process a single group here\n",
    "    # Extract group keys (safe because it's a single group)\n",
    "    userid = group_pdf[\"userid\"].iloc[0]\n",
    "    employerid = group_pdf[\"employerid\"].iloc[0]\n",
    "    employername = group_pdf.get(\"employername\", pd.Series([None])).iloc[0]\n",
    "\n",
    "    # Ensure 'weekday' exists (create if your input doesn't have it)\n",
    "    if \"weekday\" not in group_pdf.columns:\n",
    "        # If traj date is present, you could compute it; otherwise expect it precomputed\n",
    "        if \"traj_date\" in group_pdf.columns:\n",
    "            group_pdf = group_pdf.copy()\n",
    "            group_pdf[\"weekday\"] = pd.to_datetime(group_pdf[\"traj_date\"]).dt.weekday\n",
    "        else:\n",
    "            raise ValueError(\"Missing 'weekday' column and no 'traj_date' to compute it from.\")\n",
    "    paycycle_len = (group_pdf[\"paydate\"].iloc[-1] - group_pdf[\"prev_paydate\"].iloc[-1]).days\n",
    "    if paycycle_len <=8: \n",
    "        past_cycle_to_consider = 4\n",
    "    elif paycycle_len >8 and paycycle_len <=15:\n",
    "        past_cycle_to_consider = 2\n",
    "    else:\n",
    "        past_cycle_to_consider = 1\n",
    "    target_n_cycles = 1\n",
    "    outputs = []\n",
    "\n",
    "    # # Convert to proper dtypes just in case (optional but helpful)\n",
    "    # # Expect paydate to be date-like if coming from Spark DateType\n",
    "    # if not pd.api.types.is_datetime64_any_dtype(group_pdf[\"paydate\"]):\n",
    "    #     group_pdf = group_pdf.copy()\n",
    "    #     group_pdf[\"paydate\"] = pd.to_datetime(group_pdf[\"paydate\"]).dt.date\n",
    "\n",
    "    # Loop cycles (kept from your original logic; target_n_cycles = 1)\n",
    "    for target_rn in range(target_n_cycles):\n",
    "        target_paycycle = target_rn + 1\n",
    "\n",
    "        # Latest cycle slices\n",
    "        latest_weekday_df = group_pdf.loc[\n",
    "            (group_pdf[\"weekday\"].isin([0,1,2,3,4])) & (group_pdf[\"rn\"] == target_paycycle)\n",
    "        ].reset_index(drop=True)\n",
    "        latest_weekend_df = group_pdf.loc[\n",
    "            (group_pdf[\"weekday\"].isin([5,6])) & (group_pdf[\"rn\"] == target_paycycle)\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "        # Old cycles (NOTE: use bitwise & for pandas)\n",
    "        rn_mask_old = (group_pdf[\"rn\"] > target_paycycle) & (group_pdf[\"rn\"] <= target_paycycle + past_cycle_to_consider)\n",
    "        old_weekday_df = group_pdf.loc[\n",
    "            (group_pdf[\"weekday\"].isin([0,1,2,3,4])) & rn_mask_old\n",
    "        ].reset_index(drop=True)\n",
    "        old_weekend_df = group_pdf.loc[\n",
    "            (group_pdf[\"weekday\"].isin([5,6])) & rn_mask_old\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "        if len(latest_weekday_df) == 0:\n",
    "            continue\n",
    "        old_latest_df = pd.concat([latest_weekday_df, old_weekday_df], ignore_index=True).reset_index(drop=True)\n",
    "        if len(old_latest_df)< 0.7*paycycle_len:\n",
    "            continue\n",
    "        old_latest_date_label_dict = {}\n",
    "        old_latest_cluster, old_latest_cluster_dict,  old_latest_date_label_dict = get_cluster(old_latest_df)\n",
    "        latest_date_label_dict = {}\n",
    "        if is_static(old_latest_cluster_dict, len(old_latest_df)):\n",
    "            work_type = \"static\"\n",
    "            works_on_weekends = works_on_weekends_fn(\n",
    "                old_latest_cluster,\n",
    "                old_latest_df,\n",
    "                pd.concat([latest_weekend_df, old_weekend_df], ignore_index=True),\n",
    "            )\n",
    "        else:\n",
    "            latest_cluster, latest_cluster_dict, latest_date_label_dict = get_cluster(latest_weekday_df)\n",
    "            if is_static(latest_cluster_dict, len(latest_weekday_df)):\n",
    "                work_type = \"static\"\n",
    "                works_on_weekends = works_on_weekends_fn(\n",
    "                    latest_cluster, latest_weekday_df, latest_weekend_df\n",
    "                )\n",
    "            else:\n",
    "                work_type = \"moving\"\n",
    "                works_on_weekends = False\n",
    "\n",
    "        # If multiple rows exist for the latest cycle, choose a representative paydate.\n",
    "        # Here we just take the first paydate from the latest cycle rows.\n",
    "        paydate_value = latest_weekday_df[\"paydate\"].iloc[0] if len(latest_weekday_df) else None\n",
    "        # convert dict to string\n",
    "        # old_latest_date_label_dict_str = convert_dict_keys_to_str(old_latest_date_label_dict)\n",
    "        # latest_date_label_dict_str = convert_dict_keys_to_str(latest_date_label_dict)\n",
    "\n",
    "        outputs.append({\n",
    "            \"userid\": userid,\n",
    "            \"employerid\": employerid,\n",
    "            \"employername\": employername,\n",
    "            \"predicted_work_type\": work_type,\n",
    "            \"predicted_on\": today,\n",
    "            \"works_on_weekends\": works_on_weekends,\n",
    "            \"paydate\": paydate_value,\n",
    "            \"old_latest_date_label_dict\": old_latest_date_label_dict,\n",
    "            \"latest_date_label_dict\": latest_date_label_dict,\n",
    "            \"old_latest_weekday_df_length\": len(old_latest_df),\n",
    "            \"latest_weekday_df_length\": len(latest_weekday_df)\n",
    "        })\n",
    "\n",
    "    if outputs:\n",
    "        return pd.DataFrame(outputs, columns=[f.name for f in out_schema])\n",
    "    else:\n",
    "        # Return empty frame with correct columns if no output for this group\n",
    "        return pd.DataFrame(columns=[f.name for f in out_schema])\n",
    "\n",
    "# -----------------------\n",
    "# Run on Spark DataFrame\n",
    "# Assume `df` is your Spark DataFrame version of df_pd,\n",
    "# and it has the columns used above: userid, employerid, employername, weekday, rn, paydate (and/or traj_date if you need to compute weekday).\n",
    "# -----------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f553f09-0378-4423-aaee-89406427432e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_spark_df = (\n",
    "    df\n",
    "    .groupBy(\"userid\", \"employerid\")\n",
    "    .applyInPandas(compute_static_moving, schema=out_schema)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf82c45c-3735-4fe8-9af5-9c7bfa1fcfe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_spark_df.write.mode(\"overwrite\").parquet(\"/Volumes/main_prod/datascience_scratchpad/jatin/trajcl_exp/usa/static_moving_dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ae520db-960c-4b00-8417-d4da4ef79e56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = spark.read.parquet(\"/Volumes/main_prod/datascience_scratchpad/jatin/trajcl_exp/usa/static_moving_dummy\")\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dee6eef7-fef2-4aba-b31b-71977a63f83e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763272337009}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(result_df.select('paydate').groupBy('paydate').count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b68aec0-961f-4163-b27f-7eb18d022083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_count = result_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "823650db-4816-4352-b836-1c1352e5c445",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if output_count>0:\n",
    "    result_df.createOrReplaceTempView(\"new_data\")\n",
    "    spark.sql(\"\"\"MERGE INTO {} AS target USING new_data AS source ON target.userid = source.userid AND target.employerid = source.employerid AND target.paydate = source.paydate WHEN MATCHED THEN UPDATE SET * WHEN NOT MATCHED THEN INSERT *\"\"\".format(cfg.static_moving_table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2393001-cf03-4fe7-8ac7-a39d1706a59b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(*) from main_prod.ml_data.static_moving_worktype where paydate = current_date - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c99ffb1-194b-4248-9b9f-c8c544f07783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(*) from main_prod.ml_data.static_moving_worktype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56ee9761-6196-40d0-8fdd-114a27d48958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# output_df.where('predicted_work_type = \"moving\"').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93edf8ee-0aba-4f34-855a-8e9a1c4f1188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# userid = 23122754\n",
    "# df_pd[df_pd['userid']==userid].sort_values(\"traj_date\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cada198-b5af-47cc-836d-85407a100553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# test_df = df_pd[df_pd['userid']==userid].sort_values(\"traj_date\").reset_index(drop=True)\n",
    "# embs = np.stack(test_df['embedding'].to_list())\n",
    "# target_min_similarity = 0.9\n",
    "# eps = 1.0 - target_min_similarity    # cosine distance threshold\n",
    "# n_embs = embs.shape[0]\n",
    "# db = DBSCAN(eps=eps, min_samples=5, metric=\"cosine\", n_jobs=-1).fit(embs)\n",
    "# labels = db.labels_\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c76cbe59-26ac-4a88-98e3-087f11c648d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # print cosine similarity between all\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# for i in range(n_embs):\n",
    "#     for j in range(i+1, n_embs):\n",
    "#         print(test_df['traj_date'][i], test_df['traj_date'][j], cosine_similarity(embs[i].reshape(1,-1), embs[j].reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "962a7cac-f9ef-46af-845f-878b6e0f1c9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# delete table if exists main_prod.ml_data.static_moving_worktype\n",
    "# spark.sql(\"DROP TABLE IF EXISTS main_prod.ml_data.static_moving_worktype\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6532629-c5c2-4760-a3a6-c26aa1291945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7475833916655849,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Static_Moving_Prediction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
