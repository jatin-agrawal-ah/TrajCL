{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70ee4592-19b1-4fe1-a5e9-66c41228ddf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from config_infer import InferenceConfig\n",
    "cfg = InferenceConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40130cf6-bdfa-4f51-8beb-b2faceed160e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select userid, employerid, employername, paydate, prev_paydate, total_pck_amt from main_prod.earnings_analysis.fact_user_earnings_daily where paydate < current_date - 1 and total_pck_amt > 0\n",
    "\"\"\"\n",
    "\n",
    "facts_df = spark.sql(query)\n",
    "facts_df = facts_df.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "558c1bba-d2f9-4d8b-9381-1884f7cd6986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "traj_emb_df = spark.read.table(cfg.traj_emb_table_name)\n",
    "\n",
    "display(traj_emb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4be9c69-54b9-4c1c-9c4c-4c3cb48603af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select distinct(userid) from {} where traj_date = current_date - 1\n",
    "\"\"\".format(cfg.traj_emb_table_name)\n",
    "\n",
    "userids = spark.sql(query)\n",
    "display(userids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "798d76d2-34f0-402a-88ee-4825082d1277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "facts_df_fil = facts_df.join(userids, on='userid', how='inner')\n",
    "# display(facts_df_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0389b867-c702-42a9-94b2-e364e78b347e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for each userid, empolyerid pair keep the last 4 paydates and corresponding total_pck_amt\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "window = Window.partitionBy('userid','employerid').orderBy(desc('prev_paydate'))\n",
    "facts_df_fil_2 = facts_df_fil.withColumn('rn', row_number().over(window))\n",
    "facts_df_fil_2 = facts_df_fil_2.where('rn <= 4')\n",
    "# display(facts_df_fil_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d1fc8d9-9b11-494f-92c3-da01eb9a991a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joined = (\n",
    "    facts_df_fil_2.join(\n",
    "        broadcast(traj_emb_df),  # remove broadcast() if df1 is large\n",
    "        (facts_df_fil_2.userid == traj_emb_df.userid)\n",
    "        & (traj_emb_df.traj_date >= facts_df_fil_2.prev_paydate)   # for closed interval use <= below\n",
    "        & (traj_emb_df.traj_date < facts_df_fil_2.paydate),\n",
    "        \"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "facts_df_count = (\n",
    "    joined.groupBy(facts_df_fil_2.userid, facts_df_fil_2.prev_paydate, facts_df_fil_2.paydate, facts_df_fil_2.rn)\n",
    "          .agg(F.count(traj_emb_df.traj_date).alias(\"count\"))\n",
    "          .select(\"userid\", \"prev_paydate\", \"paydate\", \"rn\", \"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b52b725b-d108-4a2d-9bbe-baaf089a388d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(facts_df_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3ff8243-3083-481b-891d-309d8c150b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def filter_by_count(prev_date, current_date, count):\n",
    "    n_days = (current_date - prev_date).days\n",
    "    if count >= n_days*0.6:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "df_filtered_by_count = (\n",
    "    facts_df_count\n",
    "    .filter(F.udf(filter_by_count, returnType=BooleanType())(col(\"prev_paydate\"), col(\"paydate\"), col(\"count\")))\n",
    ")\n",
    "# display(df_filtered_by_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7b67da3-8948-4e00-8251-3762acf96a52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_facts_df = facts_df_fil_2.join(df_filtered_by_count, [\"userid\", \"prev_paydate\", \"paydate\", \"rn\"])\n",
    "# display(final_facts_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cccda6d7-a1ad-44c3-919c-ad93a85552e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.functions import col\n",
    "df = (\n",
    "    traj_emb_df.join(\n",
    "        broadcast(final_facts_df),\n",
    "        (traj_emb_df.userid == final_facts_df.userid)\n",
    "        & (col(\"traj_date\") >= col(\"prev_paydate\"))\n",
    "        & (col(\"traj_date\") < col(\"paydate\")),\n",
    "        \"inner\"       # use \"left\" if you want to keep df2 rows without a matching interval\n",
    "    )\n",
    "    .select(traj_emb_df.userid, final_facts_df.employerid, final_facts_df.employername, traj_emb_df.traj_date, traj_emb_df.embedding, final_facts_df.rn)\n",
    ")\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89a0a1f7-43f9-49f2-bfda-1c62c5be4996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87865e92-631c-40b6-9eae-0e7acf43787b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pd = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d74d76-8ef1-4576-a56d-2a198168395d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a476804-211e-48ff-8153-c02139134620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "current_traj_df = traj_emb_df.where('traj_date  = current_date - 1')\n",
    "# display(current_traj_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1a27b20-61f5-4add-86f9-9d3fc8b0e6d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "current_traj_df_pd = current_traj_df.toPandas()\n",
    "current_traj_df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fbba586-2b7a-4813-8753-6aa98ed78377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN \n",
    "def apply_dbscan(embs, target_min_similarity=0.9):\n",
    "    eps = 1.0 - target_min_similarity    # cosine distance threshold\n",
    "    n_embs = embs.shape[0]\n",
    "    db = DBSCAN(eps=eps, min_samples=max(int(n_embs*0.25), 4), metric=\"cosine\", n_jobs=-1).fit(embs)\n",
    "    return db\n",
    "\n",
    "def get_cluster(df):\n",
    "    embeddings = np.stack(df['embedding'].values)\n",
    "    db = apply_dbscan(embeddings)\n",
    "    return db\n",
    "\n",
    "def cluster_exists(db):\n",
    "    labels = db.labels_\n",
    "    for label in labels:\n",
    "        if label != -1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def dbscan_predict_all(db, X_train, X_new):\n",
    "    nn = NearestNeighbors(radius=db.eps, metric=db.metric).fit(X_train)\n",
    "    dists, idxs = nn.radius_neighbors(X_new, return_distance=True)\n",
    "    y = db.labels_\n",
    "    pred = np.full(len(X_new), -1, dtype=int)\n",
    "    for i, (di, ii) in enumerate(zip(dists, idxs)):\n",
    "        if len(ii) == 0: \n",
    "            continue\n",
    "        lbls, di = y[ii], di\n",
    "        mask = lbls != -1\n",
    "        if mask.any():\n",
    "            pred[i] = lbls[mask][np.argmin(di[mask])]\n",
    "    if pred[0]==-1:\n",
    "        return 0\n",
    "    return 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d93e444-1be0-44bd-b6cc-b797a7bd63bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faf7078d-ccf7-4f20-8344-02592cc78181",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "current_time = datetime.now()\n",
    "current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea28e7d3-09a0-46db-aa4c-097fcb0282ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pd['weekday'] = df_pd['traj_date'].apply(lambda x: x.weekday())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb6f186c-5e4b-4f88-b11e-6da42a936469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Disable all autologging\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "# Or disable just sklearn autologging\n",
    "mlflow.sklearn.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "599cc9e4-3ad0-408f-b867-dae11c0d6d5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "output_list = []\n",
    "for i in tqdm(current_traj_df_pd.index):\n",
    "    row = current_traj_df_pd.iloc[i]\n",
    "    user_df = df_pd[df_pd.userid == row.userid]\n",
    "    if len(user_df)==0:\n",
    "        continue\n",
    "    weekday_df = user_df[user_df.weekday < 5]\n",
    "    # weekend_df = user_df[user_df.traj_date.dt.weekday >= 5]\n",
    "    latest_weekday_df = weekday_df[weekday_df.rn == weekday_df.rn.min()]\n",
    "    # latest_weekend_df = weekend_df[weekend_df.rn == weekend_df.rn.min()]\n",
    "    db = get_cluster(weekday_df)\n",
    "    # print(row.embedding)\n",
    "    if cluster_exists(db):\n",
    "        pred = dbscan_predict_all(db, np.stack(weekday_df.embedding.values), np.expand_dims(row.embedding, axis=0))\n",
    "    else:\n",
    "        db = get_cluster(latest_weekday_df)\n",
    "        if cluster_exists(db):\n",
    "            pred = dbscan_predict_all(db, np.stack(latest_weekday_df.embedding.values), np.expand_dims(row.embedding, axis=0))\n",
    "        else:\n",
    "            pred = 2\n",
    "    output = {\n",
    "        \"userid\": int(row.userid),\n",
    "        \"employerid\": int(user_df.employerid.iloc[0]),\n",
    "        \"employername\": user_df.employername.iloc[0],\n",
    "        \"traj_date\": row.traj_date,\n",
    "        \"predicted_on\": current_time,\n",
    "        \"went_to_work\": pred,\n",
    "        \"model_version\": row.model_version\n",
    "    }\n",
    "    output_list.append(output)\n",
    "    # print(output)\n",
    "    # break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3998195-0c61-4a04-88b0-efb9a904d8d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # # # create empty table in df\n",
    "\n",
    "# from pyspark.sql.types import *\n",
    "# from pyspark.sql.functions import *\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField(\"userid\", IntegerType(), True),\n",
    "#     StructField(\"employerid\", IntegerType(), True),\n",
    "#     StructField(\"employername\", StringType(), True),\n",
    "#     StructField(\"traj_date\", DateType(), True),\n",
    "#     StructField(\"predicted_on\", TimestampType(), True),\n",
    "#     StructField(\"went_to_work\", IntegerType(), True),\n",
    "#     StructField(\"model_version\", StringType(), True),\n",
    "# ])\n",
    "\n",
    "# empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "# empty_df.write.mode(\"overwrite\").saveAsTable(\"main_prod.datascience_scratchpad.went_to_work_traj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "981e259a-d2a2-4792-8191-a1bac50f63f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a df from output_list\n",
    "\n",
    "output_df = spark.createDataFrame(output_list)\n",
    "\n",
    "output_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84e62b5a-dd4b-4508-b295-0d726c989a06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "092a3999-2fdb-4a59-a045-d2e9f9764029",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "output_df = output_df.withColumn(\"userid\", col(\"userid\").cast(IntegerType()))\n",
    "output_df = output_df.withColumn(\"employerid\", col(\"employerid\").cast(IntegerType()))\n",
    "output_df = output_df.withColumn(\"went_to_work\", col(\"went_to_work\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5fdb8bb-806b-4a57-98ac-127cd310b095",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_df.write.mode(\"append\").saveAsTable(cfg.went_to_work_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88814700-66f4-479e-b30e-1eafc3c45e6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "went_to_work_prediction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
