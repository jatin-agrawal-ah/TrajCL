{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "624e1fd9-f81e-48c4-9ae8-3894a0b5f745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"pip install -r requirements.txt\")\n",
    "os.system(\"pip install tqdm\")\n",
    "\n",
    "from config_infer import InferenceConfig\n",
    "from model.dual_attention import DualSTBTimeWeighted\n",
    "from utils.traj import *\n",
    "import pickle\n",
    "from utils.cellspace import *\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "cfg = InferenceConfig()\n",
    "# print(cfg.to_str())\n",
    "\n",
    "print(cfg.checkpoint_file)\n",
    "encoder_q = DualSTBTimeWeighted(cfg.seq_embedding_dim, \n",
    "                                            cfg.trans_hidden_dim, \n",
    "                                            cfg.trans_attention_head, \n",
    "                                            cfg.trans_attention_layer, \n",
    "                                            cfg.trans_attention_dropout, \n",
    "                                            cfg.trans_pos_encoder_dropout)\n",
    "\n",
    "encoder_q = encoder_q.to(cfg.device)\n",
    "device = cfg.device\n",
    "\n",
    "print(encoder_q)\n",
    "\n",
    "# load model from checkpoint\n",
    "checkpoint = torch.load(cfg.checkpoint_file, map_location=cfg.device)['model_state_dict']\n",
    "encoder_q_keys = [k for k in list(checkpoint.keys()) if 'encoder_q' in k]\n",
    "\n",
    "new_checkpoint = {}\n",
    "for k in encoder_q_keys:\n",
    "    new_k = k.replace('clmodel.encoder_q.', '')\n",
    "    new_checkpoint[new_k] = checkpoint[k]\n",
    "\n",
    "encoder_q.load_state_dict(new_checkpoint)\n",
    "encoder_q.eval()\n",
    "print(\"Model loaded from checkpoint.\")\n",
    "\n",
    "embs_parent = pickle.load(open(cfg.dataset_embs_file_parent, 'rb')).to('cpu').detach() # tensor\n",
    "embs_child = pickle.load(open(cfg.dataset_embs_file_child, 'rb')).to('cpu').detach() # tensor\n",
    "cellspace_parent = pickle.load(open(cfg.dataset_cell_file_parent, 'rb'))\n",
    "cellspace_child = pickle.load(open(cfg.dataset_cell_file_child, 'rb'))\n",
    "hier_cellspace = HirearchicalCellSpace(cellspace_parent, cellspace_child)\n",
    "\n",
    "def model_forward(trajs1_emb, trajs1_emb_p, trajs1_len, time_deltas1):\n",
    "    max_trajs1_len = trajs1_len.max().item() # trajs1_len[0]\n",
    "    src_padding_mask1 = torch.arange(max_trajs1_len, device = cfg.device)[None, :] >= trajs1_len[:, None]\n",
    "    # traj_embs = self.clmodel.encoder_q(**{'src': trajs1_emb, 'time_indices': time_indices1, 'attn_mask': None, 'src_padding_mask': src_padding_mask1, 'src_len': trajs1_len, 'srcspatial': trajs1_emb_p})\n",
    "    traj_embs = encoder_q(**{'src': trajs1_emb, 'time_deltas': time_deltas1, 'attn_mask': None, 'src_padding_mask': src_padding_mask1, 'src_len': trajs1_len, 'srcspatial': trajs1_emb_p})\n",
    "    return traj_embs\n",
    "\n",
    "def infer_batch(traj, time_indices):\n",
    "    traj_cell_parent, traj_cell_child, traj_p, traj_timedelta = zip(*[merc2cell2(l[:800],t[:800], hier_cellspace) for l,t in zip(traj, time_indices)])\n",
    "    # print(traj_cell)\n",
    "    traj_emb_p = [torch.tensor(generate_spatial_features(t, hier_cellspace)) for t in traj_p]\n",
    "    traj_emb_p = pad_sequence(traj_emb_p, batch_first = False).to(device)\n",
    "    traj_emb_cell_parent = [embs_parent[list(t)] for t in traj_cell_parent]\n",
    "    traj_emb_cell_child = [embs_child[list(t)] for t in traj_cell_child]\n",
    "    traj_emb_cell = [a + b for a, b in zip(traj_emb_cell_parent, traj_emb_cell_child)]\n",
    "    traj_emb_cell = pad_sequence(traj_emb_cell, batch_first = False).to(device)\n",
    "    traj_len = torch.tensor(list(map(len, traj_p)), dtype = torch.long, device = device)\n",
    "    traj_timedelta = pad_sequence([torch.log(torch.tensor(t)) for t in traj_timedelta], batch_first=False, padding_value=0).to(cfg.device)\n",
    "    # print(traj_emb_cell, traj_emb_p, traj_len)\n",
    "    traj_embs = model_forward(traj_emb_cell.float(), traj_emb_p.float(), traj_len, traj_timedelta)\n",
    "    return traj_embs, traj_cell_parent, traj_cell_child , traj_p, traj_timedelta\n",
    "\n",
    "batch_size = cfg.batch_size\n",
    "# def infer(traj, time_indices):\n",
    "#     if len(traj)> batch_size:\n",
    "#         traj_embs = []\n",
    "#         for i in range(0, len(traj), batch_size):\n",
    "#             traj_batch = traj[i:i+batch_size]\n",
    "#             time_indices_batch = time_indices[i:i+batch_size] \n",
    "#             traj_embs.append(infer_batch(traj_batch, time_indices_batch))\n",
    "#         return torch.cat(traj_embs, dim=0)\n",
    "#     else:\n",
    "#         return infer_batch(traj, time_indices)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9e7df88-bf54-4afe-9f3b-17d41e50297a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import current_date\n",
    "# query = \"\"\"\n",
    "# SELECT * FROM {} \"\"\".format(cfg.traj_df_table_name)\n",
    "\n",
    "# # query = \"\"\"\n",
    "# # SELECT * FROM {} LIMIT 1000\n",
    "# # \"\"\".format(cfg.traj_df_table_name)\n",
    "# df = spark.sql(query)\n",
    "\n",
    "# df.count()\n",
    "from glob import glob\n",
    "\n",
    "files = glob(\"/Volumes/main_prod/datascience_scratchpad/jatin/trajcl_exp/usa/last_20_days/*.parquet\")\n",
    "\n",
    "print(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53ce86a3-385b-407f-ab0d-eb85afe6d007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "for file in tqdm(files):\n",
    "    df_pd = pd.read_parquet(file)\n",
    "    emb_list = []\n",
    "    print(\"Started Inference\")\n",
    "    for i in range(0,len(df_pd), batch_size):\n",
    "        traj = df_pd['merc_seq'].iloc[i:i+batch_size].tolist()\n",
    "        time_indices = df_pd['sorted_ts'].iloc[i:i+batch_size].tolist()\n",
    "        time_indices = [np.array(time_indices[i], dtype='datetime64[ns]') for i in range(len(time_indices))]\n",
    "        \n",
    "        traj_embs, traj_cell_parent, traj_cell_child, traj_p, traj_timedelta = infer_batch(traj, time_indices)\n",
    "\n",
    "        emb_list.append(traj_embs.detach().cpu())\n",
    "\n",
    "    all_emb = torch.cat(emb_list, dim=0)\n",
    "    # drop all columns except userid and traj_date\n",
    "    df_pd = df_pd[['userid', 'traj_date']]\n",
    "    df_pd['embedding'] = all_emb.tolist()\n",
    "    df_pd['model_version'] = cfg.model_version\n",
    "    print(\"Pandas df created\")\n",
    "    df_spark = spark.createDataFrame(df_pd)\n",
    "    print(\"Spark df created\")\n",
    "    df_spark.createOrReplaceTempView('traj_data')\n",
    "    print(\"Stared update\")\n",
    "    query = \"\"\"\n",
    "    MERGE INTO main_prod.datascience_scratchpad.traj_emb AS target\n",
    "    USING traj_data AS source\n",
    "    ON target.userid = source.userid\n",
    "    AND target.traj_date = source.traj_date\n",
    "    WHEN MATCHED THEN \n",
    "    UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN\n",
    "    INSERT *\n",
    "        \"\"\"\n",
    "    _sqldf = spark.sql(query)\n",
    "    print(\"Spark Updated\")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05d566c8-6135-4000-bc15-05715d577bcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "traj_emb_inf_backfill",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
