{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../model\")\n",
    "from trajcl import TrajCL\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Config()\n",
    "\n",
    "conf.dataset = 'usa_large_cell'\n",
    "conf.post_value_updates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrajCL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ah_databricks_data_loader import DatabricksDataLoader\n",
    "\n",
    "# # Instantiate the DatabricksDataLoader.\n",
    "# ddl = DatabricksDataLoader()\n",
    "\n",
    "# # Load data.\n",
    "# test_df = ddl.load_as_spark(schema=\"datascience_scratchpad\", table=\"nyc_traj_data_v18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.write.mode(\"overwrite\").parquet(\"/home/sagemaker-user/TrajCL/data/nyc/nyc_data_v18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "parquet_files = glob(\"/home/sagemaker-user/TrajCL/data/nyc/nyc_data_v18/*.parquet\")\n",
    "total_count = 0\n",
    "df_list = []\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(file)\n",
    "    df_list.append(df)\n",
    "    total_count += len(df)\n",
    "\n",
    "\n",
    "print(f\"Total records in repartitioned files: {total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# test_df = pd.read_parquet(\"/home/sagemaker-user/TrajCL/data/parquet_files/test/nyc_df_v3_with_time/traj_test_df_v3_with_ts.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# userids = test_df['userid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\")\n",
    "checkpoint_file = \"/home/sagemaker-user/TrajCL/exp/nyc_hier_time_weighted_v2/ep2_batch10000\"\n",
    "checkpoint = torch.load(checkpoint_file)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.traj import *\n",
    "import pickle\n",
    "from utils.cellspace import *\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "embs_parent = pickle.load(open(Config.dataset_embs_file_parent, 'rb')).to('cpu').detach() # tensor\n",
    "embs_child = pickle.load(open(Config.dataset_embs_file_child, 'rb')).to('cpu').detach() # tensor\n",
    "cellspace_parent = pickle.load(open(Config.dataset_cell_file_parent, 'rb'))\n",
    "cellspace_child = pickle.load(open(Config.dataset_cell_file_child, 'rb'))\n",
    "hier_cellspace = HirearchicalCellSpace(cellspace_parent, cellspace_child)\n",
    "\n",
    "max_batch_size = 512\n",
    "def infer_batch(traj, time_indices):\n",
    "    traj_cell_parent, traj_cell_child, traj_p, traj_timedelta = zip(*[merc2cell2(l,t, hier_cellspace) for l,t in zip(traj, time_indices)])\n",
    "    # print(traj_cell)\n",
    "    traj_emb_p = [torch.tensor(generate_spatial_features(t, hier_cellspace)) for t in traj_p]\n",
    "    traj_emb_p = pad_sequence(traj_emb_p, batch_first = False).to(device)\n",
    "    traj_emb_cell_parent = [embs_parent[list(t)] for t in traj_cell_parent]\n",
    "    traj_emb_cell_child = [embs_child[list(t)] for t in traj_cell_child]\n",
    "    traj_emb_cell = [a + b for a, b in zip(traj_emb_cell_parent, traj_emb_cell_child)]\n",
    "    traj_emb_cell = pad_sequence(traj_emb_cell, batch_first = False).to(device)\n",
    "    traj_len = torch.tensor(list(map(len, traj_p)), dtype = torch.long, device = device)\n",
    "    traj_timedelta = pad_sequence([torch.log(torch.tensor(t)) for t in traj_timedelta], batch_first=False, padding_value=0).to(Config.device)\n",
    "    # print(traj_emb_cell, traj_emb_p, traj_len)\n",
    "    traj_embs = model.interpret(traj_emb_cell.float(), traj_emb_p.float(), traj_len, traj_timedelta)\n",
    "    return traj_embs, traj_cell_parent, traj_cell_child , traj_p, traj_timedelta\n",
    "\n",
    "def infer(traj, time_indices):\n",
    "    if len(traj)> max_batch_size:\n",
    "        traj_embs = []\n",
    "        for i in range(0, len(traj), max_batch_size):\n",
    "            traj_batch = traj[i:i+max_batch_size]\n",
    "            time_indices_batch = time_indices[i:i+max_batch_size] \n",
    "            traj_embs.append(infer_batch(traj_batch, time_indices_batch))\n",
    "        return torch.cat(traj_embs, dim=0)\n",
    "    else:\n",
    "        return infer_batch(traj, time_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[0]['time_index_list'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_userids = pd.concat([df['userid'] for df in df_list]).unique()\n",
    "print(f\"Total unique userids: {len(unique_userids)}\")\n",
    "\n",
    "unique_employernames = pd.concat([df['employername'] for df in df_list]).unique()\n",
    "print(f\"Total unique employernames: {len(unique_employernames)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_userid(userid):\n",
    "    df_user = pd.concat([df[df['userid']==userid] for df in df_list]).reset_index(drop=True)\n",
    "    return df_user\n",
    "\n",
    "def get_data_for_employername(df_user, employername):\n",
    "    df_emp = df_user[df_user['employername']==employername].reset_index(drop=True)\n",
    "    return df_emp\n",
    "\n",
    "\n",
    "def get_data_for_partition(df_emp, partition):\n",
    "    df_part = df_emp[df_emp['partition_id']==partition].reset_index(drop=True)\n",
    "    df_part = df_part[~df_part['weekday'].isin([5, 6])].reset_index(drop=True) # filter out weekends\n",
    "    df_part = df_part[df_part['pck_amt']>0].reset_index(drop=True) # filter out zero paycheck amount\n",
    "    return df_part\n",
    "\n",
    "\n",
    "def get_traj_and_time_data(df_part):\n",
    "    traj = df_part['merc_seq'].values\n",
    "    time_indices = df_part['timestamps'].values\n",
    "    return traj, time_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_min_max_xy(traj):\n",
    "#     min_x = min([p[0] for p in traj])\n",
    "#     max_x = max([p[0] for p in traj])\n",
    "#     min_y = min([p[1] for p in traj])\n",
    "#     max_y = max([p[1] for p in traj])\n",
    "#     return min_x, max_x, min_y, max_y\n",
    "\n",
    "# def calculate_dx_dy(k, target_min_x, target_min_y, min_x, max_x, min_y, max_y):\n",
    "#     dx = target_min_x+ (k - max_x - min_x) / 2\n",
    "#     dy = target_min_y+ (k - max_y - min_y) / 2\n",
    "#     return dx, dy\n",
    "\n",
    "# def transform_traj(traj, dx, dy):\n",
    "#     new_traj = [[p[0] + dx, p[1] + dy] for p in traj]\n",
    "#     return new_traj\n",
    "\n",
    "# def normalize(trajs, cellspace):\n",
    "#     # trajs: list of [[lon, lat], [,], ...]\n",
    "\n",
    "#     # 1. augment the input traj in order to form 2 augmented traj views\n",
    "#     # 2. convert augmented trajs to the trajs based on mercator space by cells\n",
    "#     # 3. read cell embeddings and form batch tensors (sort, pad)\n",
    "\n",
    "#     trajs1, trajs2 = [], []\n",
    "#     time_indices1, time_indices2 = [], []\n",
    "#     final_min_x = 1000000000\n",
    "#     final_min_y = 1000000000\n",
    "#     final_max_x = -1000000000\n",
    "#     final_max_y = -1000000000\n",
    "#     max_len_meters = min(cellspace.x_max-cellspace.x_min, cellspace.y_max-cellspace.y_min)\n",
    "#     indices_to_omit = []\n",
    "#     for i, l in enumerate(trajs):\n",
    "#         min_x, max_x, min_y, max_y = get_min_max_xy(l)\n",
    "#         # if final_max_x - min_x> max_len_meters or final_max_y-min_y> max_len_meters or max_x - final_min_x> max_len_meters or max_y - final_min_y> max_len_meters or max_x-min_x> max_len_meters or max_y-min_y> max_len_meters:\n",
    "#         #     indices_to_omit.append(i)\n",
    "#         #     continue\n",
    "#         final_min_x = min(final_min_x, min_x)\n",
    "#         final_min_y = min(final_min_y, min_y)\n",
    "#         final_max_x = max(final_max_x, max_x)\n",
    "#         final_max_y = max(final_max_y, max_y)\n",
    "    \n",
    "#     if max(final_max_x-final_min_x, final_max_y-final_min_y)>max_len_meters:\n",
    "#         # print(max(final_max_x-final_min_x, final_max_y-final_min_y), max_len_meters)\n",
    "#         return None\n",
    "#     dx, dy = calculate_dx_dy(max_len_meters, cellspace.x_min, cellspace.y_min, final_min_x, final_max_x, final_min_y, final_max_y)\n",
    "#     # print(final_min_x, final_max_x, final_min_y, final_max_y)\n",
    "#     # print(dx, dy)\n",
    "#     for i,l in enumerate(trajs):\n",
    "#         if i in indices_to_omit:\n",
    "#             continue\n",
    "#         new_l = transform_traj(l, dx, dy)\n",
    "#         trajs1.append(new_l)\n",
    "#     return trajs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dbscan(embs, target_min_similarity=0.85):\n",
    "    from sklearn.cluster import DBSCAN     # require >= 0.9 cosine similarity\n",
    "    eps = 1.0 - target_min_similarity    # cosine distance threshold\n",
    "    n_embs = embs.shape[0]\n",
    "    db = DBSCAN(eps=eps, min_samples=max(int(n_embs*0.3), 5), metric=\"cosine\", n_jobs=-1).fit(embs)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "output_dict = {}\n",
    "\n",
    "# 0: moving, 1: static, 2: not enough data\n",
    "def get_gt_and_pred_label(userid):\n",
    "    count = 0\n",
    "    user_data = get_data_for_userid(userid)\n",
    "    employers = user_data['employername'].unique()\n",
    "    for employer in employers:\n",
    "        emp_data = get_data_for_employername(user_data, employer)\n",
    "        partitions = emp_data['partition_id'].unique()\n",
    "        for partition in partitions:\n",
    "            # print(userid, employer, partition)\n",
    "            part_data = get_data_for_partition(emp_data, partition)\n",
    "            if len(part_data)<15:\n",
    "                continue\n",
    "            else:\n",
    "                # print(userid, employer, partition)\n",
    "                # print(len(part_data))\n",
    "                traj, time_indices = get_traj_and_time_data(part_data)\n",
    "                # print(traj)\n",
    "                embs, traj_cell_parent, traj_cell_child , traj_p, traj_timedelta = infer(traj, time_indices)\n",
    "                db_scan = apply_dbscan(embs.detach().cpu().numpy(), target_min_similarity=0.85)\n",
    "                labels = db_scan.labels_                      # shape: (n_samples,)\n",
    "                cluster_ids = [c for c in np.unique(labels) if c != -1]\n",
    "                if len(cluster_ids)>0:\n",
    "                    pred_label = 1\n",
    "                else:\n",
    "                    pred_label = 0\n",
    "                output_dict[(userid, employer, partition)] = pred_label\n",
    "\n",
    "    return count\n",
    "    # if sum(test_data['paycheck_amount'].values) > 0:\n",
    "    #     gt_label = 1\n",
    "    # else:\n",
    "    #     gt_label = 0\n",
    "    # pred_label = 0\n",
    "    # for i in range(len(test_embs)):\n",
    "    #     test_vector = test_embs[i].unsqueeze(0)\n",
    "    #     similarity = cosine_similarity(test_vector.numpy(), train_embs)[0]\n",
    "    #     top_3_indices = np.argsort(similarity)[-3:][::-1]\n",
    "    #     # print(i, top_3_indices)\n",
    "    #     similarity = similarity[top_3_indices]\n",
    "    #     # print(f\"User: {userid}, Test Trajectory {test_data['traj_id'].values[i]}, Top 3 Train Trajectories: {train_data['traj_id'].values[top_3_indices]}, similarity: {similarity}, PCK Amount: {train_data['paycheck_amount'].values[top_3_indices]}\")\n",
    "    #     for sim, idx in zip(similarity, top_3_indices):\n",
    "    #         if sim>0.85 and train_data['paycheck_amount'].values[idx]>0:\n",
    "    #             pred_label = 1\n",
    "    #             break\n",
    "    # return gt_label, pred_label\n",
    "count=0\n",
    "for i, userid in tqdm(enumerate(unique_userids)):\n",
    "    count+=get_gt_and_pred_label(userid)\n",
    "    if i>1000:\n",
    "        break\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output_dict\n",
    "pickle.dump(output_dict, open(\"/home/sagemaker-user/TrajCL/exp/v2.1/nyc_test_hyp1_output_dict.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict = {}\n",
    "for k,v in output_dict.items():\n",
    "    if v in count_dict:\n",
    "        count_dict[v]+=1\n",
    "    else:\n",
    "        count_dict[v]=1\n",
    "count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_userid_employer_partition(userid, employer, partition):\n",
    "    df_user = pd.concat([df[df['userid']==userid] for df in df_list]).reset_index(drop=True)\n",
    "    df_emp = df_user[df_user['employername']==employer].reset_index(drop=True)\n",
    "    df_part = df_emp[df_emp['partition_id']==partition].reset_index(drop=True)\n",
    "    df_part_fil = df_part[~df_part['weekday'].isin([5, 6])].reset_index(drop=True) # filter out weekends\n",
    "    df_part_fil = df_part_fil[df_part_fil['pck_amt']>0].reset_index(drop=True) # filter out zero paycheck amount\n",
    "    return df_part_fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict[(2523559, 'united ground express', 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part = get_data_userid_employer_partition(2523559, 'united ground express', 0)\n",
    "df_part.sort_values('traj_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity_matrix(X, eps=1e-8, dtype=np.float32):\n",
    "    X = np.asarray(X, dtype=dtype)\n",
    "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    Xn = X / np.maximum(norms, eps)\n",
    "    return Xn @ Xn.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster(part_df):\n",
    "    traj , time_indices= get_traj_and_time_data(part_df)\n",
    "    embs, _, _, _, _ = infer(traj,time_indices)\n",
    "    db_scan = apply_dbscan(embs.detach().cpu().numpy(), target_min_similarity=0.85)\n",
    "    labels = db_scan.labels_                      # shape: (n_samples,)\n",
    "    cluster_ids = [c for c in np.unique(labels) if c != -1]\n",
    "    for i in part_df.index:\n",
    "        print(labels[i], part_df.iloc[i]['traj_date'])\n",
    "    sim = cosine_similarity_matrix(embs.detach().cpu().numpy())\n",
    "    sim_pairs = []\n",
    "    for i in range(sim.shape[0]):\n",
    "        for j in range(i+1, sim.shape[1]):\n",
    "            if sim[i,j]>0.85:\n",
    "                sim_pairs.append((i, j, sim[i,j]))\n",
    "    return sim_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(part_df):\n",
    "    traj_dates = list(part_df['traj_date'])\n",
    "    traj , time_indices= get_traj_and_time_data(part_df)\n",
    "    # traj = normalize(traj, cellspace)\n",
    "    # print(traj)\n",
    "    embs, parent_cell, child_cell, traj_p, time_delta = infer(traj,time_indices)\n",
    "    similarity = cosine_similarity_matrix(embs.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "    return similarity, parent_cell, child_cell, traj_p, time_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cluster(df_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fil= df_part[df_part['traj_date'].isin([datetime.date(2024,9,19), datetime.date(2024,9,13)])]\n",
    "sim, parent_cell, child_cell, traj_p, time_delta = get_similarity(df_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_delta = time_delta.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted_indices_0 = list(np.argsort(time_delta[:,0]))[::-1]\n",
    "\n",
    "cell_parent_fil_0 = np.array(parent_cell[0])[sorted_indices_0[:10]]\n",
    "cell_fil_0 = np.array(child_cell[0])[sorted_indices_0[:10]]\n",
    "timestamps_0 = time_delta[sorted_indices_0[:10],0]\n",
    "\n",
    "sorted_indices_1 = list(np.argsort(time_delta[:,1]))[::-1]\n",
    "cell_parent_fil_1 = np.array(parent_cell[1])[sorted_indices_1[:10]]\n",
    "cell_fil_1 = np.array(child_cell[1])[sorted_indices_1[:10]]\n",
    "timestamps_1 = time_delta[sorted_indices_1[:10],1]\n",
    "\n",
    "\n",
    "for i in range(len(cell_fil_0)):\n",
    "    print(cell_parent_fil_0[i], cell_fil_0[i], timestamps_0[i], math.exp(timestamps_0[i]), cell_parent_fil_1[i], cell_fil_1[i], timestamps_1[i], math.exp(timestamps_1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted_indices_0)\n",
    "print(sorted_indices_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trajcl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
