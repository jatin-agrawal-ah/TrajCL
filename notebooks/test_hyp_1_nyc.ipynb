{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../model\")\n",
    "from trajcl import TrajCL\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Config()\n",
    "\n",
    "conf.dataset = 'nyc'\n",
    "conf.post_value_updates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.conda/envs/trajcl/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = TrajCL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrajCL(\n",
       "  (clmodel): MoCo(\n",
       "    (criterion): CrossEntropyLoss()\n",
       "    (encoder_q): DualSTB(\n",
       "      (pos_encoder): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (structural_attn): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (spatial_attn): SpatialMSM(\n",
       "        (pos_encoder): PositionalEncoding(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (trans_encoder): SpatialMSMEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-2): 3 x SpatialMSMLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=4, out_features=32, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (linear2): Linear(in_features=32, out_features=4, bias=True)\n",
       "              (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.1, inplace=False)\n",
       "              (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (encoder_k): DualSTB(\n",
       "      (pos_encoder): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (structural_attn): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (spatial_attn): SpatialMSM(\n",
       "        (pos_encoder): PositionalEncoding(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (trans_encoder): SpatialMSMEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-2): 3 x SpatialMSMLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=4, out_features=32, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (linear2): Linear(in_features=32, out_features=4, bias=True)\n",
       "              (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.1, inplace=False)\n",
       "              (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mlp_q): Projector(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (mlp_k): Projector(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/sagemaker-user/.conda/envs/trajcl/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/sagemaker-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/sagemaker-user/.ivy2/jars\n",
      "io.delta#delta-sharing-spark_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-179718a3-ae74-4574-8525-70d866752ae5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-sharing-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound io.delta#delta-sharing-client_2.12;1.0.5 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound commons-codec#commons-codec;1.11 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 380ms :: artifacts dl 20ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.11 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tio.delta#delta-sharing-client_2.12;1.0.5 from central in [default]\n",
      "\tio.delta#delta-sharing-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-179718a3-ae74-4574-8525-70d866752ae5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/10ms)\n",
      "25/09/08 18:17:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from ah_databricks_data_loader import DatabricksDataLoader\n",
    "\n",
    "# Instantiate the DatabricksDataLoader.\n",
    "ddl = DatabricksDataLoader()\n",
    "\n",
    "# Load data.\n",
    "test_df = ddl.load_as_spark(schema=\"datascience_scratchpad\", table=\"nyc_traj_data_v18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 18:15:33 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_df.write.mode(\"overwrite\").parquet(\"/home/sagemaker-user/TrajCL/data/nyc/nyc_data_v18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in repartitioned files: 1445257\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "parquet_files = glob(\"/home/sagemaker-user/TrajCL/data/nyc/nyc_data_v18/*.parquet\")\n",
    "total_count = 0\n",
    "df_list = []\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(file)\n",
    "    df_list.append(df)\n",
    "    total_count += len(df)\n",
    "\n",
    "\n",
    "print(f\"Total records in repartitioned files: {total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>traj_date</th>\n",
       "      <th>timestamps</th>\n",
       "      <th>wgs_seq</th>\n",
       "      <th>employername</th>\n",
       "      <th>partition_id</th>\n",
       "      <th>pck_amt</th>\n",
       "      <th>set_type</th>\n",
       "      <th>trajlen</th>\n",
       "      <th>merc_seq</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time_index_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5240</td>\n",
       "      <td>2025-05-12</td>\n",
       "      <td>[2025-05-12T11:23:06.000000000, 2025-05-12T11:...</td>\n",
       "      <td>[[-73.8074343, 40.909735], [-73.9277252, 40.81...</td>\n",
       "      <td>Trader Joe's</td>\n",
       "      <td>1</td>\n",
       "      <td>1478.72</td>\n",
       "      <td>train</td>\n",
       "      <td>30</td>\n",
       "      <td>[[-8216206.003033994, 4999036.699901179], [-82...</td>\n",
       "      <td>0</td>\n",
       "      <td>[68, 69, 71, 75, 76, 78, 79, 83, 89, 91, 98, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34299</td>\n",
       "      <td>2024-12-06</td>\n",
       "      <td>[2024-12-06T00:01:53.000000000, 2024-12-06T00:...</td>\n",
       "      <td>[[-74.0592517, 40.7562114], [-74.0598949, 40.7...</td>\n",
       "      <td>u s postal service</td>\n",
       "      <td>0</td>\n",
       "      <td>3026.97</td>\n",
       "      <td>train</td>\n",
       "      <td>64</td>\n",
       "      <td>[[-8244238.187774881, 4976449.130531165], [-82...</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1, 2, 4, 5, 7, 8, 9, 11, 12, 14, 15, 18, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34299</td>\n",
       "      <td>2025-02-15</td>\n",
       "      <td>[2025-02-15T00:00:06.000000000, 2025-02-15T00:...</td>\n",
       "      <td>[[-74.0596466, 40.7565547], [-74.0592928, 40.7...</td>\n",
       "      <td>u s postal service</td>\n",
       "      <td>0</td>\n",
       "      <td>4129.42</td>\n",
       "      <td>train</td>\n",
       "      <td>70</td>\n",
       "      <td>[[-8244282.147841794, 4976499.581186923], [-82...</td>\n",
       "      <td>5</td>\n",
       "      <td>[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 16, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35404</td>\n",
       "      <td>2024-11-08</td>\n",
       "      <td>[2024-11-08T00:34:19.000000000, 2024-11-08T06:...</td>\n",
       "      <td>[[-73.9452781, 40.7803969], [-73.9452432, 40.7...</td>\n",
       "      <td>Con edison</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>train</td>\n",
       "      <td>29</td>\n",
       "      <td>[[-8231550.704659003, 4980004.019033388], [-82...</td>\n",
       "      <td>4</td>\n",
       "      <td>[3, 36, 38, 39, 40, 41, 42, 44, 45, 46, 48, 51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54320</td>\n",
       "      <td>2025-05-06</td>\n",
       "      <td>[2025-05-06T02:00:27.000000000, 2025-05-06T07:...</td>\n",
       "      <td>[[-73.9059497, 40.8341471], [-73.9078743, 40.8...</td>\n",
       "      <td>Sterling Nat Bnk</td>\n",
       "      <td>0</td>\n",
       "      <td>642.83</td>\n",
       "      <td>train</td>\n",
       "      <td>28</td>\n",
       "      <td>[[-8227172.6871972885, 4987909.091900762], [-8...</td>\n",
       "      <td>1</td>\n",
       "      <td>[12, 47, 48, 50, 52, 54, 55, 56, 57, 59, 60, 6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid   traj_date                                         timestamps  \\\n",
       "0    5240  2025-05-12  [2025-05-12T11:23:06.000000000, 2025-05-12T11:...   \n",
       "1   34299  2024-12-06  [2024-12-06T00:01:53.000000000, 2024-12-06T00:...   \n",
       "2   34299  2025-02-15  [2025-02-15T00:00:06.000000000, 2025-02-15T00:...   \n",
       "3   35404  2024-11-08  [2024-11-08T00:34:19.000000000, 2024-11-08T06:...   \n",
       "4   54320  2025-05-06  [2025-05-06T02:00:27.000000000, 2025-05-06T07:...   \n",
       "\n",
       "                                             wgs_seq        employername  \\\n",
       "0  [[-73.8074343, 40.909735], [-73.9277252, 40.81...        Trader Joe's   \n",
       "1  [[-74.0592517, 40.7562114], [-74.0598949, 40.7...  u s postal service   \n",
       "2  [[-74.0596466, 40.7565547], [-74.0592928, 40.7...  u s postal service   \n",
       "3  [[-73.9452781, 40.7803969], [-73.9452432, 40.7...          Con edison   \n",
       "4  [[-73.9059497, 40.8341471], [-73.9078743, 40.8...    Sterling Nat Bnk   \n",
       "\n",
       "   partition_id  pck_amt set_type  trajlen  \\\n",
       "0             1  1478.72    train       30   \n",
       "1             0  3026.97    train       64   \n",
       "2             0  4129.42    train       70   \n",
       "3             0     0.00    train       29   \n",
       "4             0   642.83    train       28   \n",
       "\n",
       "                                            merc_seq  weekday  \\\n",
       "0  [[-8216206.003033994, 4999036.699901179], [-82...        0   \n",
       "1  [[-8244238.187774881, 4976449.130531165], [-82...        4   \n",
       "2  [[-8244282.147841794, 4976499.581186923], [-82...        5   \n",
       "3  [[-8231550.704659003, 4980004.019033388], [-82...        4   \n",
       "4  [[-8227172.6871972885, 4987909.091900762], [-8...        1   \n",
       "\n",
       "                                     time_index_list  \n",
       "0  [68, 69, 71, 75, 76, 78, 79, 83, 89, 91, 98, 9...  \n",
       "1  [0, 1, 2, 4, 5, 7, 8, 9, 11, 12, 14, 15, 18, 2...  \n",
       "2  [0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 16, 1...  \n",
       "3  [3, 36, 38, 39, 40, 41, 42, 44, 45, 46, 48, 51...  \n",
       "4  [12, 47, 48, 50, 52, 54, 55, 56, 57, 59, 60, 6...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# test_df = pd.read_parquet(\"/home/sagemaker-user/TrajCL/data/parquet_files/test/nyc_df_v3_with_time/traj_test_df_v3_with_ts.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# userids = test_df['userid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7948/2271036465.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_file)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrajCL(\n",
       "  (clmodel): MoCo(\n",
       "    (criterion): CrossEntropyLoss()\n",
       "    (encoder_q): DualSTB(\n",
       "      (pos_encoder): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (structural_attn): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (spatial_attn): SpatialMSM(\n",
       "        (pos_encoder): PositionalEncoding(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (trans_encoder): SpatialMSMEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-2): 3 x SpatialMSMLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=4, out_features=32, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (linear2): Linear(in_features=32, out_features=4, bias=True)\n",
       "              (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.1, inplace=False)\n",
       "              (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (encoder_k): DualSTB(\n",
       "      (pos_encoder): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (structural_attn): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (spatial_attn): SpatialMSM(\n",
       "        (pos_encoder): PositionalEncoding(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (trans_encoder): SpatialMSMEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-2): 3 x SpatialMSMLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=4, out_features=32, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (linear2): Linear(in_features=32, out_features=4, bias=True)\n",
       "              (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.1, inplace=False)\n",
       "              (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mlp_q): Projector(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (mlp_k): Projector(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\")\n",
    "checkpoint_file = \"/home/sagemaker-user/TrajCL/exp/generic_v2_modified/generic_v2_TrajCL_best.pt\"\n",
    "checkpoint = torch.load(checkpoint_file)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.traj import *\n",
    "import pickle\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "embs = pickle.load(open(\"/home/sagemaker-user/TrajCL/data/nyc_cell250_embdim256_embs.pkl\", 'rb')).to('cpu').detach() # tensor\n",
    "cellspace = pickle.load(open(\"/home/sagemaker-user/TrajCL/data/nyc_cell250_cellspace.pkl\", 'rb'))\n",
    "\n",
    "max_batch_size = 512\n",
    "def infer_batch(traj, time_indices):\n",
    "    traj_cell, traj_p = zip(*[merc2cell2(t, cellspace) for t in traj])\n",
    "    traj_emb_p = [torch.tensor(generate_spatial_features(t, cellspace)) for t in traj_p]\n",
    "    traj_emb_p = pad_sequence(traj_emb_p, batch_first = False).to(device)\n",
    "    traj_emb_cell = [embs[list(t)] for t in traj_cell]\n",
    "    traj_emb_cell = pad_sequence(traj_emb_cell, batch_first = False).to(device)\n",
    "    traj_len = torch.tensor(list(map(len, traj_cell)), dtype = torch.long, device = device)\n",
    "    time_indices = pad_sequence([torch.tensor(t, dtype=torch.long) for t in time_indices], batch_first=False, padding_value=-1).to(Config.device)\n",
    "    # print(traj_emb_cell, traj_emb_p, traj_len)\n",
    "    traj_embs = model.interpret(traj_emb_cell.float(), traj_emb_p.float(), traj_len, time_indices)\n",
    "    return traj_embs\n",
    "\n",
    "def infer(traj, time_indices):\n",
    "    if len(traj)> max_batch_size:\n",
    "        traj_embs = []\n",
    "        for i in range(0, len(traj), max_batch_size):\n",
    "            traj_batch = traj[i:i+max_batch_size]\n",
    "            time_indices_batch = time_indices[i:i+max_batch_size]\n",
    "            traj_embs.append(infer_batch(traj_batch, time_indices_batch))\n",
    "        return torch.cat(traj_embs, dim=0)\n",
    "    else:\n",
    "        return infer_batch(traj, time_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[0]['time_index_list'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_userids = pd.concat([df['userid'] for df in df_list]).unique()\n",
    "print(f\"Total unique userids: {len(unique_userids)}\")\n",
    "\n",
    "unique_employernames = pd.concat([df['employername'] for df in df_list]).unique()\n",
    "print(f\"Total unique employernames: {len(unique_employernames)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_userid(userid):\n",
    "    df_user = pd.concat([df[df['userid']==userid] for df in df_list]).reset_index(drop=True)\n",
    "    return df_user\n",
    "\n",
    "def get_data_for_employername(df_user, employername):\n",
    "    df_emp = df_user[df_user['employername']==employername].reset_index(drop=True)\n",
    "    return df_emp\n",
    "\n",
    "\n",
    "def get_data_for_partition(df_emp, partition):\n",
    "    df_part = df_emp[df_emp['partition_id']==partition].reset_index(drop=True)\n",
    "    df_part = df_part[~df_part['weekday'].isin([5, 6])].reset_index(drop=True) # filter out weekends\n",
    "    df_part = df_part[df_part['pck_amt']>0].reset_index(drop=True) # filter out zero paycheck amount\n",
    "    return df_part\n",
    "\n",
    "\n",
    "def get_traj_and_time_data(df_part):\n",
    "    traj = df_part['merc_seq'].values\n",
    "    time_indices = df_part['time_index_list'].values\n",
    "    return traj, time_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_max_xy(traj):\n",
    "    min_x = min([p[0] for p in traj])\n",
    "    max_x = max([p[0] for p in traj])\n",
    "    min_y = min([p[1] for p in traj])\n",
    "    max_y = max([p[1] for p in traj])\n",
    "    return min_x, max_x, min_y, max_y\n",
    "\n",
    "def calculate_dx_dy(k, target_min_x, target_min_y, min_x, max_x, min_y, max_y):\n",
    "    dx = target_min_x+ (k - max_x - min_x) / 2\n",
    "    dy = target_min_y+ (k - max_y - min_y) / 2\n",
    "    return dx, dy\n",
    "\n",
    "def transform_traj(traj, dx, dy):\n",
    "    new_traj = [[p[0] + dx, p[1] + dy] for p in traj]\n",
    "    return new_traj\n",
    "\n",
    "def normalize(trajs, cellspace):\n",
    "    # trajs: list of [[lon, lat], [,], ...]\n",
    "\n",
    "    # 1. augment the input traj in order to form 2 augmented traj views\n",
    "    # 2. convert augmented trajs to the trajs based on mercator space by cells\n",
    "    # 3. read cell embeddings and form batch tensors (sort, pad)\n",
    "\n",
    "    trajs1, trajs2 = [], []\n",
    "    time_indices1, time_indices2 = [], []\n",
    "    final_min_x = 1000000000\n",
    "    final_min_y = 1000000000\n",
    "    final_max_x = -1000000000\n",
    "    final_max_y = -1000000000\n",
    "    max_len_meters = min(cellspace.x_max-cellspace.x_min, cellspace.y_max-cellspace.y_min)\n",
    "    indices_to_omit = []\n",
    "    for i, l in enumerate(trajs):\n",
    "        min_x, max_x, min_y, max_y = get_min_max_xy(l)\n",
    "        # if final_max_x - min_x> max_len_meters or final_max_y-min_y> max_len_meters or max_x - final_min_x> max_len_meters or max_y - final_min_y> max_len_meters or max_x-min_x> max_len_meters or max_y-min_y> max_len_meters:\n",
    "        #     indices_to_omit.append(i)\n",
    "        #     continue\n",
    "        final_min_x = min(final_min_x, min_x)\n",
    "        final_min_y = min(final_min_y, min_y)\n",
    "        final_max_x = max(final_max_x, max_x)\n",
    "        final_max_y = max(final_max_y, max_y)\n",
    "    \n",
    "    if max(final_max_x-final_min_x, final_max_y-final_min_y)>max_len_meters:\n",
    "        # print(max(final_max_x-final_min_x, final_max_y-final_min_y), max_len_meters)\n",
    "        return None\n",
    "    dx, dy = calculate_dx_dy(max_len_meters, cellspace.x_min, cellspace.y_min, final_min_x, final_max_x, final_min_y, final_max_y)\n",
    "    # print(final_min_x, final_max_x, final_min_y, final_max_y)\n",
    "    # print(dx, dy)\n",
    "    for i,l in enumerate(trajs):\n",
    "        if i in indices_to_omit:\n",
    "            continue\n",
    "        new_l = transform_traj(l, dx, dy)\n",
    "        trajs1.append(new_l)\n",
    "    return trajs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dbscan(embs, target_min_similarity=0.85):\n",
    "    from sklearn.cluster import DBSCAN     # require >= 0.9 cosine similarity\n",
    "    eps = 1.0 - target_min_similarity    # cosine distance threshold\n",
    "    n_embs = embs.shape[0]\n",
    "    db = DBSCAN(eps=eps, min_samples=max(int(n_embs*0.3), 5), metric=\"cosine\", n_jobs=-1).fit(embs)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "output_dict = {}\n",
    "\n",
    "# 0: moving, 1: static, 2: not enough data\n",
    "def get_gt_and_pred_label(userid):\n",
    "    user_data = get_data_for_userid(userid)\n",
    "    employers = user_data['employername'].unique()\n",
    "    for employer in employers:\n",
    "        emp_data = get_data_for_employername(user_data, employer)\n",
    "        partitions = emp_data['partition_id'].unique()\n",
    "        for partition in partitions:\n",
    "            # print(userid, employer, partition)\n",
    "            part_data = get_data_for_partition(emp_data, partition)\n",
    "            if len(part_data)<15:\n",
    "                output_dict[(userid, employer, partition)] = 2\n",
    "            else:\n",
    "                                traj = get_traj_and_time_data(part_data)\n",
    "                traj = normalize(traj, cellspace)\n",
    "                # print(traj)\n",
    "                if traj==None:\n",
    "                    count+=1\n",
    "                    pred_label = 3\n",
    "                else:\n",
    "                    embs = infer(traj).detach().cpu().numpy()\n",
    "                    db_scan = apply_dbscan(embs, target_min_similarity=0.85)\n",
    "                    labels = db_scan.labels_                      # shape: (n_samples,)\n",
    "                    cluster_ids = [c for c in np.unique(labels) if c != -1]\n",
    "                    if len(cluster_ids)>0:\n",
    "                        pred_label = 1\n",
    "                    else:\n",
    "                        pred_label = 0\n",
    "                output_dict[(userid, employer, partition)] = pred_label\n",
    "\n",
    "    \n",
    "    # if sum(test_data['paycheck_amount'].values) > 0:\n",
    "    #     gt_label = 1\n",
    "    # else:\n",
    "    #     gt_label = 0\n",
    "    # pred_label = 0\n",
    "    # for i in range(len(test_embs)):\n",
    "    #     test_vector = test_embs[i].unsqueeze(0)\n",
    "    #     similarity = cosine_similarity(test_vector.numpy(), train_embs)[0]\n",
    "    #     top_3_indices = np.argsort(similarity)[-3:][::-1]\n",
    "    #     # print(i, top_3_indices)\n",
    "    #     similarity = similarity[top_3_indices]\n",
    "    #     # print(f\"User: {userid}, Test Trajectory {test_data['traj_id'].values[i]}, Top 3 Train Trajectories: {train_data['traj_id'].values[top_3_indices]}, similarity: {similarity}, PCK Amount: {train_data['paycheck_amount'].values[top_3_indices]}\")\n",
    "    #     for sim, idx in zip(similarity, top_3_indices):\n",
    "    #         if sim>0.85 and train_data['paycheck_amount'].values[idx]>0:\n",
    "    #             pred_label = 1\n",
    "    #             break\n",
    "    # return gt_label, pred_label\n",
    "\n",
    "for userid in tqdm(unique_userids):\n",
    "    get_gt_and_pred_label(userid)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output_dict\n",
    "pickle.dump(output_dict, open(\"/home/sagemaker-user/TrajCL/exp/v2.1/nyc_test_hyp1_output_dict.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_keys = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_userid_employer_partition(userid, employer, partition):\n",
    "    df_user = pd.concat([df[df['userid']==userid] for df in df_list]).reset_index(drop=True)\n",
    "    df_emp = df_user[df_user['employername']==employer].reset_index(drop=True)\n",
    "    df_part = df_emp[df_emp['partition_id']==partition].reset_index(drop=True)\n",
    "    df_part_fil = df_part[~df_part['weekday'].isin([5, 6])].reset_index(drop=True) # filter out weekends\n",
    "    df_part_fil = df_part_fil[df_part_fil['pck_amt']>0].reset_index(drop=True) # filter out zero paycheck amount\n",
    "    return df_part_fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part = get_data_userid_employer_partition(34299, 'u s postal service', 0)\n",
    "df_part.sort_values('traj_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trajcl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
