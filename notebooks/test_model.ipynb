{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../model\")\n",
    "from trajcl import TrajCL\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Config()\n",
    "\n",
    "conf.dataset = 'nyc'\n",
    "conf.post_value_updates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrajCL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_parquet(\"/home/sagemaker-user/TrajCL/data/parquet_files/test/nyc_df_v3_with_time/traj_test_df_v3_with_ts.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userids = test_df['userid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\")\n",
    "checkpoint_file = \"/home/sagemaker-user/TrajCL/exp/v2.2/nyc_TrajCL_best.pt\"\n",
    "checkpoint = torch.load(checkpoint_file)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.traj import *\n",
    "import pickle\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "embs = pickle.load(open(\"/home/sagemaker-user/TrajCL/data/nyc_cell250_embdim256_embs.pkl\", 'rb')).to('cpu').detach() # tensor\n",
    "cellspace = pickle.load(open(\"/home/sagemaker-user/TrajCL/data/nyc_cell250_cellspace.pkl\", 'rb'))\n",
    "\n",
    "max_batch_size = 512\n",
    "def infer_batch(traj, time_indices):\n",
    "    traj_cell, traj_p = zip(*[merc2cell2(t, cellspace) for t in traj])\n",
    "    traj_emb_p = [torch.tensor(generate_spatio_temporal_features(t, time_indices[i], cellspace)) for i, t in enumerate(traj_p)]\n",
    "    traj_emb_p = pad_sequence(traj_emb_p, batch_first = False).to(device)\n",
    "    traj_emb_cell = [embs[list(t)] for t in traj_cell]\n",
    "    traj_emb_cell = pad_sequence(traj_emb_cell, batch_first = False).to(device)\n",
    "    traj_len = torch.tensor(list(map(len, traj_cell)), dtype = torch.long, device = device)\n",
    "    time_indices = pad_sequence([torch.tensor(t, dtype=torch.long) for t in time_indices], batch_first=False, padding_value=-1).to(Config.device)\n",
    "    # print(traj_emb_cell, traj_emb_p, traj_len)\n",
    "    traj_embs = model.interpret(traj_emb_cell.float(), traj_emb_p.float(), traj_len, time_indices)\n",
    "    return traj_embs\n",
    "\n",
    "def infer(traj, time_indices):\n",
    "    if len(traj)> max_batch_size:\n",
    "        traj_embs = []\n",
    "        for i in range(0, len(traj), max_batch_size):\n",
    "            traj_batch = traj[i:i+max_batch_size]\n",
    "            time_indices_batch = time_indices[i:i+max_batch_size]\n",
    "            traj_embs.append(infer_batch(traj_batch, time_indices_batch))\n",
    "        return torch.cat(traj_embs, dim=0)\n",
    "    else:\n",
    "        return infer_batch(traj, time_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['time_index_list'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "gt_list = []\n",
    "pred_list = []\n",
    "def get_gt_and_pred_label(userid):\n",
    "    user_data = test_df[test_df['userid'] == userid].reset_index(drop=True)\n",
    "    train_data = user_data[user_data['train_test_tag'] == 'train'].reset_index(drop=True)\n",
    "    test_data = user_data[user_data['train_test_tag'] == 'test'].reset_index(drop=True)\n",
    "    train_traj = train_data['merc_seq_filtered'].values\n",
    "    test_traj = test_data['merc_seq_filtered'].values\n",
    "    train_time_indices = train_data['time_index_list'].values\n",
    "    test_time_indices = test_data['time_index_list'].values\n",
    "    train_embs = infer(train_traj, train_time_indices).detach().cpu().numpy()\n",
    "    test_embs = infer(test_traj, test_time_indices).detach().cpu()\n",
    "    if sum(test_data['paycheck_amount'].values) > 0:\n",
    "        gt_label = 1\n",
    "    else:\n",
    "        gt_label = 0\n",
    "    pred_label = 0\n",
    "    for i in range(len(test_embs)):\n",
    "        test_vector = test_embs[i].unsqueeze(0)\n",
    "        similarity = cosine_similarity(test_vector.numpy(), train_embs)[0]\n",
    "        top_3_indices = np.argsort(similarity)[-3:][::-1]\n",
    "        # print(i, top_3_indices)\n",
    "        similarity = similarity[top_3_indices]\n",
    "        # print(f\"User: {userid}, Test Trajectory {test_data['traj_id'].values[i]}, Top 3 Train Trajectories: {train_data['traj_id'].values[top_3_indices]}, similarity: {similarity}, PCK Amount: {train_data['paycheck_amount'].values[top_3_indices]}\")\n",
    "        for sim, idx in zip(similarity, top_3_indices):\n",
    "            if sim>0.85 and train_data['paycheck_amount'].values[idx]>0:\n",
    "                pred_label = 1\n",
    "                break\n",
    "    return gt_label, pred_label\n",
    "\n",
    "for userid in tqdm(userids):\n",
    "    gt_label, pred_label = get_gt_and_pred_label(userid)\n",
    "    gt_list.append(gt_label)\n",
    "    pred_list.append(pred_label)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of 0 labels\n",
    "sum(gt_list) / len(gt_list), sum(pred_list) / len(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(gt_list, pred_list)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(gt_list, pred_list)\n",
    "# Assuming you already have the confusion matrix 'cm'\n",
    "# For binary classification, cm is in the form:\n",
    "# [[TN, FP],\n",
    "#  [FN, TP]]\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(\"True Positives:\", tp)\n",
    "print(\"False Positives:\", fp)\n",
    "print(\"True Negatives:\", tn)\n",
    "print(\"False Negatives:\", fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = precision_score(gt_list, pred_list)\n",
    "recall = recall_score(gt_list, pred_list)\n",
    "f1 = f1_score(gt_list, pred_list)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_and_pred_label(userid):\n",
    "    user_data = test_df[test_df['userid'] == userid].reset_index(drop=True)\n",
    "    train_data = user_data[user_data['train_test_tag'] == 'train'].reset_index(drop=True)\n",
    "    test_data = user_data[user_data['train_test_tag'] == 'test'].reset_index(drop=True)\n",
    "    train_traj = train_data['merc_seq_filtered'].values\n",
    "    test_traj = test_data['merc_seq_filtered'].values\n",
    "    train_time_indices = train_data['time_index_list'].values\n",
    "    test_time_indices = test_data['time_index_list'].values\n",
    "    train_embs = infer(train_traj, train_time_indices).detach().cpu().numpy()\n",
    "    test_embs = infer(test_traj, test_time_indices).detach().cpu()\n",
    "    if sum(test_data['paycheck_amount'].values) > 0:\n",
    "        gt_label = 1\n",
    "    else:\n",
    "        gt_label = 0\n",
    "    pred_label = 0\n",
    "    for i in range(len(test_embs)):\n",
    "        test_vector = test_embs[i].unsqueeze(0)\n",
    "        similarity = cosine_similarity(test_vector.numpy(), train_embs)[0]\n",
    "        top_3_indices = np.argsort(similarity)[-3:][::-1]\n",
    "        # print(i, top_3_indices)\n",
    "        similarity = similarity[top_3_indices]\n",
    "        print(f\"User: {userid}, Test Trajectory {test_data['traj_id'].values[i]}, Top 3 Train Trajectories: {train_data['traj_id'].values[top_3_indices]}, similarity: {similarity}, PCK Amount: {train_data['paycheck_amount'].values[top_3_indices]}\")\n",
    "        for sim, idx in zip(similarity, top_3_indices):\n",
    "            if sim>0.85 and train_data['paycheck_amount'].values[idx]>0:\n",
    "                pred_label = 1\n",
    "                break\n",
    "    return gt_label, pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_indices = [i for i, (gt, pred) in enumerate(zip(gt_list, pred_list)) if gt == pred]\n",
    "print(\"Correct Indices:\", correct_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_indices = [i for i, (gt, pred) in enumerate(zip(gt_list, pred_list)) if gt == 1 and pred == 0]\n",
    "print(\"False Negative Indices:\", fn_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_indices = [i for i, (gt, pred) in enumerate(zip(gt_list, pred_list)) if gt == 0 and pred == 1]\n",
    "print(\"False Positive Indices:\", fp_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "userid = userids[idx]\n",
    "get_gt_and_pred_label(6542144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df[test_df['train_test_tag'] == 'test'][test_df['label']==1]['userid'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "790/1051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[test_df['userid']==5499415].sort_values('traj_date',ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trajcl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
