{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df240595-658c-40d7-b219-94252fbd2fd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"pip install -r ../requirements.txt\")\n",
    "os.system(\"pip install tqdm\")\n",
    "\n",
    "from config_infer import InferenceConfig\n",
    "from model.dual_attention import DualSTBTimeWeighted\n",
    "from utils.traj import *\n",
    "import pickle\n",
    "from utils.cellspace import *\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "cfg = InferenceConfig()\n",
    "# print(cfg.to_str())\n",
    "\n",
    "print(cfg.checkpoint_file)\n",
    "encoder_q = DualSTBTimeWeighted(cfg.seq_embedding_dim, \n",
    "                                            cfg.trans_hidden_dim, \n",
    "                                            cfg.trans_attention_head, \n",
    "                                            cfg.trans_attention_layer, \n",
    "                                            cfg.trans_attention_dropout, \n",
    "                                            cfg.trans_pos_encoder_dropout)\n",
    "\n",
    "\n",
    "device = torch.device(cfg.device)\n",
    "encoder_q = encoder_q.to(device)\n",
    "print(encoder_q)\n",
    "\n",
    "# load model from checkpoint\n",
    "checkpoint = torch.load(cfg.checkpoint_file, map_location=device)['model_state_dict']\n",
    "encoder_q_keys = [k for k in list(checkpoint.keys()) if 'encoder_q' in k]\n",
    "\n",
    "new_checkpoint = {}\n",
    "for k in encoder_q_keys:\n",
    "    new_k = k.replace('clmodel.encoder_q.', '')\n",
    "    new_checkpoint[new_k] = checkpoint[k]\n",
    "\n",
    "encoder_q.load_state_dict(new_checkpoint)\n",
    "encoder_q.eval()\n",
    "print(\"Model loaded from checkpoint.\")\n",
    "\n",
    "embs_parent = pickle.load(open(cfg.dataset_embs_file_parent, 'rb')).to('cpu').detach() # tensor\n",
    "embs_child = pickle.load(open(cfg.dataset_embs_file_child, 'rb')).to('cpu').detach() # tensor\n",
    "cellspace_parent = pickle.load(open(cfg.dataset_cell_file_parent, 'rb'))\n",
    "cellspace_child = pickle.load(open(cfg.dataset_cell_file_child, 'rb'))\n",
    "hier_cellspace = HirearchicalCellSpace(cellspace_parent, cellspace_child)\n",
    "\n",
    "def model_forward(trajs1_emb, trajs1_emb_p, trajs1_len, time_deltas1):\n",
    "    max_trajs1_len = trajs1_len.max().item() # trajs1_len[0]\n",
    "    src_padding_mask1 = torch.arange(max_trajs1_len, device = device)[None, :] >= trajs1_len[:, None]\n",
    "    # traj_embs = self.clmodel.encoder_q(**{'src': trajs1_emb, 'time_indices': time_indices1, 'attn_mask': None, 'src_padding_mask': src_padding_mask1, 'src_len': trajs1_len, 'srcspatial': trajs1_emb_p})\n",
    "    traj_embs = encoder_q(**{'src': trajs1_emb, 'time_deltas': time_deltas1, 'attn_mask': None, 'src_padding_mask': src_padding_mask1, 'src_len': trajs1_len, 'srcspatial': trajs1_emb_p})\n",
    "    return traj_embs\n",
    "\n",
    "def infer_batch(traj, time_indices):\n",
    "    traj_cell_parent, traj_cell_child, traj_p, traj_timedelta = zip(*[merc2cell2(l[:800],t[:800], hier_cellspace) for l,t in zip(traj, time_indices)])\n",
    "    # print(traj_cell)\n",
    "    traj_emb_p = [torch.tensor(generate_spatial_features(t, hier_cellspace)) for t in traj_p]\n",
    "    traj_emb_p = pad_sequence(traj_emb_p, batch_first = False).to(device)\n",
    "    traj_emb_cell_parent = [embs_parent[list(t)] for t in traj_cell_parent]\n",
    "    traj_emb_cell_child = [embs_child[list(t)] for t in traj_cell_child]\n",
    "    traj_emb_cell = [a + b for a, b in zip(traj_emb_cell_parent, traj_emb_cell_child)]\n",
    "    traj_emb_cell = pad_sequence(traj_emb_cell, batch_first = False).to(device)\n",
    "    traj_len = torch.tensor(list(map(len, traj_p)), dtype = torch.long, device = device)\n",
    "    traj_timedelta = pad_sequence([torch.tensor(t) for t in traj_timedelta], batch_first=False, padding_value=0).to(device)\n",
    "    # print(traj_emb_cell, traj_emb_p, traj_len)\n",
    "    traj_embs = model_forward(traj_emb_cell.float(), traj_emb_p.float(), traj_len, traj_timedelta)\n",
    "    return traj_embs, traj_cell_parent, traj_cell_child , traj_p, traj_timedelta\n",
    "\n",
    "batch_size = cfg.batch_size\n",
    "# def infer(traj, time_indices):\n",
    "#     if len(traj)> batch_size:\n",
    "#         traj_embs = []\n",
    "#         for i in range(0, len(traj), batch_size):\n",
    "#             traj_batch = traj[i:i+batch_size]\n",
    "#             time_indices_batch = time_indices[i:i+batch_size] \n",
    "#             traj_embs.append(infer_batch(traj_batch, time_indices_batch))\n",
    "#         return torch.cat(traj_embs, dim=0)\n",
    "#     else:\n",
    "#         return infer_batch(traj, time_indices)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0afa896a-74b7-428f-a96b-722f92dbcdb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, IterableDataset\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class TrajDatasetSpark(IterableDataset):\n",
    "    def __init__(self, data_dir):\n",
    "        # data: DataFrame\n",
    "        self.files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\".parquet\")]\n",
    "        # self.files = random.sample(self.files, len(self.files))\n",
    "        print(\"Read spark files\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file_path in self.files:\n",
    "            df = pd.read_parquet(file_path)\n",
    "            loc_features = df[\"merc_seq\"].values\n",
    "            time_features = df[\"sorted_ts\"].values\n",
    "            userids = df[\"userid\"].values\n",
    "            traj_dates = df[\"traj_date\"].values\n",
    "            wgs_seq_list = df[\"wgs_seq\"].values\n",
    "            for loc_feature, time_feature, userid, traj_date, wgs_seq in zip(loc_features, time_features, userids, traj_dates, wgs_seq_list):\n",
    "                output = {\n",
    "                    \"merc_seq\": loc_feature,\n",
    "                    \"sorted_ts\": time_feature,\n",
    "                    \"userid\": userid,\n",
    "                    \"traj_date\": traj_date\n",
    "                }\n",
    "                yield output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fc7b239-013b-4fd2-92e8-3fb6da5bbde4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import current_date\n",
    "# df = spark.read.parquet('/Volumes/main_prod/datascience_scratchpad/jatin/trajcl_exp/usa/last_365_days_backfill_parquet')\n",
    "# display(df)\n",
    "\n",
    "data_dir = '/Volumes/main_prod/datascience_scratchpad/jatin/trajcl_exp/usa/backfill_traj_pre_data_rem_parquet'\n",
    "dataset = TrajDatasetSpark(data_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "615b6d91-da94-46c8-9605-183a99582e13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # repartition df in a list of 100 dfs\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# num_splits = 100\n",
    "# dfs = df.randomSplit([1.0] * num_splits, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42764c5b-c47f-4f03-aaee-1fd8ae750aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "893b15ff-477d-402c-948f-1b9b3297b2a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import *\n",
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField(\"userid\", IntegerType(), True),\n",
    "#     StructField(\"traj_date\", DateType(), True),\n",
    "#     StructField(\"embedding\", ArrayType(FloatType()), True),\n",
    "#     StructField(\"model_version\", StringType(), True)\n",
    "# ])\n",
    "\n",
    "# empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "# empty_df.write.mode(\"overwrite\").saveAsTable(\"main_prod.datascience_scratchpad.traj_emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1993a943-0487-4543-a896-2641692a45de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a31b2080-72f9-49de-82c8-d78cbfa7b8ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "201c3e8a-31c3-489e-b4a3-c42a8f51e60c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emb_list = []\n",
    "data_list = []\n",
    "userids = []\n",
    "traj_dates = []\n",
    "batch_size = 32\n",
    "files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\".parquet\")]\n",
    "for file in tqdm(files):\n",
    "    df_pd = pd.read_parquet(file)\n",
    "    for i in range(0, len(df_pd), batch_size):\n",
    "        data_list = df_pd.iloc[i:i+batch_size].to_dict('records')\n",
    "        traj = [data['merc_seq'] for data in data_list]\n",
    "        time_indices = [data['sorted_ts'] for data in data_list]\n",
    "        time_indices = [np.array(time_indices[i], dtype='datetime64[ns]') for i in range(len(time_indices))]\n",
    "        \n",
    "        traj_embs, traj_cell_parent, traj_cell_child, traj_p, traj_timedelta = infer_batch(traj, time_indices)\n",
    "        userids.extend([data['userid'] for data in data_list])\n",
    "        traj_dates.extend([data['traj_date'] for data in data_list])\n",
    "        emb_list.append(traj_embs.detach().cpu())\n",
    "        \n",
    "all_emb = torch.cat(emb_list, dim=0)\n",
    "df_new = pd.DataFrame()\n",
    "df_new['userid'] = userids\n",
    "df_new['traj_date'] = traj_dates\n",
    "df_new['embedding'] = all_emb.tolist()\n",
    "df_new['model_version'] = cfg.model_version\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0491a8c-cd76-4664-a254-682ab90010c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "318d70e9-e238-4689-a1cb-c7859222c470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60d6c95f-6d6d-46e9-86c9-d9ae2c44066a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_spark = spark.createDataFrame(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "665318c7-a5b5-4e4b-b178-a9bd37cbf40b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_spark.write.mode(\"overwrite\").parquet('/Volumes/main_prod/datascience_scratchpad/jatin/trajcl_exp/usa/backfill_emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61750a0b-e31d-4a94-986d-f40884552514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# df_spark.write.mode(\"append\").saveAsTable(\"main_prod.datascience_scratchpad.traj_emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64ad7705-fc2a-45d2-80a6-a89832cf0c8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "backfill_traj_model_inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
