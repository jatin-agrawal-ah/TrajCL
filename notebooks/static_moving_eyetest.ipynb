{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0bfc96d-915f-418f-8141-ec7efefe4d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install folium\n",
    "import folium\n",
    "\n",
    "from folium import plugins\n",
    "\n",
    "def display_traj(lon_lat_list,time_indices=None):\n",
    "    coordinates = [[x[1],x[0]] for x in lon_lat_list]\n",
    "    # Initialize map\n",
    "    m = folium.Map(location=coordinates[0], zoom_start=15)\n",
    "\n",
    "\n",
    "    # for lat,lon in pip_list:\n",
    "    #     folium.Marker(\n",
    "    #         location=[lat, lon],\n",
    "    #         popup=\"Pip Location\",\n",
    "    #         icon=folium.Icon(color='red')  # Color can be 'red', 'blue', 'green', 'purple', etc.\n",
    "    #     ).add_to(m)\n",
    "\n",
    "    folium.Marker(\n",
    "        location=coordinates[0],\n",
    "        popup=time_indices[0],\n",
    "        icon=folium.Icon(color='green')  # Color can be 'red', 'blue', 'green', 'purple', etc.\n",
    "    ).add_to(m)\n",
    "\n",
    "    folium.Marker(\n",
    "        location=coordinates[-1],\n",
    "        popup=time_indices[-1],\n",
    "        icon=folium.Icon(color='red')  # Color can be 'red', 'blue', 'green', 'purple', etc.\n",
    "    ).add_to(m)\n",
    "    # Add markers\n",
    "    for i, (lat, lon) in enumerate(coordinates[1:-1]):\n",
    "        if time_indices is None:\n",
    "            folium.Marker([lat, lon], popup=(lat, lon)).add_to(m)\n",
    "        else:\n",
    "            folium.Marker([lat, lon], popup=(i+1, time_indices[i+1], lat, lon)).add_to(m)\n",
    "        \n",
    "\n",
    "    # Draw arrows between points\n",
    "    for i in range(len(coordinates) - 1):\n",
    "        start = coordinates[i]\n",
    "        end = coordinates[i + 1]\n",
    "\n",
    "        # Draw the line\n",
    "        line = folium.PolyLine([start, end], color=\"blue\", weight=3, opacity=0.7).add_to(m)\n",
    "\n",
    "        # Add directional arrow\n",
    "        plugins.PolyLineTextPath(\n",
    "            line,\n",
    "            'âž¤',  # arrow symbol\n",
    "            repeat=True,\n",
    "            offset=7,\n",
    "            attributes={'fill': 'blue', 'font-weight': 'bold', 'font-size': '16'}\n",
    "            ).add_to(m)\n",
    "        \n",
    "        \n",
    "\n",
    "    # Save and show the map\n",
    "    m.save(\"map_with_arrows.html\")\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5909737c-03f6-49b1-9abb-704bd3832e07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/Volumes/main_prod/datascience_scratchpad/jatin/trajcl_exp/usa/backfill_static_moving_relax_condition_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "435d9440-abd4-4881-9730-1258c063c70e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)\n",
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "263df45c-5261-49a3-8660-ad4a36ef7ba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.where('predicted_work_type = \"moving\"').count()/df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22eac0ff-3ef6-4905-8abb-e0a0664340aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imp points: 150862, 228036"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5938505c-1ada-472b-a9ce-ee6fa9e67984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.where('predicted_work_type = \"moving\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59746a52-2dec-450e-a784-34bcaddd0564",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763022091634}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.where(\"userid  = 6771816\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db4e5629-9309-4d5f-9298-b510a281d91c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763022313169}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from main_prod.ml_data.cm_work_dwell_time_v3_2 where userid = 15776078 order by calc_date desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15c1306d-8aa5-428e-8244-07c82b1ee88f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "traj_df = spark.read.table('main_prod.ml_data.traj_data')\n",
    "data_dir = '/Volumes/main_prod/datascience_scratchpad/jatin/trajcl_exp/usa/backfill_traj_data_relaxed'\n",
    "\n",
    "traj_df_v2 = spark.read.format('delta').load(data_dir)\n",
    "display(traj_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "031ac1a8-223a-4ab8-bc08-18caf19a2de8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763108757939}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(traj_df.where('userid = 4515091').orderBy(\"traj_date\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66318770-778e-492e-abf9-9b9b8c4c44a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "def get_candidate_df(userid):\n",
    "    candidate_df = traj_df.filter(col(\"userid\") == userid)\n",
    "    candidate_df_v2 = traj_df_v2.filter(col(\"userid\") == userid)\n",
    "    candidate_df_final = candidate_df.union(candidate_df_v2)\n",
    "    return candidate_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04e4eb6a-388f-4239-952d-cf43bbd6b216",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763142412159}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "userid = 17049707\n",
    "traj_dates = [\"2024-08-01\",\"2024-08-02\",\"2024-08-03\", \"2024-08-04\"]\n",
    "list_lng_lat_list = []\n",
    "time_indices_list = []\n",
    "candidate_df = get_candidate_df(userid)\n",
    "for traj_date in traj_dates:\n",
    "    # print(traj_date)\n",
    "    lng_lat_list = candidate_df.where(\"traj_date = '{}'\".format(traj_date)).select(\"wgs_seq\").collect()[0][0]\n",
    "    time_indices = candidate_df.where(\"traj_date = '{}'\".format(traj_date)).select(\"sorted_ts\").collect()[0][0]\n",
    "    list_lng_lat_list.append(lng_lat_list)\n",
    "    time_indices_list.append(time_indices)\n",
    "display(candidate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60d515d3-034c-4b6b-90a7-9eb497c03886",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display_traj(list_lng_lat_list[0], time_indices_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfc3def4-600c-4a3f-b200-b2ec85d152c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "display_traj(list_lng_lat_list[1], time_indices_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46b230e5-96ad-472e-ae45-3f3dcf326e21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display_traj(list_lng_lat_list[2], time_indices_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ac9cba8-5c63-4b2c-83f5-297eed868a89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display_traj(list_lng_lat_list[3], time_indices_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54352e92-c447-4d87-a588-b002ac8a57fe",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763111751012}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emb_df = spark.read.table('main_prod.ml_data.traj_emb')\n",
    "display(emb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9db6db53-b8b9-4dca-9970-118bbd4f36a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "def calculate_similarity(embeddings):\n",
    "    similarity = {}\n",
    "    for i in range(len(embeddings)):\n",
    "        for j in range(i+1, len(embeddings)):\n",
    "            similarity[(i, j)] = cosine_similarity(embeddings[i], embeddings[j])\n",
    "    return similarity\n",
    "\n",
    "def get_emb_df(userid, traj_dates):\n",
    "    df = emb_df.where(\n",
    "        'userid = {} and traj_date in ({})'.format(\n",
    "            userid,\n",
    "            ','.join([\"'{}'\".format(traj_date) for traj_date in traj_dates])\n",
    "        )\n",
    "    ).orderBy(\"traj_date\")\n",
    "    # Extract embedding arrays from Row objects\n",
    "    embeddings = [\n",
    "        np.array(row['embedding']) for row in df.select(\"embedding\").collect()\n",
    "    ]\n",
    "    return embeddings\n",
    "\n",
    "# print(new_dates)\n",
    "embeddings = get_emb_df(userid, traj_dates)\n",
    "calculate_similarity(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "282620f2-3a1b-49a7-95c4-0b13b1f375ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import numpy as np\n",
    "\n",
    "def neighbor_counts(embs, target_min_similarity=0.9):\n",
    "    eps = 1.0 - target_min_similarity\n",
    "    D = pairwise_distances(embs, metric=\"cosine\")       # cosine distance\n",
    "    # neighbors within eps (including self on diagonal)\n",
    "    N = (D <= eps).sum(axis=1)\n",
    "    return N, D\n",
    "\n",
    "N, D = neighbor_counts(embeddings, target_min_similarity=0.9)\n",
    "print(\"Per-point neighbor counts (including self):\", N)\n",
    "print(\"Max neighbors:\", N.max(), \"Median:\", np.median(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f593681d-6f51-4a02-9ce8-ece0dacd18c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN \n",
    "def apply_dbscan(embs, paycycle_len, target_min_similarity=0.9):\n",
    "    eps = 1.0 - target_min_similarity    # cosine distance threshold\n",
    "    n_embs = embs.shape[0]\n",
    "    min_samples = 3\n",
    "    db = DBSCAN(eps=eps, min_samples=max(int(n_embs*0.2), min_samples), metric=\"cosine\", n_jobs=-1).fit(embs)\n",
    "    return db\n",
    "\n",
    "def get_cluster(df, paycycle_len):\n",
    "    embeddings = np.stack(df['embedding'].values)\n",
    "    db = apply_dbscan(embeddings, paycycle_len)\n",
    "    labels = db.labels_\n",
    "    cluster_dict = {}\n",
    "    date_label_dict = {}\n",
    "    \n",
    "    for label in labels:\n",
    "        if label != -1:\n",
    "            if label in cluster_dict:\n",
    "                cluster_dict[label]+=1\n",
    "            else:\n",
    "                cluster_dict[label]=1\n",
    "            date_label_dict[df['traj_date'].iloc[0]] = label\n",
    "    \n",
    "    return db\n",
    "\n",
    "\n",
    "def is_static(cluster_dict, traj_count):\n",
    "    cluster_count = 0\n",
    "    for key in cluster_dict:\n",
    "        cluster_count += cluster_dict[key]\n",
    "    if cluster_count >= int(0.7*traj_count):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def cluster_exists(db):\n",
    "    labels = db.labels_\n",
    "    for label in labels:\n",
    "        if label != -1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def dbscan_predict_all(db, X_train, X_new):\n",
    "    nn = NearestNeighbors(radius=db.eps, metric=db.metric).fit(X_train)\n",
    "    dists, idxs = nn.radius_neighbors(X_new, return_distance=True)\n",
    "    y = db.labels_\n",
    "    pred = np.full(len(X_new), -1, dtype=int)\n",
    "    for i, (di, ii) in enumerate(zip(dists, idxs)):\n",
    "        if len(ii) == 0: \n",
    "            continue\n",
    "        lbls, di = y[ii], di\n",
    "        mask = lbls != -1\n",
    "        if mask.any():\n",
    "            pred[i] = lbls[mask][np.argmin(di[mask])]\n",
    "    return pred\n",
    "    \n",
    "def works_on_weekends_fn(db, weekday_df, weekend_df):\n",
    "    if len(weekday_df) == 0 or len(weekend_df) == 0:\n",
    "        return False\n",
    "    weekday_embs = np.stack(weekday_df['embedding'].values)\n",
    "    weekend_embs = np.stack(weekend_df['embedding'].values)\n",
    "    pred = dbscan_predict_all(db, weekday_embs, weekend_embs)\n",
    "    # if 40% of pred is not -1, then it works on weekends\n",
    "    pred_not_neg = pred[pred != -1]\n",
    "    return len(pred_not_neg) > 0.4*len(pred)\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F, types as T\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------\n",
    "# Helper functions assumed to exist and be importable on workers:\n",
    "#   get_cluster(pdf) -> any\n",
    "#   cluster_exists(cluster) -> bool\n",
    "#   works_on_weekends_fn(cluster, weekday_pdf, weekend_pdf) -> bool\n",
    "# Ensure they are defined in the same file or available on PYTHONPATH for executors.\n",
    "# -----------------------\n",
    "\n",
    "# Output schema (adjust types if your real types differ)\n",
    "out_schema = T.StructType([\n",
    "    T.StructField(\"userid\", T.IntegerType(), False),\n",
    "    T.StructField(\"employerid\", T.IntegerType(), False),\n",
    "    T.StructField(\"employername\", T.StringType(), True),\n",
    "    T.StructField(\"predicted_work_type\", T.StringType(), True),\n",
    "    T.StructField(\"predicted_on\", T.DateType(), True),\n",
    "    T.StructField(\"works_on_weekends\", T.BooleanType(), True),\n",
    "    T.StructField(\"paydate\", T.DateType(), True),\n",
    "    T.StructField(\"old_latest_date_label_dict\", T.MapType(T.DateType(), T.IntegerType()), True),\n",
    "    T.StructField(\"new_latest_date_label_dict\", T.MapType(T.DateType(), T.IntegerType()), True),\n",
    "    T.StructField(\"old_latest_weekday_df_length\", T.LongType(), True),\n",
    "    T.StructField(\"latest_weekday_df_length\", T.LongType(), True),\n",
    "])\n",
    "\n",
    "def compute_static_moving(group_pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Runs your pandas logic for one (userid, employerid, employername) group.\n",
    "    This function executes on a Spark worker.\n",
    "    \"\"\"\n",
    "    import datetime\n",
    "    today = datetime.date.today()\n",
    "\n",
    "    # We only process a single group here\n",
    "    # Extract group keys (safe because it's a single group)\n",
    "    userid = group_pdf[\"userid\"].iloc[0]\n",
    "    employerid = group_pdf[\"employerid\"].iloc[0]\n",
    "    employername = group_pdf.get(\"employername\", pd.Series([None])).iloc[0]\n",
    "\n",
    "    # Ensure 'weekday' exists (create if your input doesn't have it)\n",
    "    if \"weekday\" not in group_pdf.columns:\n",
    "        # If traj date is present, you could compute it; otherwise expect it precomputed\n",
    "        if \"traj_date\" in group_pdf.columns:\n",
    "            group_pdf = group_pdf.copy()\n",
    "            group_pdf[\"weekday\"] = pd.to_datetime(group_pdf[\"traj_date\"]).dt.weekday\n",
    "        else:\n",
    "            raise ValueError(\"Missing 'weekday' column and no 'traj_date' to compute it from.\")\n",
    "    paycycle_len = (group_pdf[\"paydate\"].iloc[-1] - group_pdf[\"prev_paydate\"].iloc[-1]).days\n",
    "    if paycycle_len <=8: \n",
    "        past_cycle_to_consider = 4\n",
    "    elif paycycle_len >8 and paycycle_len <=15:\n",
    "        past_cycle_to_consider = 2\n",
    "    else:\n",
    "        past_cycle_to_consider = 1\n",
    "    target_n_cycles = 80\n",
    "    outputs = []\n",
    "\n",
    "    # # Convert to proper dtypes just in case (optional but helpful)\n",
    "    # # Expect paydate to be date-like if coming from Spark DateType\n",
    "    # if not pd.api.types.is_datetime64_any_dtype(group_pdf[\"paydate\"]):\n",
    "    #     group_pdf = group_pdf.copy()\n",
    "    #     group_pdf[\"paydate\"] = pd.to_datetime(group_pdf[\"paydate\"]).dt.date\n",
    "\n",
    "    # Loop cycles (kept from your original logic; target_n_cycles = 1)\n",
    "    for target_rn in range(target_n_cycles):\n",
    "        target_paycycle = target_rn + 1\n",
    "\n",
    "        # Latest cycle slices\n",
    "        latest_weekday_df = group_pdf.loc[\n",
    "            (group_pdf[\"weekday\"].isin([0,1,2,3,4])) & (group_pdf[\"rn\"] == target_paycycle)\n",
    "        ]\n",
    "        latest_weekend_df = group_pdf.loc[\n",
    "            (group_pdf[\"weekday\"].isin([5,6])) & (group_pdf[\"rn\"] == target_paycycle)\n",
    "        ]\n",
    "\n",
    "        # Old cycles (NOTE: use bitwise & for pandas)\n",
    "        rn_mask_old = (group_pdf[\"rn\"] > target_paycycle) & (group_pdf[\"rn\"] <= target_paycycle + past_cycle_to_consider)\n",
    "        old_weekday_df = group_pdf.loc[\n",
    "            (group_pdf[\"weekday\"].isin([0,1,2,3,4])) & rn_mask_old\n",
    "        ]\n",
    "        old_weekend_df = group_pdf.loc[\n",
    "            (group_pdf[\"weekday\"].isin([5,6])) & rn_mask_old\n",
    "        ]\n",
    "\n",
    "        if len(latest_weekday_df) == 0:\n",
    "            continue\n",
    "        old_latest_df = pd.concat([latest_weekday_df, old_weekday_df], ignore_index=True)\n",
    "        old_latest_cluster, old_latest_cluster_dict,  old_latest_date_label_dict = get_cluster(old_latest_df)\n",
    "        latest_date_label_dict = {}\n",
    "        if is_static(old_latest_cluster_dict, len(old_latest_df)):\n",
    "            work_type = \"static\"\n",
    "            works_on_weekends = works_on_weekends_fn(\n",
    "                old_latest_cluster,\n",
    "                old_latest_df,\n",
    "                pd.concat([latest_weekend_df, old_weekend_df], ignore_index=True),\n",
    "            )\n",
    "        else:\n",
    "            latest_cluster, latest_cluster_dict, latest_date_label_dict = get_cluster(pd.concat(latest_weekday_df, ignore_index=True))\n",
    "            if is_static(latest_cluster_dict, len(latest_weekday_df)):\n",
    "                work_type = \"static\"\n",
    "                works_on_weekends = works_on_weekends_fn(\n",
    "                    latest_cluster, latest_weekday_df, latest_weekend_df\n",
    "                )\n",
    "            else:\n",
    "                work_type = \"moving\"\n",
    "                works_on_weekends = False\n",
    "\n",
    "        # If multiple rows exist for the latest cycle, choose a representative paydate.\n",
    "        # Here we just take the first paydate from the latest cycle rows.\n",
    "        paydate_value = latest_weekday_df[\"paydate\"].iloc[0] if len(latest_weekday_df) else None\n",
    "\n",
    "        outputs.append({\n",
    "            \"userid\": userid,\n",
    "            \"employerid\": employerid,\n",
    "            \"employername\": employername,\n",
    "            \"predicted_work_type\": work_type,\n",
    "            \"predicted_on\": today,\n",
    "            \"works_on_weekends\": works_on_weekends,\n",
    "            \"paydate\": paydate_value,\n",
    "            \"old_latest_date_label_dict\": old_latest_date_label_dict,\n",
    "            \"latest_date_label_dict\": latest_date_label_dict,\n",
    "            \"old_latest_weekday_df_length\": len(old_latest_df),\n",
    "            \"latest_weekday_df_length\": len(latest_weekday_df)\n",
    "        })\n",
    "\n",
    "    if outputs:\n",
    "        return pd.DataFrame(outputs, columns=[f.name for f in out_schema])\n",
    "    else:\n",
    "        # Return empty frame with correct columns if no output for this group\n",
    "        return pd.DataFrame(columns=[f.name for f in out_schema])\n",
    "\n",
    "# -----------------------\n",
    "# Run on Spark DataFrame\n",
    "# Assume `df` is your Spark DataFrame version of df_pd,\n",
    "# and it has the columns used above: userid, employerid, employername, weekday, rn, paydate (and/or traj_date if you need to compute weekday).\n",
    "# -----------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8be3be1b-2ed3-473c-bad5-67aa19bb5936",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"traj_date\":124},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763112654254}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emb_df_fil = emb_df.where('userid = 19822535 and traj_date > \"2025-08-27\" and traj_date <= \"2025-09-27\"').toPandas()\n",
    "display(emb_df_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "829ada48-e18f-4672-8fa6-977176aa5e91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "paycycle_len = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80bd1950-a67c-4049-b7a4-9ca4d614b0ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "db = get_cluster(emb_df_fil, paycycle_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cf19aa6-796b-4c2d-8875-042417767384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_dates = []\n",
    "for i,j in zip(emb_df_fil['traj_date'].values, db.labels_):\n",
    "    if (j != -1):\n",
    "        new_dates.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2346f5a0-6c55-4939-bc96-d99be3e04dac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11cd08d1-8089-4627-bd7b-07f859e5d790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "neighbor_counts(np.stack(emb_df_fil['embedding'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd950e06-f9a6-45c7-adec-6085f75fa253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7224192601001823,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "static_moving_eyetest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
