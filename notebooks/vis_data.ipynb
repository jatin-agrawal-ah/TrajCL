{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = \"/home/sagemaker-user/TrajCL/data/porto_20200\"\n",
    "trajs = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "# class TrajDataset(Dataset):\n",
    "#     def __init__(self, data):\n",
    "#         # data: DataFrame\n",
    "#         self.data = data\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         return self.data.loc[index].merc_seq\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = trajs.shape[0]\n",
    "# train_idx = (int(l*0), 200000)\n",
    "# eval_idx = (int(l*0.7), int(l*0.8))\n",
    "# test_idx = (int(l*0.8), int(l*1.0))\n",
    "\n",
    "# train = TrajDataset(trajs[train_idx[0]: train_idx[1]])\n",
    "# eval = TrajDataset(trajs[eval_idx[0]: eval_idx[1]])\n",
    "# test = TrajDataset(trajs[test_idx[0]: test_idx[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 15\n",
    "lon_lat_list = trajs['wgs_seq'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_lat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install folium\n",
    "import folium\n",
    "\n",
    "from folium import plugins\n",
    "coordinates = [[x[0],x[1]] for x in lon_lat_list]\n",
    "# Initialize map\n",
    "m = folium.Map(location=coordinates[0], zoom_start=30)\n",
    "\n",
    "folium.Marker(\n",
    "    location=coordinates[0],\n",
    "    popup=\"Start Location\",\n",
    "    icon=folium.Icon(color='green')  # Color can be 'red', 'blue', 'green', 'purple', etc.\n",
    ").add_to(m)\n",
    "# Add markers\n",
    "for lat, lon in coordinates[1:]:\n",
    "    folium.Marker([lat, lon]).add_to(m)\n",
    "\n",
    "# Draw arrows between points\n",
    "for i in range(len(coordinates) - 1):\n",
    "    start = coordinates[i]\n",
    "    end = coordinates[i + 1]\n",
    "\n",
    "    # Draw the line\n",
    "    line = folium.PolyLine([start, end], color=\"blue\", weight=3, opacity=0.7).add_to(m)\n",
    "\n",
    "    # Add directional arrow\n",
    "    plugins.PolyLineTextPath(\n",
    "        line,\n",
    "        'âž¤',  # arrow symbol\n",
    "        repeat=True,\n",
    "        offset=7,\n",
    "        attributes={'fill': 'blue', 'font-weight': 'bold', 'font-size': '16'}\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save and show the map\n",
    "m.save(\"map_with_arrows.html\")\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.cellspace import CellSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "porto_cs = pickle.load(open(\"/home/sagemaker-user/TrajCL/data/porto_20200_cell100_cellspace.pkl\",\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_cs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_lat_list = [[-8.660646, 41.168574],\n",
    " [-8.661087, 41.167926],\n",
    " [-8.661231, 41.166576],\n",
    " [-8.660637, 41.166396],\n",
    " [-8.660295, 41.166819],\n",
    " [-8.658954, 41.168394],\n",
    " [-8.657649, 41.169906],\n",
    " [-8.656371, 41.171454],\n",
    " [-8.654706, 41.173479],\n",
    " [-8.653014, 41.17527],\n",
    " [-8.651349, 41.17644],\n",
    " [-8.652213, 41.177241],\n",
    " [-8.651358, 41.178069],\n",
    " [-8.65071, 41.178924],\n",
    " [-8.65125, 41.17968],\n",
    " [-8.649648, 41.180643],\n",
    " [-8.647515, 41.18193],\n",
    " [-8.644491, 41.183541],\n",
    " [-8.641701, 41.184108],\n",
    " [-8.638677, 41.182452],\n",
    " [-8.635284, 41.180589],\n",
    " [-8.632476, 41.179104],\n",
    " [-8.629983, 41.179365],\n",
    " [-8.629857, 41.179374],\n",
    " [-8.629857, 41.179374],\n",
    " [-8.629263, 41.179437],\n",
    " [-8.625996, 41.179779],\n",
    " [-8.622018, 41.180463],\n",
    " [-8.61885, 41.181912],\n",
    " [-8.617077, 41.182821],\n",
    " [-8.614215, 41.183082],\n",
    " [-8.613675, 41.183136],\n",
    " [-8.613702, 41.183145],\n",
    " [-8.613288, 41.18319],\n",
    " [-8.610813, 41.184],\n",
    " [-8.607474, 41.184648],\n",
    " [-8.604018, 41.183892],\n",
    " [-8.601768, 41.183253],\n",
    " [-8.601588, 41.182632],\n",
    " [-8.601957, 41.181831],\n",
    " [-8.601903, 41.181795],\n",
    " [-8.601948, 41.181822],\n",
    " [-8.601894, 41.181813]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_cs.x_min, porto_cs.x_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tool_funcs import lonlat2meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonlat2meters(lon_lat_list[0][0],lon_lat_list[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_ids = [porto_cs.get_cellid_by_point(*lonlat2meters(x[0],x[1])) for x in lon_lat_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ah_databricks_data_loader import DatabricksDataLoader\n",
    "\n",
    "# Instantiate the DatabricksDataLoader.\n",
    "ddl = DatabricksDataLoader()\n",
    "\n",
    "# List available shares.\n",
    "shares = ddl.list_shares()\n",
    "\n",
    "# Print shares.\n",
    "print(shares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9069952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/sagemaker-user/.conda/envs/trajcl/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/sagemaker-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/sagemaker-user/.ivy2/jars\n",
      "io.delta#delta-sharing-spark_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-017fc001-4bc7-4032-90b8-bd6e28757d28;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-sharing-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound io.delta#delta-sharing-client_2.12;1.0.5 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound commons-codec#commons-codec;1.11 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 364ms :: artifacts dl 17ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.11 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tio.delta#delta-sharing-client_2.12;1.0.5 from central in [default]\n",
      "\tio.delta#delta-sharing-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-017fc001-4bc7-4032-90b8-bd6e28757d28\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/10ms)\n",
      "25/09/17 17:16:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/17 17:16:45 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "118115"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ah_databricks_data_loader import DatabricksDataLoader\n",
    "\n",
    "# Instantiate the DatabricksDataLoader.\n",
    "ddl = DatabricksDataLoader()\n",
    "\n",
    "# Load data.\n",
    "df = ddl.load_as_spark(schema=\"datascience_scratchpad\", table=\"all_traj_val\")\n",
    "\n",
    "# Display the loaded DataFrame.\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for row in df.toLocalIterator():\n",
    "    print(row)\n",
    "    count+=1\n",
    "    if count>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row['traj_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = df.randomSplit([0.8, 0.1, 0.1], seed=42)  # seed for reproducibility\n",
    "train_df, val_df, test_df = splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_index = df.rdd.zipWithIndex().map(lambda x: (x[1], x[0])).toDF([\"row_index\", \"row\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_index.filter(df_with_index['row_index']==2).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(df.toLocalIterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.repartition(10).write.parquet(\"/mnt/sagemaker-nvme/data/usa/val\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = \"/home/sagemaker-user/TrajCL/data/nyc/train_with_ts/\"\n",
    "files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\".parquet\")]\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "total_count = 0\n",
    "for file_path in tqdm(files):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    total_count+=len(df)\n",
    "\n",
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(\"/home/sagemaker-user/TrajCL/data/parquet_files/nyc_v2/part-00000-76372e3b-95b4-498e-9e85-2765482fcef5-c000.snappy.parquet\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.stack(test['merc_seq_filtered'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "class ParquetDataset(IterableDataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\".parquet\")]\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file_path in self.files:\n",
    "            df = pd.read_parquet(file_path)\n",
    "            features = df[\"userid\"].values\n",
    "            for feature in features:\n",
    "                yield feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_dataset = ParquetDataset(\"/mnt/sagemaker-nvme/data/usa/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dl = DataLoader(parquet_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1118204], dtype=torch.int32)\n",
      "tensor([6679147], dtype=torch.int32)\n",
      "tensor([4422568], dtype=torch.int32)\n",
      "tensor([16068847], dtype=torch.int32)\n",
      "tensor([13086361], dtype=torch.int32)\n",
      "tensor([16079326], dtype=torch.int32)\n",
      "tensor([6811073], dtype=torch.int32)\n",
      "tensor([12946400], dtype=torch.int32)\n",
      "tensor([2620032], dtype=torch.int32)\n",
      "tensor([6800596], dtype=torch.int32)\n",
      "tensor([19365363], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for batch in dl:\n",
    "    print(batch)\n",
    "    count+=1\n",
    "    if count>10:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trajcl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
