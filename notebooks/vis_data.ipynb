{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = \"/home/sagemaker-user/TrajCL/data/porto_20200\"\n",
    "trajs = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "# class TrajDataset(Dataset):\n",
    "#     def __init__(self, data):\n",
    "#         # data: DataFrame\n",
    "#         self.data = data\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         return self.data.loc[index].merc_seq\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = trajs.shape[0]\n",
    "# train_idx = (int(l*0), 200000)\n",
    "# eval_idx = (int(l*0.7), int(l*0.8))\n",
    "# test_idx = (int(l*0.8), int(l*1.0))\n",
    "\n",
    "# train = TrajDataset(trajs[train_idx[0]: train_idx[1]])\n",
    "# eval = TrajDataset(trajs[eval_idx[0]: eval_idx[1]])\n",
    "# test = TrajDataset(trajs[test_idx[0]: test_idx[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 15\n",
    "lon_lat_list = trajs['wgs_seq'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_lat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install folium\n",
    "import folium\n",
    "\n",
    "from folium import plugins\n",
    "coordinates = [[x[0],x[1]] for x in lon_lat_list]\n",
    "# Initialize map\n",
    "m = folium.Map(location=coordinates[0], zoom_start=30)\n",
    "\n",
    "folium.Marker(\n",
    "    location=coordinates[0],\n",
    "    popup=\"Start Location\",\n",
    "    icon=folium.Icon(color='green')  # Color can be 'red', 'blue', 'green', 'purple', etc.\n",
    ").add_to(m)\n",
    "# Add markers\n",
    "for lat, lon in coordinates[1:]:\n",
    "    folium.Marker([lat, lon]).add_to(m)\n",
    "\n",
    "# Draw arrows between points\n",
    "for i in range(len(coordinates) - 1):\n",
    "    start = coordinates[i]\n",
    "    end = coordinates[i + 1]\n",
    "\n",
    "    # Draw the line\n",
    "    line = folium.PolyLine([start, end], color=\"blue\", weight=3, opacity=0.7).add_to(m)\n",
    "\n",
    "    # Add directional arrow\n",
    "    plugins.PolyLineTextPath(\n",
    "        line,\n",
    "        'âž¤',  # arrow symbol\n",
    "        repeat=True,\n",
    "        offset=7,\n",
    "        attributes={'fill': 'blue', 'font-weight': 'bold', 'font-size': '16'}\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save and show the map\n",
    "m.save(\"map_with_arrows.html\")\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.cellspace import CellSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "porto_cs = pickle.load(open(\"/home/sagemaker-user/TrajCL/data/porto_20200_cell100_cellspace.pkl\",\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_cs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_lat_list = [[-8.660646, 41.168574],\n",
    " [-8.661087, 41.167926],\n",
    " [-8.661231, 41.166576],\n",
    " [-8.660637, 41.166396],\n",
    " [-8.660295, 41.166819],\n",
    " [-8.658954, 41.168394],\n",
    " [-8.657649, 41.169906],\n",
    " [-8.656371, 41.171454],\n",
    " [-8.654706, 41.173479],\n",
    " [-8.653014, 41.17527],\n",
    " [-8.651349, 41.17644],\n",
    " [-8.652213, 41.177241],\n",
    " [-8.651358, 41.178069],\n",
    " [-8.65071, 41.178924],\n",
    " [-8.65125, 41.17968],\n",
    " [-8.649648, 41.180643],\n",
    " [-8.647515, 41.18193],\n",
    " [-8.644491, 41.183541],\n",
    " [-8.641701, 41.184108],\n",
    " [-8.638677, 41.182452],\n",
    " [-8.635284, 41.180589],\n",
    " [-8.632476, 41.179104],\n",
    " [-8.629983, 41.179365],\n",
    " [-8.629857, 41.179374],\n",
    " [-8.629857, 41.179374],\n",
    " [-8.629263, 41.179437],\n",
    " [-8.625996, 41.179779],\n",
    " [-8.622018, 41.180463],\n",
    " [-8.61885, 41.181912],\n",
    " [-8.617077, 41.182821],\n",
    " [-8.614215, 41.183082],\n",
    " [-8.613675, 41.183136],\n",
    " [-8.613702, 41.183145],\n",
    " [-8.613288, 41.18319],\n",
    " [-8.610813, 41.184],\n",
    " [-8.607474, 41.184648],\n",
    " [-8.604018, 41.183892],\n",
    " [-8.601768, 41.183253],\n",
    " [-8.601588, 41.182632],\n",
    " [-8.601957, 41.181831],\n",
    " [-8.601903, 41.181795],\n",
    " [-8.601948, 41.181822],\n",
    " [-8.601894, 41.181813]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_cs.x_min, porto_cs.x_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tool_funcs import lonlat2meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonlat2meters(lon_lat_list[0][0],lon_lat_list[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_ids = [porto_cs.get_cellid_by_point(*lonlat2meters(x[0],x[1])) for x in lon_lat_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ah_databricks_data_loader import DatabricksDataLoader\n",
    "\n",
    "# Instantiate the DatabricksDataLoader.\n",
    "ddl = DatabricksDataLoader()\n",
    "\n",
    "# List available shares.\n",
    "shares = ddl.list_shares()\n",
    "\n",
    "# Print shares.\n",
    "print(shares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spark.sql.extensions': 'io.delta.sql.DeltaSparkSessionExtension',\n",
       " 'spark.sql.catalog.spark_catalog': 'org.apache.spark.sql.delta.catalog.DeltaCatalog',\n",
       " 'fs.s3a.aws.credentials.provider': 'com.amazonaws.auth.ContainerCredentialsProvider',\n",
       " 'spark.driver.memory': '16g',\n",
       " 'spark.executor.memory': '16g',\n",
       " 'spark.executor.cores': '2',\n",
       " 'spark.executor.instances': '4'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/sagemaker-user/.conda/envs/trajcl/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/sagemaker-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/sagemaker-user/.ivy2/jars\n",
      "io.delta#delta-sharing-spark_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c194df8d-4963-4e76-8e0b-e39e280e2ba3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-sharing-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound io.delta#delta-sharing-client_2.12;1.0.5 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound commons-codec#commons-codec;1.11 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 331ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.11 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tio.delta#delta-sharing-client_2.12;1.0.5 from central in [default]\n",
      "\tio.delta#delta-sharing-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c194df8d-4963-4e76-8e0b-e39e280e2ba3\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/9ms)\n",
      "25/09/13 13:42:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/13 13:42:30 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "25/09/13 13:42:42 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31447648"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ah_databricks_data_loader import DatabricksDataLoader\n",
    "from ah_databricks_data_loader.spark_session_builder import SparkSessionBuilder\n",
    "\n",
    "share_name = \"sagemaker_share_prod\"\n",
    "schema = \"datascience_scratchpad\"\n",
    "table = \"all_traj_train\"\n",
    "\n",
    "# Instantiate the DatabricksDataLoader.\n",
    "spark_params = {\n",
    "    \"driver_memory\": \"16g\",\n",
    "    \"executor_memory\": \"8g\",\n",
    "    \"executor_cores\": \"2\",\n",
    "    \"executor_instances\": \"4\"\n",
    "    \n",
    "}\n",
    "\n",
    "ddl = DatabricksDataLoader( )\n",
    "builder = SparkSessionBuilder(**spark_params)\n",
    "builder.configs.update({\"spark.local.dir\":\"/home/sagemaker-user/tmp\"})\n",
    "spark_session = builder.get_or_create_spark_session()\n",
    "temp_file = ddl._get_delta_sharing_config()\n",
    "config_path = ddl._create_delta_sharing_url(\n",
    "    file_name=temp_file.name, share_name=share_name, schema=schema, table=table\n",
    ")\n",
    "\n",
    "df =spark_session.read.format(\"deltaSharing\").option(\"responseFormat\", \"delta\").load(config_path)\n",
    "\n",
    "# Display the loaded DataFrame.\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for row in df.toLocalIterator():\n",
    "    print(row)\n",
    "    count+=1\n",
    "    if count>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row['traj_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = df.randomSplit([0.8, 0.1, 0.1], seed=42)  # seed for reproducibility\n",
    "train_df, val_df, test_df = splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_index = df.rdd.zipWithIndex().map(lambda x: (x[1], x[0])).toDF([\"row_index\", \"row\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_index.filter(df_with_index['row_index']==2).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(df.toLocalIterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:==================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+--------------------+--------------------+\n",
      "| userid| traj_date|          timestamps|             wgs_seq|            merc_seq|\n",
      "+-------+----------+--------------------+--------------------+--------------------+\n",
      "|4421213|2022-11-13|[2022-11-13 05:42...|[[-73.781794, 42....|[[-8213351.737894...|\n",
      "|4421213|2022-11-14|[2022-11-14 00:43...|[[-73.7817955, 42...|[[-8213351.904873...|\n",
      "|4421213|2022-11-15|[2022-11-15 08:25...|[[-73.7743822, 42...|[[-8212526.660092...|\n",
      "|4421213|2022-11-16|[2022-11-16 00:06...|[[-73.781837, 42....|[[-8213356.524632...|\n",
      "|4421213|2022-11-17|[2022-11-17 00:44...|[[-73.7818355, 42...|[[-8213356.357653...|\n",
      "|4421213|2022-11-18|[2022-11-18 00:40...|[[-73.7792254, 42...|[[-8213065.802650...|\n",
      "|4421213|2022-11-19|[2022-11-19 00:00...|[[-73.7818073, 42...|[[-8213353.218443...|\n",
      "|4421213|2022-11-20|[2022-11-20 04:59...|[[-73.7818337, 42...|[[-8213356.157277...|\n",
      "|4421213|2022-11-21|[2022-11-21 00:40...|[[-73.7818223, 42...|[[-8213354.888235...|\n",
      "|4421213|2022-11-22|[2022-11-22 05:46...|[[-73.7818135, 42...|[[-8213353.908624...|\n",
      "|4421213|2022-11-23|[2022-11-23 00:01...|[[-73.7795931, 42...|[[-8213106.734826...|\n",
      "|4421213|2022-11-24|[2022-11-24 00:10...|[[-73.781899, 42....|[[-8213363.426440...|\n",
      "|4421213|2022-11-25|[2022-11-25 13:30...|[[-72.2102845, 41...|[[-8038412.100577...|\n",
      "|4421213|2022-11-26|[2022-11-26 04:41...|[[-72.2154987, 41...|[[-8038992.542666...|\n",
      "|4421213|2022-11-27|[2022-11-27 00:01...|[[-73.7792254, 42...|[[-8213065.802650...|\n",
      "|4421213|2022-11-28|[2022-11-28 03:21...|[[-73.7817918, 42...|[[-8213351.492991...|\n",
      "|4421213|2022-11-29|[2022-11-29 00:57...|[[-73.7817832, 42...|[[-8213350.535643...|\n",
      "|4421213|2022-11-30|[2022-11-30 00:15...|[[-73.7792254, 42...|[[-8213065.802650...|\n",
      "|4421213|2022-12-01|[2022-12-01 00:45...|[[-73.7818017, 42...|[[-8213352.595054...|\n",
      "|4421213|2022-12-04|[2022-12-04 02:36...|[[-73.7818155, 42...|[[-8213354.131263...|\n",
      "+-------+----------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.repartition(1000).write.parquet(\"/mnt/sagemaker-nvme/usa/train\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = \"/home/sagemaker-user/TrajCL/data/nyc/train_with_ts/\"\n",
    "files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\".parquet\")]\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "total_count = 0\n",
    "for file_path in tqdm(files):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    total_count+=len(df)\n",
    "\n",
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(\"/home/sagemaker-user/TrajCL/data/parquet_files/nyc_v2/part-00000-76372e3b-95b4-498e-9e85-2765482fcef5-c000.snappy.parquet\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.stack(test['merc_seq_filtered'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "class ParquetDataset(IterableDataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\".parquet\")]\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file_path in self.files:\n",
    "            df = pd.read_parquet(file_path)\n",
    "            features = df[\"userid\"].values\n",
    "            for feature in features:\n",
    "                yield feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_dataset = ParquetDataset(\"/mnt/sagemaker-nvme/data/usa/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dl = DataLoader(parquet_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1118204], dtype=torch.int32)\n",
      "tensor([6679147], dtype=torch.int32)\n",
      "tensor([4422568], dtype=torch.int32)\n",
      "tensor([16068847], dtype=torch.int32)\n",
      "tensor([13086361], dtype=torch.int32)\n",
      "tensor([16079326], dtype=torch.int32)\n",
      "tensor([6811073], dtype=torch.int32)\n",
      "tensor([12946400], dtype=torch.int32)\n",
      "tensor([2620032], dtype=torch.int32)\n",
      "tensor([6800596], dtype=torch.int32)\n",
      "tensor([19365363], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for batch in dl:\n",
    "    print(batch)\n",
    "    count+=1\n",
    "    if count>10:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trajcl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
