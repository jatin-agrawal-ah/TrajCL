{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc253c63-ee49-4c99-8d61-d0b6b7fdceae",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758645354788}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select * from clientstate.full_locations\n",
    "\"\"\"\n",
    "\n",
    "df = spark.sql(query)\n",
    "# df.createOrReplaceTempView(\"gps_data\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e21a9e0b-402f-4761-b14e-575ac6468643",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759131544038}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_fil = df.where(\n",
    "  \"to_date(from_unixtime(location_timestamp/1000)) >= date_sub(current_date(), 20) and \" +\n",
    "  \"to_date(from_unixtime(location_timestamp/1000)) < current_date()\"\n",
    ")\n",
    "display(df_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e8ac91-c09e-4401-aa94-46f9754930d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_fil.selectExpr(\"min(location_timestamp)\", \"max(location_timestamp)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6f658f7-9d66-492a-8898-20679ca86953",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "userpiphistory = spark.read.table(\"main_prod.datascience.userpiphistory\")\n",
    "\n",
    "# keep only valid times (optional but wise)\n",
    "userpiphistory = userpiphistory.filter(F.col(\"createdon\").isNotNull())\n",
    "\n",
    "userid_timezone_df = (\n",
    "    userpiphistory.groupBy(\"userid\")\n",
    "      .agg(F.max(F.struct(\"createdon\", \"timezone\")).alias(\"maxrow\"))\n",
    "      .select(\n",
    "          \"userid\",\n",
    "          F.col(\"maxrow.timezone\").alias(\"timezone\"),\n",
    "      )\n",
    ").where(F.col(\"timezone\").isNotNull())\n",
    "\n",
    "display(userid_timezone_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5321f53-99c5-438f-bd34-e3a8f89f80dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_tz = df_fil.join(userid_timezone_df, [\"userid\"], \"inner\")\n",
    "\n",
    "df_with_tz.createOrReplaceTempView(\"df_with_tz\")\n",
    "display(df_with_tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39be73c3-197d-4959-b073-dc9ae45cd16f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select distinct userid, latitude, longitude, from_utc_timestamp(from_unixtime(location_timestamp / 1000), timezone) AS localized_timestamp, timezone from df_with_tz\"\"\"\n",
    "\n",
    "df = spark.sql(query)\n",
    "df.createOrReplaceTempView(\"gps_data_loc_ts\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fade3208-9fb2-4790-99e7-f721ce805e22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_v2 = df.where(\"latitude is not NULL and longitude is not NULL and localized_timestamp is not NULL and timezone is not NULL\")\n",
    "display(df_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ff69bc7-2a4a-4ada-9321-0274da73bfd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def get_current_date_in_tz(tz):\n",
    "    try:\n",
    "        tz_time = datetime.now(ZoneInfo(tz))\n",
    "        tz_date = tz_time.date()\n",
    "        return tz_date\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "get_current_date_in_tz_udf = udf(get_current_date_in_tz, DateType())\n",
    "\n",
    "df_v3  = df_v2.withColumn(\"current_tz_date\", get_current_date_in_tz_udf(\"timezone\"))\n",
    "display(df_v3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fd0a312-4a12-46a3-b5c7-599ed304cbc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_v4 = df_v3.where(\"current_tz_date is not NULL\")\n",
    "display(df_v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88abfcda-4e1c-4d49-9ca8-a1364b35d278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_v4.createOrReplaceTempView(\"all_traj_data_loc_ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5905a61-b50e-4bbf-9d93-91e4f31641a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    userid,\n",
    "    DATE(localized_timestamp) AS traj_date,\n",
    "    COLLECT_LIST(localized_timestamp) AS timestamps,\n",
    "    COLLECT_LIST(latitude) AS latitudes,\n",
    "    COLLECT_LIST(longitude) AS longitudes\n",
    "FROM \n",
    "    all_traj_data_loc_ts\n",
    "GROUP BY \n",
    "    userid, DATE(localized_timestamp)\n",
    "ORDER BY \n",
    "    userid, traj_date\n",
    "\"\"\"\n",
    "\n",
    "result_df = spark.sql(query)\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dd81307-b5e3-4797-879d-5f4764109698",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, DoubleType, DateType, TimestampType\n",
    "def sort_by_time(timestamps, longitudes, latitudes):\n",
    "    sorted_ts = []\n",
    "    polylines = []\n",
    "    for ts, lon, lat in sorted(zip(timestamps, longitudes, latitudes), key=lambda x: x[0]):\n",
    "        sorted_ts.append(ts)\n",
    "        polylines.append([float(lon), float(lat)])\n",
    "    return sorted_ts, polylines\n",
    "\n",
    "\n",
    "sort_and_extract_udf = udf(sort_by_time, \n",
    "                           StructType([\n",
    "                               StructField(\"sorted_ts\", ArrayType(TimestampType())),\n",
    "                               StructField(\"polylines\", ArrayType(ArrayType(DoubleType())))\n",
    "                           ]))\n",
    "\n",
    "\n",
    "result_df_v2 = result_df.withColumn(\"sorted_data\", \n",
    "                                         sort_and_extract_udf(\"timestamps\", \"longitudes\", \"latitudes\"))\n",
    "\n",
    "result_df_v3 = result_df_v2.withColumn(\"sorted_ts\", col(\"sorted_data.sorted_ts\")) \\\n",
    "                             .withColumn(\"wgs_seq\", col(\"sorted_data.polylines\")) \\\n",
    "                             .drop(\"sorted_data\", \"timestamps\", \"longitudes\", \"latitudes\")\n",
    "display(result_df_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58993789-5a04-4613-ad01-43162033fc1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# result_df_v3 = result_df_v2.drop(\"timestamps\", \"longitudes\", \"latitudes\")\n",
    "# display(result_df_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05a24bd3-b580-4e4f-808c-54450c6e7c7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, ArrayType, DoubleType, BooleanType\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "import math\n",
    "def lonlat2meters(lon, lat):\n",
    "    semimajoraxis = 6378137.0\n",
    "    east = lon * 0.017453292519943295\n",
    "    north = lat * 0.017453292519943295\n",
    "    t = math.sin(north)\n",
    "    return semimajoraxis * east, 3189068.5 * math.log((1 + t + 1e-5) / (1 - t + 1e-5))\n",
    "\n",
    "\n",
    "lonlat2meters_udf = udf(lambda traj: [list(lonlat2meters(p[0], p[1])) for p in traj], ArrayType(ArrayType(DoubleType())))\n",
    "result_df_v4 = result_df_v3.withColumn(\"merc_seq\", lonlat2meters_udf(col(\"wgs_seq\")))\n",
    "display(result_df_v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "740f5b44-b507-465b-b1f8-d921d1f903a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# result_df_v4.write.mode(\"overwrite\").saveAsTable(\"main_prod.datascience_scratchpad.result_df_v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "068e5bed-a298-4833-b59a-708b76b42adb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def filter_based_on_timestamps(ts_list):\n",
    "    unique_hours = set()\n",
    "    for ts in ts_list:\n",
    "        unique_hours.add(ts.hour)\n",
    "    if len(unique_hours) >= 7:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "filter_based_on_timestamps_udf = udf(filter_based_on_timestamps, BooleanType())\n",
    "result_df_v5 = result_df_v4.filter(filter_based_on_timestamps_udf(col(\"sorted_ts\")))\n",
    "display(result_df_v5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2535f9ef-b2d8-49fb-bce9-fe5571819710",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, ArrayType, DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "def udf_get_min_max_lat_lon(wgs_seq):\n",
    "    min_lat = 100000\n",
    "    max_lat = -100000\n",
    "    min_lon = 100000\n",
    "    max_lon = -100000\n",
    "\n",
    "    for lon, lat in wgs_seq:\n",
    "        if lat < min_lat:\n",
    "            min_lat = lat\n",
    "        if lat > max_lat:\n",
    "            max_lat = lat\n",
    "        if lon < min_lon:\n",
    "            min_lon = lon\n",
    "        if lon > max_lon:\n",
    "            max_lon = lon\n",
    "    return min_lat, max_lat, min_lon, max_lon\n",
    "\n",
    "get_min_max_lat_lon_udf = udf(udf_get_min_max_lat_lon, StructType([\n",
    "    StructField(\"min_lat\", DoubleType()),\n",
    "    StructField(\"max_lat\", DoubleType()),\n",
    "    StructField(\"min_lon\", DoubleType()),\n",
    "    StructField(\"max_lon\", DoubleType())\n",
    "]))\n",
    "result_df_v6 = result_df_v5.withColumn(\"min_max_lat_lon\", get_min_max_lat_lon_udf(col(\"wgs_seq\")))\n",
    "result_df_v6 = result_df_v6.withColumn(\"min_lat\", col(\"min_max_lat_lon.min_lat\")) \\\n",
    "       .withColumn(\"max_lat\", col(\"min_max_lat_lon.max_lat\")) \\\n",
    "       .withColumn(\"min_lon\", col(\"min_max_lat_lon.min_lon\")) \\\n",
    "       .withColumn(\"max_lon\", col(\"min_max_lat_lon.max_lon\")) \\\n",
    "       .drop(\"min_max_lat_lon\")\n",
    "\n",
    "display(result_df_v6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d1f05ed-3e1c-4bab-86e9-5d6e77e8575b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target_min_lat = 25.11833\n",
    "target_max_lat = 49.38447\n",
    "target_min_lon = -124.73306\n",
    "target_max_lon = -66.94978\n",
    "\n",
    "df_fil = result_df_v6.filter((col(\"min_lat\") >= target_min_lat) & (col(\"max_lat\") <= target_max_lat) & (col(\"min_lon\") >= target_min_lon) & (col(\"max_lon\") <= target_max_lon))\n",
    "df_fil = df_fil.drop(\"min_lat\", \"max_lat\", \"min_lon\", \"max_lon\")\n",
    "display(df_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d512daea-beb6-4f4d-9832-4b934fd34675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# MERGE INTO main_prod.datascience_scratchpad.traj_data AS target\n",
    "# USING traj_data AS source\n",
    "# ON target.userid = source.userid\n",
    "#    AND target.traj_date = source.traj_date\n",
    "# WHEN MATCHED THEN \n",
    "#   UPDATE SET *\n",
    "# WHEN NOT MATCHED THEN\n",
    "#   INSERT *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aacbb034-8864-45fb-a3f9-dd68a688b70d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_fil.repartition(100).write.mode(\"overwrite\").parquet(\"/Volumes/main_prod/datascience_scratchpad/jatin/trajcl_exp/usa/last_20_days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ce6e4ae-f56d-4b8e-8747-c6ee3a0bfdde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-09-23 22_03_42",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
