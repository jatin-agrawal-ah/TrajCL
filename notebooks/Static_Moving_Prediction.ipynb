{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b66c78b-9d67-488c-9d0e-9c966299d73f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select * from main_prod.earnings_analysis.fact_user_earnings_daily where paydate = current_date - 1\n",
    "\"\"\"\n",
    "\n",
    "df = spark.sql(query)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "379e017c-343f-4f90-b657-a280d5626edd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pos = df.where('total_pck_amt > 0')\n",
    "# display(df_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb423b0-f3df-4e87-a728-f169041450ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_pos.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4768a4fa-e7c5-45e7-938e-9ae9e9b3cb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get userid employername and employerid\n",
    "\n",
    "\n",
    "userid_emp = df_pos.select('userid','employerid').distinct()\n",
    "# display(userid_emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6b4980f-9bb7-42bc-81f8-1b297adf5db2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# userid_emp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ac6f92-2656-4a30-a455-3950354e0724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select userid, employerid, employername, paydate, prev_paydate, total_pck_amt from main_prod.earnings_analysis.fact_user_earnings_daily where paydate is not NULL and paydate<= current_date -1 \n",
    "\"\"\"\n",
    "\n",
    "facts_df = spark.sql(query)\n",
    "# display(facts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "878ef026-1829-434a-9544-cdf5b42d7f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "facts_df = facts_df.distinct()\n",
    "# display(facts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "005710f0-fc21-42cb-a1d4-f4ecebeae23f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "facts_df_fil= facts_df.join(userid_emp, ['userid','employerid'])\n",
    "# display(facts_df_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "792262f3-a4fc-4f40-b98e-fa623b9d40a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# facts_df_fil.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11d2f041-b315-4b9a-8e33-6e6a4a46c5c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for each userid, empolyerid pair keep the last 4 paydates and corresponding total_pck_amt\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "window = Window.partitionBy('userid','employerid').orderBy(desc('paydate'))\n",
    "facts_df_fil_2 = facts_df_fil.withColumn('rn', row_number().over(window))\n",
    "facts_df_fil_2 = facts_df_fil_2.where('rn <= 4 and total_pck_amt>0')\n",
    "# display(facts_df_fil_2)\n",
    "# for each userid, empolyerid pair keep the last 4 paydates and corresponding total_pck_amt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96403891-90bc-4cbd-aea7-98b944f59fb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# facts_df_pd = facts_df_fil_2.toPandas()\n",
    "# facts_df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0318c113-dac4-4e5a-8159-cbc1f2cb304c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758794477698}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "traj_emb_df = spark.read.table(\"main_prod.datascience_scratchpad.traj_emb\")\n",
    "\n",
    "display(traj_emb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8854e2c1-4374-447d-95a3-4ac71b2a0810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joined = (\n",
    "    facts_df_fil_2.join(\n",
    "        broadcast(traj_emb_df),  # remove broadcast() if df1 is large\n",
    "        (facts_df_fil_2.userid == traj_emb_df.userid)\n",
    "        & (traj_emb_df.traj_date >= facts_df_fil_2.prev_paydate)   # for closed interval use <= below\n",
    "        & (traj_emb_df.traj_date < facts_df_fil_2.paydate),\n",
    "        \"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# display(joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a9da4e9-48e7-4669-bb94-2f1603a28c29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate per interval\n",
    "facts_df_count = (\n",
    "    joined.groupBy(facts_df_fil_2.userid, facts_df_fil_2.prev_paydate, facts_df_fil_2.paydate, facts_df_fil_2.rn)\n",
    "          .agg(F.count(traj_emb_df.traj_date).alias(\"count\"))\n",
    "          .select(\"userid\", \"prev_paydate\", \"paydate\", \"rn\", \"count\")\n",
    ")\n",
    "\n",
    "# display(facts_df_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "286b9119-6e11-4267-be4a-4663c3ad88a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def filter_by_count(prev_date, current_date, count):\n",
    "    n_days = (current_date - prev_date).days\n",
    "    if count >= n_days*0.6:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "df_filtered_by_count = (\n",
    "    facts_df_count\n",
    "    .filter(F.udf(filter_by_count, returnType=BooleanType())(col(\"prev_paydate\"), col(\"paydate\"), col(\"count\")))\n",
    ")\n",
    "# display(df_filtered_by_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4732f163-fdc5-4fa5-99b1-fe0cff57bc0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_facts_df = facts_df_fil_2.join(df_filtered_by_count, [\"userid\", \"prev_paydate\", \"paydate\", \"rn\"])\n",
    "# display(final_facts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b00f443f-2121-4475-98a8-0d9edeec8783",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759254706819}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.functions import col\n",
    "df = (\n",
    "    traj_emb_df.join(\n",
    "        broadcast(final_facts_df),\n",
    "        (traj_emb_df.userid == final_facts_df.userid)\n",
    "        & (col(\"traj_date\") >= col(\"prev_paydate\"))\n",
    "        & (col(\"traj_date\") < col(\"paydate\")),\n",
    "        \"inner\"       # use \"left\" if you want to keep df2 rows without a matching interval\n",
    "    )\n",
    "    .select(traj_emb_df.userid, final_facts_df.employerid, final_facts_df.employername, traj_emb_df.traj_date, traj_emb_df.embedding, final_facts_df.rn)\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2beda12-082e-4fb1-89e9-05954f41130d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pd = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5636558d-72dd-45a4-bdf9-7d345dff5317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pd['weekday'] = df_pd['traj_date'].apply(lambda x: x.weekday())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab174f4a-a0cd-4b7b-922b-2f5ef259c00f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f31a9db-b0ab-406c-85c5-bc378b3250bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN \n",
    "def apply_dbscan(embs, target_min_similarity=0.9):\n",
    "    eps = 1.0 - target_min_similarity    # cosine distance threshold\n",
    "    n_embs = embs.shape[0]\n",
    "    db = DBSCAN(eps=eps, min_samples=max(int(n_embs*0.25), 4), metric=\"cosine\", n_jobs=-1).fit(embs)\n",
    "    return db\n",
    "\n",
    "def get_cluster(df):\n",
    "    embeddings = np.stack(df['embedding'].values)\n",
    "    db = apply_dbscan(embeddings)\n",
    "    return db\n",
    "\n",
    "def cluster_exists(db):\n",
    "    labels = db.labels_\n",
    "    for label in labels:\n",
    "        if label != -1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def dbscan_predict_all(db, X_train, X_new):\n",
    "    nn = NearestNeighbors(radius=db.eps, metric=db.metric).fit(X_train)\n",
    "    dists, idxs = nn.radius_neighbors(X_new, return_distance=True)\n",
    "    y = db.labels_\n",
    "    pred = np.full(len(X_new), -1, dtype=int)\n",
    "    for i, (di, ii) in enumerate(zip(dists, idxs)):\n",
    "        if len(ii) == 0: \n",
    "            continue\n",
    "        lbls, di = y[ii], di\n",
    "        mask = lbls != -1\n",
    "        if mask.any():\n",
    "            pred[i] = lbls[mask][np.argmin(di[mask])]\n",
    "    return pred\n",
    "    \n",
    "def works_on_weekends_fn(db, weekday_df, weekend_df):\n",
    "    if len(weekday_df) == 0 or len(weekend_df) == 0:\n",
    "        return False\n",
    "    weekday_embs = np.stack(weekday_df['embedding'].values)\n",
    "    weekend_embs = np.stack(weekend_df['embedding'].values)\n",
    "    pred = dbscan_predict_all(db, weekday_embs, weekend_embs)\n",
    "    # if 40% of pred is not -1, then it works on weekends\n",
    "    pred_not_neg = pred[pred != -1]\n",
    "    return len(pred_not_neg) > 0.4*len(pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b29f6b3f-fdf2-4c27-be10-5500e23d8c0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1850762d-df4a-4480-8795-eb7b70ee22e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# current date\n",
    "today = datetime.date.today()\n",
    "visited_userid_employerid = set()\n",
    "output_list = []\n",
    "for i, row in tqdm(df_pd.iterrows()):\n",
    "    if (row['userid'], row['employerid']) not in visited_userid_employerid:\n",
    "        visited_userid_employerid.add((row['userid'], row['employerid']))\n",
    "        latest_weekday_df = df_pd.loc[(df_pd['userid'] == row['userid']) & (df_pd['employerid'] == row['employerid']) & (df_pd['weekday'].isin([0, 1, 2, 3, 4])) & (df_pd['rn'] == 1)]\n",
    "        latest_weekend_df = df_pd.loc[(df_pd['userid'] == row['userid']) & (df_pd['employerid'] == row['employerid']) & (df_pd['weekday'].isin([5, 6])) & (df_pd['rn'] == 1)]\n",
    "        old_weekday_df = df_pd.loc[(df_pd['userid'] == row['userid']) & (df_pd['employerid'] == row['employerid']) & (df_pd['weekday'].isin([0, 1, 2, 3, 4])) & (df_pd['rn'] != 1)]\n",
    "        old_weekend_df = df_pd.loc[(df_pd['userid'] == row['userid']) & (df_pd['employerid'] == row['employerid']) & (df_pd['weekday'].isin([5, 6])) & (df_pd['rn'] != 1)]\n",
    "        # print(row['userid'], row['employerid'],len(latest_weekday_df), len(latest_weekend_df), len(old_weekday_df), len(old_weekend_df))\n",
    "        if len(latest_weekday_df) == 0:\n",
    "            continue\n",
    "        old_latest_cluster = get_cluster(pd.concat([latest_weekday_df, old_weekday_df]))\n",
    "        if cluster_exists(old_latest_cluster):\n",
    "            work_type = \"static\"\n",
    "            works_on_weekends = works_on_weekends_fn(old_latest_cluster, pd.concat([latest_weekday_df, old_weekday_df]), pd.concat([latest_weekend_df, old_weekend_df]))\n",
    "        else:\n",
    "            old_latest_cluster = get_cluster(pd.concat([latest_weekday_df, latest_weekend_df, old_weekday_df, old_weekend_df]))\n",
    "            if cluster_exists(old_latest_cluster):\n",
    "                work_type = \"static\"\n",
    "                works_on_weekends = True\n",
    "            else:\n",
    "                latest_cluster = get_cluster(pd.concat([latest_weekday_df, latest_weekend_df]))\n",
    "                if cluster_exists(latest_cluster):\n",
    "                    work_type = \"static\"\n",
    "                    works_on_weekends = works_on_weekends_fn(latest_cluster, latest_weekday_df, latest_weekend_df)\n",
    "                else:\n",
    "                    latest_cluster = get_cluster(pd.concat([latest_weekday_df, latest_weekend_df]))\n",
    "                    if cluster_exists(latest_cluster):\n",
    "                        work_type = \"static\"\n",
    "                        works_on_weekends = True\n",
    "                    else:\n",
    "                        work_type = \"moving\"\n",
    "                        rks_on_weekends = False\n",
    "        output = {\n",
    "            \"userid\": row['userid'],\n",
    "            \"employerid\": row['employerid'],\n",
    "            \"employername\": row['employername'],\n",
    "            \"predicted_work_type\": work_type,\n",
    "            \"predicted_on\": today,\n",
    "            \"works_on_weekends\": works_on_weekends\n",
    "        }\n",
    "        output_list.append(output)\n",
    "\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a54c9a4-2acd-435d-93a7-cb60a5e907c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f553f09-0378-4423-aaee-89406427432e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a df from output_list\n",
    "\n",
    "output_df = spark.createDataFrame(output_list)\n",
    "\n",
    "display(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33eea1c5-459a-40e9-b2e2-f3e502fd63ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "output_df = output_df.withColumn(\"userid\", col(\"userid\").cast(IntegerType()))\n",
    "output_df = output_df.withColumn(\"employerid\", col(\"employerid\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc897854-c7f6-4205-916f-3712a77ecf1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56ee9761-6196-40d0-8fdd-114a27d48958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_df.where('predicted_work_type = \"moving\"').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93edf8ee-0aba-4f34-855a-8e9a1c4f1188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# userid = 23122754\n",
    "# df_pd[df_pd['userid']==userid].sort_values(\"traj_date\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cada198-b5af-47cc-836d-85407a100553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# test_df = df_pd[df_pd['userid']==userid].sort_values(\"traj_date\").reset_index(drop=True)\n",
    "# embs = np.stack(test_df['embedding'].to_list())\n",
    "# target_min_similarity = 0.9\n",
    "# eps = 1.0 - target_min_similarity    # cosine distance threshold\n",
    "# n_embs = embs.shape[0]\n",
    "# db = DBSCAN(eps=eps, min_samples=5, metric=\"cosine\", n_jobs=-1).fit(embs)\n",
    "# labels = db.labels_\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c76cbe59-26ac-4a88-98e3-087f11c648d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # print cosine similarity between all\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# for i in range(n_embs):\n",
    "#     for j in range(i+1, n_embs):\n",
    "#         print(test_df['traj_date'][i], test_df['traj_date'][j], cosine_similarity(embs[i].reshape(1,-1), embs[j].reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05566265-8a09-47c4-9f43-e60d1c4c88a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e4f9ceb-3a8d-47f5-8475-1d7cf29dcbe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # # create empty table in df\n",
    "\n",
    "# from pyspark.sql.types import *\n",
    "# from pyspark.sql.functions import *\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField(\"userid\", IntegerType(), True),\n",
    "#     StructField(\"employerid\", IntegerType(), True),\n",
    "#     StructField(\"employername\", StringType(), True),\n",
    "#     StructField(\"predicted_work_type\", StringType(), True),\n",
    "#     StructField(\"predicted_on\", DateType(), True),\n",
    "#     StructField(\"works_on_weekends\", BooleanType(), True)\n",
    "# ])\n",
    "\n",
    "# empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "# empty_df.write.mode(\"overwrite\").saveAsTable(\"main_prod.datascience_scratchpad.static_moving_worktype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f401434a-dabd-487c-b3dc-8fd378660d4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c09d600-a783-40ed-a1db-e72ffe5645ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_df.write.mode(\"append\").saveAsTable(\"main_prod.datascience_scratchpad.static_moving_worktype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd67d8d2-e04a-405f-97fa-1cc047004cf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(spark.read.table(\"main_prod.datascience_scratchpad.static_moving_worktype\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7f7def5-3889-4b64-8ba3-3359564e5b15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Static_Moving_Prediction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
