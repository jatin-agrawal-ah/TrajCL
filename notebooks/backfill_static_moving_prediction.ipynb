{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "506b5c4b-8690-4801-8787-5092c69bcc07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from config_infer import InferenceConfig\n",
    "cfg = InferenceConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3580b78-49f7-40db-9eaf-3edf099716c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select * from main_prod.earnings_analysis.fact_user_earnings_daily where total_pck_amt > 0 and paydate is not NULL and paydate>= current_date - 5\n",
    "\"\"\"\n",
    "\n",
    "df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50e0735-5319-427d-8573-836950acf110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "userid_emp = df.select('userid','employerid').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64a0a04a-e834-483e-90fe-a4ba64055031",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759917673156}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(userid_emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f96dd81-2d67-46e4-909b-1cda098d3af7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select userid, employerid, employername, paydate, prev_paydate, total_pck_amt from main_prod.earnings_analysis.fact_user_earnings_daily where paydate is not NULL and paydate<= current_date -1\n",
    "\"\"\"\n",
    "facts_df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b42d112-c517-402a-900f-d59fe2377257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "facts_df = facts_df.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c175bd07-8cf4-4174-a279-5b0b363b62c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "facts_df_fil= facts_df.join(userid_emp, ['userid','employerid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dbcfb19-307c-49cc-b949-b0c16f3b810b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(facts_df_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42ed114d-aca5-485d-adcb-af4516326e73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for each userid, empolyerid pair keep the last 4 paydates and corresponding total_pck_amt\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "window = Window.partitionBy('userid','employerid').orderBy(desc('paydate'))\n",
    "facts_df_fil_2 = facts_df_fil.withColumn('rn', row_number().over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f46c837-5767-44e5-b84f-9336ebcdb94f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(facts_df_fil_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86373892-536a-495b-9e06-3caad999d3ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "traj_emb_df = spark.read.table(\"main_prod.datascience_scratchpad.traj_emb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f8fc6fc-0d75-44f7-bea4-7c17d9bc422b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joined = (\n",
    "    facts_df_fil_2.join(\n",
    "        broadcast(traj_emb_df),  # remove broadcast() if df1 is large\n",
    "        (facts_df_fil_2.userid == traj_emb_df.userid)\n",
    "        & (traj_emb_df.traj_date >= facts_df_fil_2.prev_paydate)   # for closed interval use <= below\n",
    "        & (traj_emb_df.traj_date < facts_df_fil_2.paydate),\n",
    "        \"left\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06300d0d-1f49-4d4a-921f-f719b85daba1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "facts_df_count = (\n",
    "    joined.groupBy(facts_df_fil_2.userid, facts_df_fil_2.prev_paydate, facts_df_fil_2.paydate, facts_df_fil_2.rn)\n",
    "          .agg(F.count(traj_emb_df.traj_date).alias(\"count\"))\n",
    "          .select(\"userid\", \"prev_paydate\", \"paydate\", \"rn\", \"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3844de7a-b7ef-436b-bcd5-64e0751aaf31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def filter_by_count(prev_date, current_date, count):\n",
    "    n_days = (current_date - prev_date).days\n",
    "    if count >= n_days*0.6:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "df_filtered_by_count = (\n",
    "    facts_df_count\n",
    "    .filter(F.udf(filter_by_count, returnType=BooleanType())(col(\"prev_paydate\"), col(\"paydate\"), col(\"count\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d21dc1a6-97a9-4189-a0b6-160298be9118",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_facts_df = facts_df_fil_2.join(df_filtered_by_count, [\"userid\", \"prev_paydate\", \"paydate\", \"rn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bcbcf7f-2259-4800-9e9f-07b327d133fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final_facts_df_pd = final_facts_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "576057a2-57d9-4a31-9359-a684608420be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# len(final_facts_df_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd74d2e-6ab0-4ac9-b5b2-bcd7b3a9f2cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.functions import col\n",
    "df = (\n",
    "    traj_emb_df.join(\n",
    "        broadcast(final_facts_df),\n",
    "        (traj_emb_df.userid == final_facts_df.userid)\n",
    "        & (col(\"traj_date\") >= col(\"prev_paydate\"))\n",
    "        & (col(\"traj_date\") < col(\"paydate\")),\n",
    "        \"inner\"       # use \"left\" if you want to keep df2 rows without a matching interval\n",
    "    )\n",
    "    .select(traj_emb_df.userid, final_facts_df.employerid, final_facts_df.employername, traj_emb_df.traj_date, traj_emb_df.embedding, final_facts_df.paydate, final_facts_df.rn)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb440b7e-9114-4a51-b171-ad6edf0b32e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e848958-b98d-4e63-b0c9-375b0929bb50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN \n",
    "def apply_dbscan(embs, target_min_similarity=0.9):\n",
    "    eps = 1.0 - target_min_similarity    # cosine distance threshold\n",
    "    n_embs = embs.shape[0]\n",
    "    db = DBSCAN(eps=eps, min_samples=max(int(n_embs*0.25), 4), metric=\"cosine\", n_jobs=-1).fit(embs)\n",
    "    return db\n",
    "\n",
    "def get_cluster(df):\n",
    "    embeddings = np.stack(df['embedding'].values)\n",
    "    db = apply_dbscan(embeddings)\n",
    "    return db\n",
    "\n",
    "def cluster_exists(db):\n",
    "    labels = db.labels_\n",
    "    for label in labels:\n",
    "        if label != -1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def dbscan_predict_all(db, X_train, X_new):\n",
    "    nn = NearestNeighbors(radius=db.eps, metric=db.metric).fit(X_train)\n",
    "    dists, idxs = nn.radius_neighbors(X_new, return_distance=True)\n",
    "    y = db.labels_\n",
    "    pred = np.full(len(X_new), -1, dtype=int)\n",
    "    for i, (di, ii) in enumerate(zip(dists, idxs)):\n",
    "        if len(ii) == 0: \n",
    "            continue\n",
    "        lbls, di = y[ii], di\n",
    "        mask = lbls != -1\n",
    "        if mask.any():\n",
    "            pred[i] = lbls[mask][np.argmin(di[mask])]\n",
    "    return pred\n",
    "    \n",
    "def works_on_weekends_fn(db, weekday_df, weekend_df):\n",
    "    if len(weekday_df) == 0 or len(weekend_df) == 0:\n",
    "        return False\n",
    "    weekday_embs = np.stack(weekday_df['embedding'].values)\n",
    "    weekend_embs = np.stack(weekend_df['embedding'].values)\n",
    "    pred = dbscan_predict_all(db, weekday_embs, weekend_embs)\n",
    "    # if 40% of pred is not -1, then it works on weekends\n",
    "    pred_not_neg = pred[pred != -1]\n",
    "    return len(pred_not_neg) > 0.4*len(pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe8d63d-07da-4aaf-9670-1eafd375c7ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Disable all autologging\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "# Or disable just sklearn autologging\n",
    "mlflow.sklearn.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e65bb186-b2e8-4ac3-84b5-0b02d91e1876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24501313-ddcb-4cbe-b9ac-0aaf2d00a95d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------\n",
    "# Helper functions assumed to exist and be importable on workers:\n",
    "#   get_cluster(pdf) -> any\n",
    "#   cluster_exists(cluster) -> bool\n",
    "#   works_on_weekends_fn(cluster, weekday_pdf, weekend_pdf) -> bool\n",
    "# Ensure they are defined in the same file or available on PYTHONPATH for executors.\n",
    "# -----------------------\n",
    "\n",
    "# Output schema (adjust types if your real types differ)\n",
    "out_schema = T.StructType([\n",
    "    T.StructField(\"userid\", T.IntegerType(), False),\n",
    "    T.StructField(\"employerid\", T.IntegerType(), False),\n",
    "    T.StructField(\"employername\", T.StringType(), True),\n",
    "    T.StructField(\"predicted_work_type\", T.StringType(), True),\n",
    "    T.StructField(\"predicted_on\", T.DateType(), True),\n",
    "    T.StructField(\"works_on_weekends\", T.BooleanType(), True),\n",
    "    T.StructField(\"paydate\", T.DateType(), True),\n",
    "])\n",
    "\n",
    "def compute_static_moving(group_pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Runs your pandas logic for one (userid, employerid, employername) group.\n",
    "    This function executes on a Spark worker.\n",
    "    \"\"\"\n",
    "    import datetime\n",
    "    today = datetime.date.today()\n",
    "\n",
    "    # We only process a single group here\n",
    "    # Extract group keys (safe because it's a single group)\n",
    "    userid = group_pdf[\"userid\"].iloc[0]\n",
    "    employerid = group_pdf[\"employerid\"].iloc[0]\n",
    "    employername = group_pdf.get(\"employername\", pd.Series([None])).iloc[0]\n",
    "\n",
    "    # Ensure 'weekday' exists (create if your input doesn't have it)\n",
    "    if \"weekday\" not in group_pdf.columns:\n",
    "        # If traj date is present, you could compute it; otherwise expect it precomputed\n",
    "        if \"traj_date\" in group_pdf.columns:\n",
    "            group_pdf = group_pdf.copy()\n",
    "            group_pdf[\"weekday\"] = pd.to_datetime(group_pdf[\"traj_date\"]).dt.weekday\n",
    "        else:\n",
    "            raise ValueError(\"Missing 'weekday' column and no 'traj_date' to compute it from.\")\n",
    "\n",
    "    target_n_cycles = 2\n",
    "    outputs = []\n",
    "\n",
    "    # # Convert to proper dtypes just in case (optional but helpful)\n",
    "    # # Expect paydate to be date-like if coming from Spark DateType\n",
    "    # if not pd.api.types.is_datetime64_any_dtype(group_pdf[\"paydate\"]):\n",
    "    #     group_pdf = group_pdf.copy()\n",
    "    #     group_pdf[\"paydate\"] = pd.to_datetime(group_pdf[\"paydate\"]).dt.date\n",
    "\n",
    "    # Loop cycles (kept from your original logic; target_n_cycles = 1)\n",
    "    for target_rn in range(target_n_cycles):\n",
    "        target_paycycle = target_rn + 1\n",
    "\n",
    "        # Latest cycle slices\n",
    "        latest_weekday_df = group_pdf.loc[\n",
    "            (group_pdf[\"weekday\"].isin([0,1,2,3,4])) & (group_pdf[\"rn\"] == target_paycycle)\n",
    "        ]\n",
    "        latest_weekend_df = group_pdf.loc[\n",
    "            (group_pdf[\"weekday\"].isin([5,6])) & (group_pdf[\"rn\"] == target_paycycle)\n",
    "        ]\n",
    "\n",
    "        # Old cycles (NOTE: use bitwise & for pandas)\n",
    "        rn_mask_old = (group_pdf[\"rn\"] > target_paycycle) & (group_pdf[\"rn\"] <= target_paycycle + 3)\n",
    "        old_weekday_df = group_pdf.loc[\n",
    "            (group_pdf[\"weekday\"].isin([0,1,2,3,4])) & rn_mask_old\n",
    "        ]\n",
    "        old_weekend_df = group_pdf.loc[\n",
    "            (group_pdf[\"weekday\"].isin([5,6])) & rn_mask_old\n",
    "        ]\n",
    "\n",
    "        if len(latest_weekday_df) == 0:\n",
    "            continue\n",
    "\n",
    "        old_latest_cluster = get_cluster(pd.concat([latest_weekday_df, old_weekday_df], ignore_index=True))\n",
    "        if cluster_exists(old_latest_cluster):\n",
    "            work_type = \"static\"\n",
    "            works_on_weekends = works_on_weekends_fn(\n",
    "                old_latest_cluster,\n",
    "                pd.concat([latest_weekday_df, old_weekday_df], ignore_index=True),\n",
    "                pd.concat([latest_weekend_df, old_weekend_df], ignore_index=True),\n",
    "            )\n",
    "        else:\n",
    "            latest_cluster = get_cluster(pd.concat([latest_weekday_df, latest_weekend_df], ignore_index=True))\n",
    "            if cluster_exists(latest_cluster):\n",
    "                work_type = \"static\"\n",
    "                works_on_weekends = works_on_weekends_fn(\n",
    "                    latest_cluster, latest_weekday_df, latest_weekend_df\n",
    "                )\n",
    "            else:\n",
    "                work_type = \"moving\"\n",
    "                works_on_weekends = False\n",
    "\n",
    "        # If multiple rows exist for the latest cycle, choose a representative paydate.\n",
    "        # Here we just take the first paydate from the latest cycle rows.\n",
    "        paydate_value = latest_weekday_df[\"paydate\"].iloc[0] if len(latest_weekday_df) else None\n",
    "\n",
    "        outputs.append({\n",
    "            \"userid\": userid,\n",
    "            \"employerid\": employerid,\n",
    "            \"employername\": employername,\n",
    "            \"predicted_work_type\": work_type,\n",
    "            \"predicted_on\": today,\n",
    "            \"works_on_weekends\": works_on_weekends,\n",
    "            \"paydate\": paydate_value,\n",
    "        })\n",
    "\n",
    "    if outputs:\n",
    "        return pd.DataFrame(outputs, columns=[f.name for f in out_schema])\n",
    "    else:\n",
    "        # Return empty frame with correct columns if no output for this group\n",
    "        return pd.DataFrame(columns=[f.name for f in out_schema])\n",
    "\n",
    "# -----------------------\n",
    "# Run on Spark DataFrame\n",
    "# Assume `df` is your Spark DataFrame version of df_pd,\n",
    "# and it has the columns used above: userid, employerid, employername, weekday, rn, paydate (and/or traj_date if you need to compute weekday).\n",
    "# -----------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9955dce3-eddc-4981-8b5f-6b42840e68f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_spark_df = (\n",
    "    df\n",
    "    .groupBy(\"userid\", \"employerid\")\n",
    "    .applyInPandas(compute_static_moving, schema=out_schema)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02ead0d-e5c2-4068-a2ca-8b00f7d04cd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(result_spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a25affef-364b-4f43-b9d2-eef05193ed86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_spark_df.write.mode(\"overwrite\").parquet(\"/Volumes/main_prod/datascience_scratchpad/jatin/trajcl_exp/usa/backfill_static_moving_rem_corrected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa0a66b2-5c2b-442d-97a4-8feb8697def8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db11680-fffe-4a0f-868b-50d6c07a9765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"main_prod.datascience_scratchpad.static_moving_worktype\")\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c18e4c1-a256-4d37-9083-00b47bec875d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# delete enteries from df\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "DELETE FROM main_prod.datascience_scratchpad.static_moving_worktype\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a263e490-1eea-4907-8a97-5b560ed70c37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.parquet(\"/Volumes/main_prod/datascience_scratchpad/jatin/trajcl_exp/usa/backfill_365_static_moving_prediction/\")\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "055f6efd-0f03-45c9-ae50-0f58d488643a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# delete table main_prod.datascience_scratchpad.static_moving_worktype\n",
    "spark.sql(f\"\"\"\n",
    "DROP TABLE main_prod.datascience_scratchpad.static_moving_worktype\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f50d549-14fa-41da-9bb4-952967124b80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # create empty table in df\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"userid\", IntegerType(), True),\n",
    "    StructField(\"employerid\", IntegerType(), True),\n",
    "    StructField(\"employername\", StringType(), True),\n",
    "    StructField(\"predicted_work_type\", StringType(), True),\n",
    "    StructField(\"paydate\", DateType(), True),\n",
    "    StructField(\"predicted_on\", DateType(), True),\n",
    "    StructField(\"works_on_weekends\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "empty_df.write.mode(\"overwrite\").saveAsTable(\"main_prod.datascience_scratchpad.static_moving_worktype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c726a767-fb55-470b-b09d-75882c903f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.write.mode(\"append\").saveAsTable(\"main_prod.datascience_scratchpad.static_moving_worktype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00f799fd-30fb-43fe-b43f-1653763e1755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = spark.read.parquet(\"/Volumes/main_prod/datascience_scratchpad/jatin/trajcl_exp/usa/backfill_static_moving_rem_corrected\")\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc4e964d-6c35-4a14-bf46-16a7ab504cbf",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760071724460}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "351b120c-7f54-402a-a078-cc8d9974eb1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView(\"traj_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b430b078-f9f8-43cc-b141-c972c9268206",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "MERGE INTO main_prod.datascience_scratchpad.static_moving_worktype AS target\n",
    "USING traj_data AS source\n",
    "ON target.userid = source.userid\n",
    "   AND target.employerid = source.employerid AND target.paydate = source.paydate\n",
    "WHEN MATCHED THEN \n",
    "  UPDATE SET *\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c198317e-3b11-4c04-a612-9408fa574d79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"main_prod.datascience_scratchpad.static_moving_worktype\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af34ec2a-201e-45fc-b510-07c58a4fcbd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6998776338348088,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "backfill_static_moving_prediction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
