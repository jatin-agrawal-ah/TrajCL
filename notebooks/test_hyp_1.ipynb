{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../model\")\n",
    "from trajcl import TrajCL\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Config()\n",
    "\n",
    "conf.dataset = 'usa_large_cellyc'\n",
    "conf.post_value_updates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrajCL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ah_databricks_data_loader import DatabricksDataLoader\n",
    "\n",
    "# # Instantiate the DatabricksDataLoader.\n",
    "# ddl = DatabricksDataLoader()\n",
    "\n",
    "# # Load data.\n",
    "# test_df = ddl.load_as_spark(schema=\"datascience_scratchpad\", table=\"nyc_traj_data_v18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.write.mode(\"overwrite\").parquet(\"/home/sagemaker-user/TrajCL/data/nyc/test_hyp1/nyc_data_v18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "parquet_files = glob(\"/home/sagemaker-user/TrajCL/data/nyc/test_hyp1/nyc_data_v18/*.parquet\")\n",
    "total_count = 0\n",
    "df_list = []\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(file)\n",
    "    df_list.append(df)\n",
    "    total_count += len(df)\n",
    "\n",
    "\n",
    "print(f\"Total records in repartitioned files: {total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# test_df = pd.read_parquet(\"/home/sagemaker-user/TrajCL/data/parquet_files/test/nyc_df_v3_with_time/traj_test_df_v3_with_ts.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# userids = test_df['userid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\")\n",
    "checkpoint_file = \"/home/sagemaker-user/TrajCL/exp/la_nyc_emb_add/usa_large_cell_TrajCL_best.pt\"\n",
    "checkpoint = torch.load(checkpoint_file)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.traj import *\n",
    "import pickle\n",
    "from utils.cellspace import *\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "embs_parent = pickle.load(open(Config.dataset_embs_file_parent, 'rb')).to('cpu').detach() # tensor\n",
    "embs_child = pickle.load(open(Config.dataset_embs_file_child, 'rb')).to('cpu').detach() # tensor\n",
    "cellspace_parent = pickle.load(open(Config.dataset_cell_file_parent, 'rb'))\n",
    "cellspace_child = pickle.load(open(Config.dataset_cell_file_child, 'rb'))\n",
    "hier_cellspace = HirearchicalCellSpace(cellspace_parent, cellspace_child)\n",
    "\n",
    "max_batch_size = 512\n",
    "def infer_batch(traj, time_indices):\n",
    "    traj_cell_parent, traj_cell_child, traj_p = zip(*[merc2cell2(t, hier_cellspace) for t in traj])\n",
    "    # print(traj_cell)\n",
    "    traj_emb_p = [torch.tensor(generate_spatial_features(t, hier_cellspace)) for t in traj_p]\n",
    "    traj_emb_p = pad_sequence(traj_emb_p, batch_first = False).to(device)\n",
    "    traj_emb_cell_parent = [embs_parent[list(t)] for t in traj_cell_parent]\n",
    "    traj_emb_cell_child = [embs_child[list(t)] for t in traj_cell_child]\n",
    "    traj_emb_cell = [a + b for a, b in zip(traj_emb_cell_parent, traj_emb_cell_child)]\n",
    "    traj_emb_cell = pad_sequence(traj_emb_cell, batch_first = False).to(device)\n",
    "    traj_len = torch.tensor(list(map(len, traj_p)), dtype = torch.long, device = device)\n",
    "    time_indices = pad_sequence([torch.tensor(t, dtype=torch.long) for t in time_indices], batch_first=False, padding_value=-1).to(Config.device)\n",
    "    # print(traj_emb_cell, traj_emb_p, traj_len)\n",
    "    traj_embs = model.interpret(traj_emb_cell.float(), traj_emb_p.float(), traj_len, time_indices)\n",
    "    return traj_embs\n",
    "\n",
    "def infer(traj, time_indices):\n",
    "    if len(traj)> max_batch_size:\n",
    "        traj_embs = []\n",
    "        for i in range(0, len(traj), max_batch_size):\n",
    "            traj_batch = traj[i:i+max_batch_size]\n",
    "            time_indices_batch = time_indices[i:i+max_batch_size] \n",
    "            traj_embs.append(infer_batch(traj_batch, time_indices_batch))\n",
    "        return torch.cat(traj_embs, dim=0)\n",
    "    else:\n",
    "        return infer_batch(traj, time_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[0]['time_index_list'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_userids = pd.concat([df['userid'] for df in df_list]).unique()\n",
    "print(f\"Total unique userids: {len(unique_userids)}\")\n",
    "\n",
    "unique_employernames = pd.concat([df['employername'] for df in df_list]).unique()\n",
    "print(f\"Total unique employernames: {len(unique_employernames)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_userid(userid):\n",
    "    df_user = pd.concat([df[df['userid']==userid] for df in df_list]).reset_index(drop=True)\n",
    "    return df_user\n",
    "\n",
    "def get_data_for_employername(df_user, employername):\n",
    "    df_emp = df_user[df_user['employername']==employername].reset_index(drop=True)\n",
    "    return df_emp\n",
    "\n",
    "\n",
    "def get_data_for_partition(df_emp, partition):\n",
    "    df_part = df_emp[df_emp['partition_id']==partition].reset_index(drop=True)\n",
    "    df_part = df_part[~df_part['weekday'].isin([5, 6])].reset_index(drop=True) # filter out weekends\n",
    "    df_part = df_part[df_part['pck_amt']>0].reset_index(drop=True) # filter out zero paycheck amount\n",
    "    return df_part\n",
    "\n",
    "\n",
    "def get_traj_and_time_data(df_part):\n",
    "    traj = df_part['merc_seq'].values\n",
    "    time_indices = df_part['time_index_list'].values\n",
    "    return traj, time_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dbscan(embs, target_min_similarity=0.85):\n",
    "    from sklearn.cluster import DBSCAN     # require >= 0.9 cosine similarity\n",
    "    eps = 1.0 - target_min_similarity    # cosine distance threshold\n",
    "    n_embs = embs.shape[0]\n",
    "    db = DBSCAN(eps=eps, min_samples=max(int(n_embs*0.3), 5), metric=\"cosine\", n_jobs=-1).fit(embs)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 423/25831 [00:41<37:28, 11.30it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "output_dict = {}\n",
    "\n",
    "# 0: moving, 1: static, 2: not enough data\n",
    "def get_gt_and_pred_label(userid):\n",
    "    count = 0\n",
    "    user_data = get_data_for_userid(userid)\n",
    "    employers = user_data['employername'].unique()\n",
    "    for employer in employers:\n",
    "        emp_data = get_data_for_employername(user_data, employer)\n",
    "        partitions = emp_data['partition_id'].unique()\n",
    "        for partition in partitions:\n",
    "            # print(userid, employer, partition)\n",
    "            part_data = get_data_for_partition(emp_data, partition)\n",
    "            if len(part_data)<15:\n",
    "                continue\n",
    "            else:\n",
    "                # print(userid, employer, partition)\n",
    "                # print(len(part_data))\n",
    "                traj, time_indices = get_traj_and_time_data(part_data)\n",
    "                # print(traj)\n",
    "                embs = infer(traj, time_indices).detach().cpu().numpy()\n",
    "                db_scan = apply_dbscan(embs, target_min_similarity=0.85)\n",
    "                labels = db_scan.labels_                      # shape: (n_samples,)\n",
    "                cluster_ids = [c for c in np.unique(labels) if c != -1]\n",
    "                if len(cluster_ids)>0:\n",
    "                    pred_label = 1\n",
    "                else:\n",
    "                    pred_label = 0\n",
    "                output_dict[(userid, employer, partition)] = pred_label\n",
    "\n",
    "    return count\n",
    "    # if sum(test_data['paycheck_amount'].values) > 0:\n",
    "    #     gt_label = 1\n",
    "    # else:\n",
    "    #     gt_label = 0\n",
    "    # pred_label = 0\n",
    "    # for i in range(len(test_embs)):\n",
    "    #     test_vector = test_embs[i].unsqueeze(0)\n",
    "    #     similarity = cosine_similarity(test_vector.numpy(), train_embs)[0]\n",
    "    #     top_3_indices = np.argsort(similarity)[-3:][::-1]\n",
    "    #     # print(i, top_3_indices)\n",
    "    #     similarity = similarity[top_3_indices]\n",
    "    #     # print(f\"User: {userid}, Test Trajectory {test_data['traj_id'].values[i]}, Top 3 Train Trajectories: {train_data['traj_id'].values[top_3_indices]}, similarity: {similarity}, PCK Amount: {train_data['paycheck_amount'].values[top_3_indices]}\")\n",
    "    #     for sim, idx in zip(similarity, top_3_indices):\n",
    "    #         if sim>0.85 and train_data['paycheck_amount'].values[idx]>0:\n",
    "    #             pred_label = 1\n",
    "    #             break\n",
    "    # return gt_label, pred_label\n",
    "count=0\n",
    "for userid in tqdm(unique_userids):\n",
    "    count+=get_gt_and_pred_label(userid)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output_dict\n",
    "pickle.dump(output_dict, open(\"/home/sagemaker-user/TrajCL/exp/la_nyc_emb_add/nyc_test_hyp1_output_dict.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict = {}\n",
    "for k,v in output_dict.items():\n",
    "    if v in count_dict:\n",
    "        count_dict[v]+=1\n",
    "    else:\n",
    "        count_dict[v]=1\n",
    "count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_userid_employer_partition(userid, employer, partition):\n",
    "    df_user = pd.concat([df[df['userid']==userid] for df in df_list]).reset_index(drop=True)\n",
    "    df_emp = df_user[df_user['employername']==employer].reset_index(drop=True)\n",
    "    df_part = df_emp[df_emp['partition_id']==partition].reset_index(drop=True)\n",
    "    df_part_fil = df_part[~df_part['weekday'].isin([5, 6])].reset_index(drop=True) # filter out weekends\n",
    "    df_part_fil = df_part_fil[df_part_fil['pck_amt']>0].reset_index(drop=True) # filter out zero paycheck amount\n",
    "    return df_part_fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part = get_data_userid_employer_partition(34299, 'u s postal service', 0)\n",
    "df_part.sort_values('traj_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trajcl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
