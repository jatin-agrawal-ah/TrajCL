{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e1dc3f4-b8cc-4eee-bf73-9231a825464b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select * from clientstate.full_locations\n",
    "\"\"\"\n",
    "\n",
    "df = spark.sql(query)\n",
    "# df.createOrReplaceTempView(\"gps_data\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f2ab898-21b9-452a-b7fc-1b49622aed02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "userpiphistory = spark.read.table(\"main_prod.datascience.userpiphistory\")\n",
    "\n",
    "# keep only valid times (optional but wise)\n",
    "userpiphistory = userpiphistory.filter(F.col(\"createdon\").isNotNull())\n",
    "\n",
    "userid_timezone_df = (\n",
    "    userpiphistory.groupBy(\"userid\")\n",
    "      .agg(F.max(F.struct(\"createdon\", \"timezone\")).alias(\"maxrow\"))\n",
    "      .select(\n",
    "          \"userid\",\n",
    "          F.col(\"maxrow.timezone\").alias(\"timezone\"),\n",
    "      )\n",
    ").where(F.col(\"timezone\").isNotNull())\n",
    "\n",
    "display(userid_timezone_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6df18aad-6d0d-41a7-be63-fb79cc39a6b7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758559268230}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# remove rows where location_timestamp is within 2 days of the current date\n",
    "\n",
    "df_fil = df.where(\"location_timestamp < (unix_timestamp(current_date()) * 1000) and location_timestamp > (unix_timestamp(current_date()) * 1000) - (3 * 24 * 60 * 60 * 1000)\")\n",
    "display(df_fil)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2a39f0a-506d-4ac0-bb06-6d6ca1cacf5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_tz = df_fil.join(userid_timezone_df, [\"userid\"], \"inner\")\n",
    "\n",
    "df_with_tz.createOrReplaceTempView(\"df_with_tz\")\n",
    "display(df_with_tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "049cfb6b-4562-4f73-a030-f2dbe614f250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select distinct userid, latitude, longitude, from_utc_timestamp(from_unixtime(location_timestamp / 1000), timezone) AS localized_timestamp, timezone from df_with_tz\"\"\"\n",
    "\n",
    "df = spark.sql(query)\n",
    "df.createOrReplaceTempView(\"gps_data_loc_ts\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfef9ae5-9821-4271-b29a-15d0312b028d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_v2 = df.where(\"latitude is not NULL and longitude is not NULL and localized_timestamp is not NULL and timezone is not NULL\")\n",
    "display(df_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "254659c6-899f-427c-8213-c79ba67a4ff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def get_current_date_in_tz(tz):\n",
    "    try:\n",
    "        tz_time = datetime.now(ZoneInfo(tz))\n",
    "        tz_date = tz_time.date()\n",
    "        return tz_date\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "get_current_date_in_tz_udf = udf(get_current_date_in_tz, DateType())\n",
    "\n",
    "df_v3  = df_v2.withColumn(\"current_tz_date\", get_current_date_in_tz_udf(\"timezone\"))\n",
    "display(df_v3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8ea78c5-83e5-49d5-a227-d1f3203c9c87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_v4 = df_v3.where(\"current_tz_date is not NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7dbe5e0-fdc5-492b-8d5a-3d012182aa68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# keep only locations with timestamps in the previous day\n",
    "from datetime import timedelta\n",
    "df_v5 = df_v4.where(\"localized_timestamp >= current_tz_date - interval 1 day and localized_timestamp < current_tz_date\")\n",
    "display(df_v5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eece14cf-0ae4-4686-b428-c187ed89abbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_v5.createOrReplaceTempView(\"all_traj_data_loc_ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "813d199d-6c79-479e-9d24-bcb2900e9087",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    userid,\n",
    "    DATE(localized_timestamp) AS traj_date,\n",
    "    COLLECT_LIST(localized_timestamp) AS timestamps,\n",
    "    COLLECT_LIST(latitude) AS latitudes,\n",
    "    COLLECT_LIST(longitude) AS longitudes\n",
    "FROM \n",
    "    all_traj_data_loc_ts\n",
    "GROUP BY \n",
    "    userid, DATE(localized_timestamp)\n",
    "ORDER BY \n",
    "    userid, traj_date\n",
    "\"\"\"\n",
    "\n",
    "result_df = spark.sql(query)\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7b0ee08-ef07-4093-accd-5b94050b27b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, DoubleType, DateType, TimestampType\n",
    "def sort_by_time(timestamps, longitudes, latitudes):\n",
    "    sorted_ts = []\n",
    "    polylines = []\n",
    "    for ts, lon, lat in sorted(zip(timestamps, longitudes, latitudes), key=lambda x: x[0]):\n",
    "        sorted_ts.append(ts)\n",
    "        polylines.append([float(lon), float(lat)])\n",
    "    return sorted_ts, polylines\n",
    "\n",
    "\n",
    "sort_and_extract_udf = udf(sort_by_time, \n",
    "                           StructType([\n",
    "                               StructField(\"sorted_ts\", ArrayType(TimestampType())),\n",
    "                               StructField(\"polylines\", ArrayType(ArrayType(DoubleType())))\n",
    "                           ]))\n",
    "\n",
    "\n",
    "result_df_v2 = result_df.withColumn(\"sorted_data\", \n",
    "                                         sort_and_extract_udf(\"timestamps\", \"longitudes\", \"latitudes\"))\n",
    "\n",
    "result_df_v2 = result_df_v2.withColumn(\"sorted_ts\", col(\"sorted_data.sorted_ts\")) \\\n",
    "                             .withColumn(\"wgs_seq\", col(\"sorted_data.polylines\")) \\\n",
    "                             .drop(\"sorted_data\")\n",
    "display(result_df_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cce5683b-0ebe-4df0-a690-4086d6f0db9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df_v3 = result_df_v2.drop(\"timestamps\", \"longitudes\", \"latitudes\")\n",
    "display(result_df_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36c43268-a08d-4c36-af77-f1774b3e7598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, ArrayType, DoubleType, BooleanType\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "import math\n",
    "def lonlat2meters(lon, lat):\n",
    "    semimajoraxis = 6378137.0\n",
    "    east = lon * 0.017453292519943295\n",
    "    north = lat * 0.017453292519943295\n",
    "    t = math.sin(north)\n",
    "    return semimajoraxis * east, 3189068.5 * math.log((1 + t + 1e-5) / (1 - t + 1e-5))\n",
    "\n",
    "\n",
    "lonlat2meters_udf = udf(lambda traj: [list(lonlat2meters(p[0], p[1])) for p in traj], ArrayType(ArrayType(DoubleType())))\n",
    "result_df_v4 = result_df_v3.withColumn(\"merc_seq\", lonlat2meters_udf(col(\"wgs_seq\")))\n",
    "display(result_df_v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6a0c286-715d-4360-9d0e-a00f00575011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def filter_based_on_timestamps(ts_list):\n",
    "    unique_hours = set()\n",
    "    for ts in ts_list:\n",
    "        unique_hours.add(ts.hour)\n",
    "    if len(unique_hours) > 7:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "filter_based_on_timestamps_udf = udf(filter_based_on_timestamps, BooleanType())\n",
    "result_df_v5 = result_df_v4.filter(filter_based_on_timestamps_udf(col(\"sorted_ts\")))\n",
    "display(result_df_v5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9025a135-a43f-4453-a40b-20aedeebfeac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# result_df_v5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1324e8b-4809-42e2-b1c6-41b31bd7dc6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # create empty table in df\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField(\"userid\", IntegerType(), True),\n",
    "#     StructField(\"traj_date\", DateType(), True),\n",
    "#     StructField(\"sorted_ts\", ArrayType(TimestampType()), True),\n",
    "#     StructField(\"wgs_seq\", ArrayType(ArrayType(DoubleType()))),\n",
    "#     StructField(\"merc_seq\", ArrayType(ArrayType(DoubleType())))\n",
    "# ])\n",
    "\n",
    "# empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "# empty_df.write.mode(\"overwrite\").saveAsTable(\"main_prod.datascience_scratchpad.traj_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5adf274-2d76-428b-9af7-c0ff93037333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df_v5.createOrReplaceTempView(\"traj_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71133654-ba1e-4dd4-ba31-7d59356d4bbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "MERGE INTO main_prod.datascience_scratchpad.traj_data AS target\n",
    "USING traj_data AS source\n",
    "ON target.userid = source.userid\n",
    "   AND target.traj_date = source.traj_date\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aca1375e-caab-4dd8-a48a-721483c5f79d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from main_prod.datascience_scratchpad.traj_data"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8862198372592439,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Trajectory Model Inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
